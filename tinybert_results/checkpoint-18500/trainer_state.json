{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 9.866666666666667,
  "eval_steps": 500,
  "global_step": 18500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.05333333333333334,
      "grad_norm": 7.362849712371826,
      "learning_rate": 0.0009947200000000002,
      "loss": 0.5551,
      "step": 100
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 6.284451961517334,
      "learning_rate": 0.0009893866666666666,
      "loss": 0.35,
      "step": 200
    },
    {
      "epoch": 0.16,
      "grad_norm": 1.4628201723098755,
      "learning_rate": 0.0009840533333333333,
      "loss": 0.3264,
      "step": 300
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 3.9975996017456055,
      "learning_rate": 0.00097872,
      "loss": 0.3231,
      "step": 400
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 3.348611354827881,
      "learning_rate": 0.0009733866666666667,
      "loss": 0.2992,
      "step": 500
    },
    {
      "epoch": 0.32,
      "grad_norm": 1.3593659400939941,
      "learning_rate": 0.0009680533333333333,
      "loss": 0.2987,
      "step": 600
    },
    {
      "epoch": 0.37333333333333335,
      "grad_norm": 1.7879390716552734,
      "learning_rate": 0.00096272,
      "loss": 0.3073,
      "step": 700
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 3.699535608291626,
      "learning_rate": 0.0009573866666666667,
      "loss": 0.2864,
      "step": 800
    },
    {
      "epoch": 0.48,
      "grad_norm": 1.9343971014022827,
      "learning_rate": 0.0009520533333333333,
      "loss": 0.2864,
      "step": 900
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 2.8498761653900146,
      "learning_rate": 0.0009467200000000001,
      "loss": 0.2867,
      "step": 1000
    },
    {
      "epoch": 0.5866666666666667,
      "grad_norm": 2.7453463077545166,
      "learning_rate": 0.0009413866666666668,
      "loss": 0.2747,
      "step": 1100
    },
    {
      "epoch": 0.64,
      "grad_norm": 2.0454256534576416,
      "learning_rate": 0.0009360533333333333,
      "loss": 0.2676,
      "step": 1200
    },
    {
      "epoch": 0.6933333333333334,
      "grad_norm": 1.1605989933013916,
      "learning_rate": 0.00093072,
      "loss": 0.2537,
      "step": 1300
    },
    {
      "epoch": 0.7466666666666667,
      "grad_norm": 2.760418176651001,
      "learning_rate": 0.0009253866666666667,
      "loss": 0.2718,
      "step": 1400
    },
    {
      "epoch": 0.8,
      "grad_norm": 2.8108749389648438,
      "learning_rate": 0.0009200533333333333,
      "loss": 0.2529,
      "step": 1500
    },
    {
      "epoch": 0.8533333333333334,
      "grad_norm": 2.662377119064331,
      "learning_rate": 0.0009147199999999999,
      "loss": 0.26,
      "step": 1600
    },
    {
      "epoch": 0.9066666666666666,
      "grad_norm": 1.8640881776809692,
      "learning_rate": 0.0009093866666666667,
      "loss": 0.2597,
      "step": 1700
    },
    {
      "epoch": 0.96,
      "grad_norm": 2.4968597888946533,
      "learning_rate": 0.0009040533333333334,
      "loss": 0.2514,
      "step": 1800
    },
    {
      "epoch": 1.0133333333333334,
      "grad_norm": 2.0354270935058594,
      "learning_rate": 0.00089872,
      "loss": 0.2457,
      "step": 1900
    },
    {
      "epoch": 1.0666666666666667,
      "grad_norm": 2.356489658355713,
      "learning_rate": 0.0008933866666666667,
      "loss": 0.1805,
      "step": 2000
    },
    {
      "epoch": 1.12,
      "grad_norm": 1.6249947547912598,
      "learning_rate": 0.0008880533333333334,
      "loss": 0.1804,
      "step": 2100
    },
    {
      "epoch": 1.1733333333333333,
      "grad_norm": 1.9286633729934692,
      "learning_rate": 0.0008827199999999999,
      "loss": 0.1791,
      "step": 2200
    },
    {
      "epoch": 1.2266666666666666,
      "grad_norm": 3.395906686782837,
      "learning_rate": 0.0008773866666666667,
      "loss": 0.1816,
      "step": 2300
    },
    {
      "epoch": 1.28,
      "grad_norm": 2.3978536128997803,
      "learning_rate": 0.0008720533333333334,
      "loss": 0.1836,
      "step": 2400
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 1.5409213304519653,
      "learning_rate": 0.0008667200000000001,
      "loss": 0.1922,
      "step": 2500
    },
    {
      "epoch": 1.3866666666666667,
      "grad_norm": 3.4150381088256836,
      "learning_rate": 0.0008613866666666667,
      "loss": 0.1905,
      "step": 2600
    },
    {
      "epoch": 1.44,
      "grad_norm": 0.8658549785614014,
      "learning_rate": 0.0008560533333333333,
      "loss": 0.1801,
      "step": 2700
    },
    {
      "epoch": 1.4933333333333334,
      "grad_norm": 5.981366157531738,
      "learning_rate": 0.00085072,
      "loss": 0.176,
      "step": 2800
    },
    {
      "epoch": 1.5466666666666666,
      "grad_norm": 1.465886116027832,
      "learning_rate": 0.0008453866666666666,
      "loss": 0.1752,
      "step": 2900
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.9365718960762024,
      "learning_rate": 0.0008400533333333334,
      "loss": 0.1826,
      "step": 3000
    },
    {
      "epoch": 1.6533333333333333,
      "grad_norm": 1.48076331615448,
      "learning_rate": 0.0008347200000000001,
      "loss": 0.1923,
      "step": 3100
    },
    {
      "epoch": 1.7066666666666666,
      "grad_norm": 1.9270596504211426,
      "learning_rate": 0.0008293866666666667,
      "loss": 0.1833,
      "step": 3200
    },
    {
      "epoch": 1.76,
      "grad_norm": 2.3087568283081055,
      "learning_rate": 0.0008240533333333333,
      "loss": 0.1797,
      "step": 3300
    },
    {
      "epoch": 1.8133333333333335,
      "grad_norm": 1.1432702541351318,
      "learning_rate": 0.00081872,
      "loss": 0.1905,
      "step": 3400
    },
    {
      "epoch": 1.8666666666666667,
      "grad_norm": 0.6631373763084412,
      "learning_rate": 0.0008133866666666667,
      "loss": 0.1839,
      "step": 3500
    },
    {
      "epoch": 1.92,
      "grad_norm": 1.3505665063858032,
      "learning_rate": 0.0008080533333333334,
      "loss": 0.2015,
      "step": 3600
    },
    {
      "epoch": 1.9733333333333334,
      "grad_norm": 2.0597071647644043,
      "learning_rate": 0.00080272,
      "loss": 0.1911,
      "step": 3700
    },
    {
      "epoch": 2.026666666666667,
      "grad_norm": 1.0913465023040771,
      "learning_rate": 0.0007973866666666667,
      "loss": 0.1532,
      "step": 3800
    },
    {
      "epoch": 2.08,
      "grad_norm": 1.7325456142425537,
      "learning_rate": 0.0007920533333333334,
      "loss": 0.1265,
      "step": 3900
    },
    {
      "epoch": 2.1333333333333333,
      "grad_norm": 0.9107606410980225,
      "learning_rate": 0.00078672,
      "loss": 0.1188,
      "step": 4000
    },
    {
      "epoch": 2.1866666666666665,
      "grad_norm": 2.106166124343872,
      "learning_rate": 0.0007813866666666667,
      "loss": 0.122,
      "step": 4100
    },
    {
      "epoch": 2.24,
      "grad_norm": 1.0412553548812866,
      "learning_rate": 0.0007760533333333333,
      "loss": 0.1319,
      "step": 4200
    },
    {
      "epoch": 2.2933333333333334,
      "grad_norm": 7.455487251281738,
      "learning_rate": 0.00077072,
      "loss": 0.1268,
      "step": 4300
    },
    {
      "epoch": 2.3466666666666667,
      "grad_norm": 1.8364379405975342,
      "learning_rate": 0.0007653866666666667,
      "loss": 0.1148,
      "step": 4400
    },
    {
      "epoch": 2.4,
      "grad_norm": 2.239408254623413,
      "learning_rate": 0.0007600533333333334,
      "loss": 0.1474,
      "step": 4500
    },
    {
      "epoch": 2.453333333333333,
      "grad_norm": 1.3716422319412231,
      "learning_rate": 0.00075472,
      "loss": 0.1287,
      "step": 4600
    },
    {
      "epoch": 2.506666666666667,
      "grad_norm": 1.2214300632476807,
      "learning_rate": 0.0007493866666666666,
      "loss": 0.1411,
      "step": 4700
    },
    {
      "epoch": 2.56,
      "grad_norm": 1.8803951740264893,
      "learning_rate": 0.0007440533333333333,
      "loss": 0.1362,
      "step": 4800
    },
    {
      "epoch": 2.6133333333333333,
      "grad_norm": 1.758133053779602,
      "learning_rate": 0.0007387200000000001,
      "loss": 0.1336,
      "step": 4900
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 0.6046545505523682,
      "learning_rate": 0.0007333866666666667,
      "loss": 0.1233,
      "step": 5000
    },
    {
      "epoch": 2.7199999999999998,
      "grad_norm": 1.5075781345367432,
      "learning_rate": 0.0007280533333333334,
      "loss": 0.1364,
      "step": 5100
    },
    {
      "epoch": 2.7733333333333334,
      "grad_norm": 6.636039733886719,
      "learning_rate": 0.00072272,
      "loss": 0.1304,
      "step": 5200
    },
    {
      "epoch": 2.8266666666666667,
      "grad_norm": 1.7320002317428589,
      "learning_rate": 0.0007173866666666666,
      "loss": 0.1223,
      "step": 5300
    },
    {
      "epoch": 2.88,
      "grad_norm": 3.257150888442993,
      "learning_rate": 0.0007120533333333333,
      "loss": 0.1337,
      "step": 5400
    },
    {
      "epoch": 2.9333333333333336,
      "grad_norm": 1.4255166053771973,
      "learning_rate": 0.00070672,
      "loss": 0.1312,
      "step": 5500
    },
    {
      "epoch": 2.986666666666667,
      "grad_norm": 4.216322898864746,
      "learning_rate": 0.0007013866666666668,
      "loss": 0.1296,
      "step": 5600
    },
    {
      "epoch": 3.04,
      "grad_norm": 0.5312800407409668,
      "learning_rate": 0.0006960533333333333,
      "loss": 0.0886,
      "step": 5700
    },
    {
      "epoch": 3.0933333333333333,
      "grad_norm": 0.5667928457260132,
      "learning_rate": 0.00069072,
      "loss": 0.081,
      "step": 5800
    },
    {
      "epoch": 3.1466666666666665,
      "grad_norm": 1.1849132776260376,
      "learning_rate": 0.0006853866666666667,
      "loss": 0.0877,
      "step": 5900
    },
    {
      "epoch": 3.2,
      "grad_norm": 0.2376827597618103,
      "learning_rate": 0.0006800533333333333,
      "loss": 0.0864,
      "step": 6000
    },
    {
      "epoch": 3.2533333333333334,
      "grad_norm": 0.5149827003479004,
      "learning_rate": 0.00067472,
      "loss": 0.0843,
      "step": 6100
    },
    {
      "epoch": 3.3066666666666666,
      "grad_norm": 2.202829122543335,
      "learning_rate": 0.0006693866666666666,
      "loss": 0.0906,
      "step": 6200
    },
    {
      "epoch": 3.36,
      "grad_norm": 0.4967600107192993,
      "learning_rate": 0.0006640533333333334,
      "loss": 0.0863,
      "step": 6300
    },
    {
      "epoch": 3.413333333333333,
      "grad_norm": 0.6361421346664429,
      "learning_rate": 0.00065872,
      "loss": 0.0975,
      "step": 6400
    },
    {
      "epoch": 3.466666666666667,
      "grad_norm": 6.424553394317627,
      "learning_rate": 0.0006533866666666667,
      "loss": 0.0954,
      "step": 6500
    },
    {
      "epoch": 3.52,
      "grad_norm": 1.76100754737854,
      "learning_rate": 0.0006480533333333334,
      "loss": 0.0898,
      "step": 6600
    },
    {
      "epoch": 3.5733333333333333,
      "grad_norm": 1.3994253873825073,
      "learning_rate": 0.0006427199999999999,
      "loss": 0.1011,
      "step": 6700
    },
    {
      "epoch": 3.626666666666667,
      "grad_norm": 3.4559431076049805,
      "learning_rate": 0.0006373866666666666,
      "loss": 0.099,
      "step": 6800
    },
    {
      "epoch": 3.68,
      "grad_norm": 0.8625317215919495,
      "learning_rate": 0.0006320533333333334,
      "loss": 0.0973,
      "step": 6900
    },
    {
      "epoch": 3.7333333333333334,
      "grad_norm": 5.982828617095947,
      "learning_rate": 0.0006267200000000001,
      "loss": 0.1061,
      "step": 7000
    },
    {
      "epoch": 3.7866666666666666,
      "grad_norm": 0.9295176267623901,
      "learning_rate": 0.0006213866666666667,
      "loss": 0.102,
      "step": 7100
    },
    {
      "epoch": 3.84,
      "grad_norm": 1.6309698820114136,
      "learning_rate": 0.0006160533333333334,
      "loss": 0.0939,
      "step": 7200
    },
    {
      "epoch": 3.8933333333333335,
      "grad_norm": 0.3006655275821686,
      "learning_rate": 0.00061072,
      "loss": 0.1061,
      "step": 7300
    },
    {
      "epoch": 3.9466666666666668,
      "grad_norm": 2.582160711288452,
      "learning_rate": 0.0006053866666666666,
      "loss": 0.0952,
      "step": 7400
    },
    {
      "epoch": 4.0,
      "grad_norm": 1.1900811195373535,
      "learning_rate": 0.0006000533333333333,
      "loss": 0.1003,
      "step": 7500
    },
    {
      "epoch": 4.053333333333334,
      "grad_norm": 2.08111572265625,
      "learning_rate": 0.0005947200000000001,
      "loss": 0.0618,
      "step": 7600
    },
    {
      "epoch": 4.1066666666666665,
      "grad_norm": 1.3513015508651733,
      "learning_rate": 0.0005893866666666667,
      "loss": 0.0572,
      "step": 7700
    },
    {
      "epoch": 4.16,
      "grad_norm": 1.742775797843933,
      "learning_rate": 0.0005840533333333333,
      "loss": 0.0623,
      "step": 7800
    },
    {
      "epoch": 4.213333333333333,
      "grad_norm": 4.037193775177002,
      "learning_rate": 0.00057872,
      "loss": 0.0531,
      "step": 7900
    },
    {
      "epoch": 4.266666666666667,
      "grad_norm": 4.395145416259766,
      "learning_rate": 0.0005733866666666667,
      "loss": 0.0608,
      "step": 8000
    },
    {
      "epoch": 4.32,
      "grad_norm": 1.3329523801803589,
      "learning_rate": 0.0005680533333333333,
      "loss": 0.0633,
      "step": 8100
    },
    {
      "epoch": 4.373333333333333,
      "grad_norm": 0.89250248670578,
      "learning_rate": 0.0005627200000000001,
      "loss": 0.0653,
      "step": 8200
    },
    {
      "epoch": 4.426666666666667,
      "grad_norm": 2.1930601596832275,
      "learning_rate": 0.0005573866666666667,
      "loss": 0.0687,
      "step": 8300
    },
    {
      "epoch": 4.48,
      "grad_norm": 0.8505581021308899,
      "learning_rate": 0.0005520533333333333,
      "loss": 0.0664,
      "step": 8400
    },
    {
      "epoch": 4.533333333333333,
      "grad_norm": 0.39098891615867615,
      "learning_rate": 0.00054672,
      "loss": 0.0669,
      "step": 8500
    },
    {
      "epoch": 4.586666666666667,
      "grad_norm": 0.6103433966636658,
      "learning_rate": 0.0005413866666666667,
      "loss": 0.0629,
      "step": 8600
    },
    {
      "epoch": 4.64,
      "grad_norm": 0.6697975993156433,
      "learning_rate": 0.0005360533333333334,
      "loss": 0.069,
      "step": 8700
    },
    {
      "epoch": 4.693333333333333,
      "grad_norm": 0.5715216398239136,
      "learning_rate": 0.0005307199999999999,
      "loss": 0.0682,
      "step": 8800
    },
    {
      "epoch": 4.746666666666667,
      "grad_norm": 1.3211902379989624,
      "learning_rate": 0.0005253866666666667,
      "loss": 0.0686,
      "step": 8900
    },
    {
      "epoch": 4.8,
      "grad_norm": 0.6977686882019043,
      "learning_rate": 0.0005200533333333334,
      "loss": 0.0711,
      "step": 9000
    },
    {
      "epoch": 4.8533333333333335,
      "grad_norm": 2.4323222637176514,
      "learning_rate": 0.00051472,
      "loss": 0.0593,
      "step": 9100
    },
    {
      "epoch": 4.906666666666666,
      "grad_norm": 0.8489453196525574,
      "learning_rate": 0.0005093866666666667,
      "loss": 0.0705,
      "step": 9200
    },
    {
      "epoch": 4.96,
      "grad_norm": 1.8016189336776733,
      "learning_rate": 0.0005040533333333333,
      "loss": 0.0696,
      "step": 9300
    },
    {
      "epoch": 5.013333333333334,
      "grad_norm": 2.366487503051758,
      "learning_rate": 0.00049872,
      "loss": 0.0549,
      "step": 9400
    },
    {
      "epoch": 5.066666666666666,
      "grad_norm": 0.2830754518508911,
      "learning_rate": 0.0004933866666666667,
      "loss": 0.0462,
      "step": 9500
    },
    {
      "epoch": 5.12,
      "grad_norm": 3.6871628761291504,
      "learning_rate": 0.00048805333333333333,
      "loss": 0.042,
      "step": 9600
    },
    {
      "epoch": 5.173333333333334,
      "grad_norm": 1.5695933103561401,
      "learning_rate": 0.00048272,
      "loss": 0.0458,
      "step": 9700
    },
    {
      "epoch": 5.226666666666667,
      "grad_norm": 7.449938774108887,
      "learning_rate": 0.0004773866666666667,
      "loss": 0.0354,
      "step": 9800
    },
    {
      "epoch": 5.28,
      "grad_norm": 0.8012632727622986,
      "learning_rate": 0.0004720533333333333,
      "loss": 0.0384,
      "step": 9900
    },
    {
      "epoch": 5.333333333333333,
      "grad_norm": 3.2168233394622803,
      "learning_rate": 0.00046672000000000006,
      "loss": 0.0465,
      "step": 10000
    },
    {
      "epoch": 5.386666666666667,
      "grad_norm": 0.9882327914237976,
      "learning_rate": 0.0004613866666666667,
      "loss": 0.0453,
      "step": 10100
    },
    {
      "epoch": 5.44,
      "grad_norm": 1.1321966648101807,
      "learning_rate": 0.0004560533333333333,
      "loss": 0.0455,
      "step": 10200
    },
    {
      "epoch": 5.493333333333333,
      "grad_norm": 0.7401571273803711,
      "learning_rate": 0.00045072,
      "loss": 0.0479,
      "step": 10300
    },
    {
      "epoch": 5.546666666666667,
      "grad_norm": 0.15492036938667297,
      "learning_rate": 0.0004453866666666667,
      "loss": 0.0406,
      "step": 10400
    },
    {
      "epoch": 5.6,
      "grad_norm": 2.7768898010253906,
      "learning_rate": 0.00044005333333333336,
      "loss": 0.0535,
      "step": 10500
    },
    {
      "epoch": 5.653333333333333,
      "grad_norm": 1.5424840450286865,
      "learning_rate": 0.00043472,
      "loss": 0.0514,
      "step": 10600
    },
    {
      "epoch": 5.706666666666667,
      "grad_norm": 0.4367693066596985,
      "learning_rate": 0.0004293866666666667,
      "loss": 0.0466,
      "step": 10700
    },
    {
      "epoch": 5.76,
      "grad_norm": 1.8831675052642822,
      "learning_rate": 0.00042405333333333335,
      "loss": 0.0606,
      "step": 10800
    },
    {
      "epoch": 5.8133333333333335,
      "grad_norm": 0.08655188232660294,
      "learning_rate": 0.00041872,
      "loss": 0.0515,
      "step": 10900
    },
    {
      "epoch": 5.866666666666667,
      "grad_norm": 1.2767008543014526,
      "learning_rate": 0.0004133866666666667,
      "loss": 0.0472,
      "step": 11000
    },
    {
      "epoch": 5.92,
      "grad_norm": 1.0051745176315308,
      "learning_rate": 0.00040805333333333334,
      "loss": 0.0494,
      "step": 11100
    },
    {
      "epoch": 5.973333333333334,
      "grad_norm": 0.3300437331199646,
      "learning_rate": 0.00040272,
      "loss": 0.0433,
      "step": 11200
    },
    {
      "epoch": 6.026666666666666,
      "grad_norm": 3.5606374740600586,
      "learning_rate": 0.00039738666666666665,
      "loss": 0.0395,
      "step": 11300
    },
    {
      "epoch": 6.08,
      "grad_norm": 4.202638149261475,
      "learning_rate": 0.00039205333333333333,
      "loss": 0.0276,
      "step": 11400
    },
    {
      "epoch": 6.133333333333334,
      "grad_norm": 0.5551910400390625,
      "learning_rate": 0.00038672,
      "loss": 0.0326,
      "step": 11500
    },
    {
      "epoch": 6.1866666666666665,
      "grad_norm": 0.06535632163286209,
      "learning_rate": 0.00038138666666666664,
      "loss": 0.0324,
      "step": 11600
    },
    {
      "epoch": 6.24,
      "grad_norm": 1.5349594354629517,
      "learning_rate": 0.0003760533333333334,
      "loss": 0.0305,
      "step": 11700
    },
    {
      "epoch": 6.293333333333333,
      "grad_norm": 3.1132545471191406,
      "learning_rate": 0.00037072,
      "loss": 0.0344,
      "step": 11800
    },
    {
      "epoch": 6.346666666666667,
      "grad_norm": 0.06890223175287247,
      "learning_rate": 0.0003653866666666667,
      "loss": 0.0361,
      "step": 11900
    },
    {
      "epoch": 6.4,
      "grad_norm": 5.197762966156006,
      "learning_rate": 0.00036005333333333336,
      "loss": 0.0318,
      "step": 12000
    },
    {
      "epoch": 6.453333333333333,
      "grad_norm": 3.169757843017578,
      "learning_rate": 0.00035472,
      "loss": 0.027,
      "step": 12100
    },
    {
      "epoch": 6.506666666666667,
      "grad_norm": 0.07639428228139877,
      "learning_rate": 0.0003493866666666667,
      "loss": 0.0293,
      "step": 12200
    },
    {
      "epoch": 6.5600000000000005,
      "grad_norm": 0.4296119511127472,
      "learning_rate": 0.00034405333333333336,
      "loss": 0.031,
      "step": 12300
    },
    {
      "epoch": 6.613333333333333,
      "grad_norm": 1.041074514389038,
      "learning_rate": 0.00033872000000000004,
      "loss": 0.032,
      "step": 12400
    },
    {
      "epoch": 6.666666666666667,
      "grad_norm": 0.3821200430393219,
      "learning_rate": 0.00033338666666666666,
      "loss": 0.0293,
      "step": 12500
    },
    {
      "epoch": 6.72,
      "grad_norm": 0.5839501619338989,
      "learning_rate": 0.0003280533333333333,
      "loss": 0.0352,
      "step": 12600
    },
    {
      "epoch": 6.773333333333333,
      "grad_norm": 3.5193071365356445,
      "learning_rate": 0.00032272000000000003,
      "loss": 0.0306,
      "step": 12700
    },
    {
      "epoch": 6.826666666666666,
      "grad_norm": 1.9026761054992676,
      "learning_rate": 0.00031738666666666665,
      "loss": 0.035,
      "step": 12800
    },
    {
      "epoch": 6.88,
      "grad_norm": 0.750337541103363,
      "learning_rate": 0.00031205333333333334,
      "loss": 0.028,
      "step": 12900
    },
    {
      "epoch": 6.933333333333334,
      "grad_norm": 2.0106801986694336,
      "learning_rate": 0.00030672,
      "loss": 0.0304,
      "step": 13000
    },
    {
      "epoch": 6.986666666666666,
      "grad_norm": 0.4689732491970062,
      "learning_rate": 0.0003013866666666667,
      "loss": 0.0422,
      "step": 13100
    },
    {
      "epoch": 7.04,
      "grad_norm": 0.06351369619369507,
      "learning_rate": 0.0002960533333333333,
      "loss": 0.0271,
      "step": 13200
    },
    {
      "epoch": 7.093333333333334,
      "grad_norm": 1.0276261568069458,
      "learning_rate": 0.00029072,
      "loss": 0.0202,
      "step": 13300
    },
    {
      "epoch": 7.1466666666666665,
      "grad_norm": 3.481928586959839,
      "learning_rate": 0.0002853866666666667,
      "loss": 0.0192,
      "step": 13400
    },
    {
      "epoch": 7.2,
      "grad_norm": 0.8346734642982483,
      "learning_rate": 0.0002800533333333333,
      "loss": 0.0167,
      "step": 13500
    },
    {
      "epoch": 7.253333333333333,
      "grad_norm": 0.1095859482884407,
      "learning_rate": 0.00027472,
      "loss": 0.0116,
      "step": 13600
    },
    {
      "epoch": 7.306666666666667,
      "grad_norm": 1.6349962949752808,
      "learning_rate": 0.0002693866666666667,
      "loss": 0.0185,
      "step": 13700
    },
    {
      "epoch": 7.36,
      "grad_norm": 0.517981231212616,
      "learning_rate": 0.0002640533333333333,
      "loss": 0.0271,
      "step": 13800
    },
    {
      "epoch": 7.413333333333333,
      "grad_norm": 0.08587189763784409,
      "learning_rate": 0.00025872,
      "loss": 0.0228,
      "step": 13900
    },
    {
      "epoch": 7.466666666666667,
      "grad_norm": 4.064233779907227,
      "learning_rate": 0.00025338666666666667,
      "loss": 0.0192,
      "step": 14000
    },
    {
      "epoch": 7.52,
      "grad_norm": 0.9682847261428833,
      "learning_rate": 0.0002480533333333333,
      "loss": 0.0213,
      "step": 14100
    },
    {
      "epoch": 7.573333333333333,
      "grad_norm": 2.024186134338379,
      "learning_rate": 0.00024272,
      "loss": 0.019,
      "step": 14200
    },
    {
      "epoch": 7.626666666666667,
      "grad_norm": 0.2657102346420288,
      "learning_rate": 0.00023738666666666666,
      "loss": 0.0173,
      "step": 14300
    },
    {
      "epoch": 7.68,
      "grad_norm": 0.3212015926837921,
      "learning_rate": 0.00023205333333333334,
      "loss": 0.0156,
      "step": 14400
    },
    {
      "epoch": 7.733333333333333,
      "grad_norm": 0.20668736100196838,
      "learning_rate": 0.00022672,
      "loss": 0.0225,
      "step": 14500
    },
    {
      "epoch": 7.786666666666667,
      "grad_norm": 0.006226444151252508,
      "learning_rate": 0.00022138666666666668,
      "loss": 0.0196,
      "step": 14600
    },
    {
      "epoch": 7.84,
      "grad_norm": 0.22016577422618866,
      "learning_rate": 0.00021605333333333333,
      "loss": 0.0227,
      "step": 14700
    },
    {
      "epoch": 7.8933333333333335,
      "grad_norm": 0.09102494269609451,
      "learning_rate": 0.00021072,
      "loss": 0.0262,
      "step": 14800
    },
    {
      "epoch": 7.946666666666666,
      "grad_norm": 0.20558084547519684,
      "learning_rate": 0.00020538666666666667,
      "loss": 0.023,
      "step": 14900
    },
    {
      "epoch": 8.0,
      "grad_norm": 0.7536460161209106,
      "learning_rate": 0.00020005333333333332,
      "loss": 0.0165,
      "step": 15000
    },
    {
      "epoch": 8.053333333333333,
      "grad_norm": 7.699744701385498,
      "learning_rate": 0.00019472,
      "loss": 0.0085,
      "step": 15100
    },
    {
      "epoch": 8.106666666666667,
      "grad_norm": 0.0049005914479494095,
      "learning_rate": 0.0001893866666666667,
      "loss": 0.0171,
      "step": 15200
    },
    {
      "epoch": 8.16,
      "grad_norm": 0.32689473032951355,
      "learning_rate": 0.00018405333333333334,
      "loss": 0.0103,
      "step": 15300
    },
    {
      "epoch": 8.213333333333333,
      "grad_norm": 0.007888773456215858,
      "learning_rate": 0.00017872,
      "loss": 0.0146,
      "step": 15400
    },
    {
      "epoch": 8.266666666666667,
      "grad_norm": 0.008077251724898815,
      "learning_rate": 0.00017338666666666668,
      "loss": 0.0112,
      "step": 15500
    },
    {
      "epoch": 8.32,
      "grad_norm": 2.4543347358703613,
      "learning_rate": 0.00016805333333333333,
      "loss": 0.0106,
      "step": 15600
    },
    {
      "epoch": 8.373333333333333,
      "grad_norm": 0.03552936017513275,
      "learning_rate": 0.00016272000000000001,
      "loss": 0.013,
      "step": 15700
    },
    {
      "epoch": 8.426666666666666,
      "grad_norm": 0.6302736401557922,
      "learning_rate": 0.00015738666666666667,
      "loss": 0.0122,
      "step": 15800
    },
    {
      "epoch": 8.48,
      "grad_norm": 0.006878710817545652,
      "learning_rate": 0.00015205333333333335,
      "loss": 0.0145,
      "step": 15900
    },
    {
      "epoch": 8.533333333333333,
      "grad_norm": 3.24643874168396,
      "learning_rate": 0.00014672,
      "loss": 0.0107,
      "step": 16000
    },
    {
      "epoch": 8.586666666666666,
      "grad_norm": 0.36351561546325684,
      "learning_rate": 0.00014138666666666666,
      "loss": 0.0201,
      "step": 16100
    },
    {
      "epoch": 8.64,
      "grad_norm": 0.22913512587547302,
      "learning_rate": 0.00013605333333333334,
      "loss": 0.0143,
      "step": 16200
    },
    {
      "epoch": 8.693333333333333,
      "grad_norm": 1.5531827211380005,
      "learning_rate": 0.00013072,
      "loss": 0.0118,
      "step": 16300
    },
    {
      "epoch": 8.746666666666666,
      "grad_norm": 0.022318247705698013,
      "learning_rate": 0.00012538666666666668,
      "loss": 0.0137,
      "step": 16400
    },
    {
      "epoch": 8.8,
      "grad_norm": 0.035695936530828476,
      "learning_rate": 0.00012005333333333333,
      "loss": 0.015,
      "step": 16500
    },
    {
      "epoch": 8.853333333333333,
      "grad_norm": 2.634281873703003,
      "learning_rate": 0.00011472,
      "loss": 0.0148,
      "step": 16600
    },
    {
      "epoch": 8.906666666666666,
      "grad_norm": 0.1686905473470688,
      "learning_rate": 0.00010938666666666667,
      "loss": 0.0166,
      "step": 16700
    },
    {
      "epoch": 8.96,
      "grad_norm": 5.633025646209717,
      "learning_rate": 0.00010405333333333334,
      "loss": 0.0135,
      "step": 16800
    },
    {
      "epoch": 9.013333333333334,
      "grad_norm": 0.012088228948414326,
      "learning_rate": 9.872e-05,
      "loss": 0.0086,
      "step": 16900
    },
    {
      "epoch": 9.066666666666666,
      "grad_norm": 0.04787912219762802,
      "learning_rate": 9.338666666666667e-05,
      "loss": 0.01,
      "step": 17000
    },
    {
      "epoch": 9.12,
      "grad_norm": 0.05025792121887207,
      "learning_rate": 8.805333333333333e-05,
      "loss": 0.0082,
      "step": 17100
    },
    {
      "epoch": 9.173333333333334,
      "grad_norm": 0.013630981557071209,
      "learning_rate": 8.272000000000001e-05,
      "loss": 0.0075,
      "step": 17200
    },
    {
      "epoch": 9.226666666666667,
      "grad_norm": 0.012331145815551281,
      "learning_rate": 7.738666666666668e-05,
      "loss": 0.0055,
      "step": 17300
    },
    {
      "epoch": 9.28,
      "grad_norm": 0.11972061544656754,
      "learning_rate": 7.205333333333333e-05,
      "loss": 0.005,
      "step": 17400
    },
    {
      "epoch": 9.333333333333334,
      "grad_norm": 0.7078624963760376,
      "learning_rate": 6.672e-05,
      "loss": 0.0073,
      "step": 17500
    },
    {
      "epoch": 9.386666666666667,
      "grad_norm": 0.9042026996612549,
      "learning_rate": 6.138666666666667e-05,
      "loss": 0.0116,
      "step": 17600
    },
    {
      "epoch": 9.44,
      "grad_norm": 0.007539211306720972,
      "learning_rate": 5.6053333333333334e-05,
      "loss": 0.0099,
      "step": 17700
    },
    {
      "epoch": 9.493333333333334,
      "grad_norm": 0.02406853251159191,
      "learning_rate": 5.072e-05,
      "loss": 0.0042,
      "step": 17800
    },
    {
      "epoch": 9.546666666666667,
      "grad_norm": 0.5386208295822144,
      "learning_rate": 4.5386666666666664e-05,
      "loss": 0.008,
      "step": 17900
    },
    {
      "epoch": 9.6,
      "grad_norm": 0.04790593311190605,
      "learning_rate": 4.005333333333334e-05,
      "loss": 0.0127,
      "step": 18000
    },
    {
      "epoch": 9.653333333333332,
      "grad_norm": 0.01434418000280857,
      "learning_rate": 3.472e-05,
      "loss": 0.0102,
      "step": 18100
    },
    {
      "epoch": 9.706666666666667,
      "grad_norm": 0.020770184695720673,
      "learning_rate": 2.9386666666666664e-05,
      "loss": 0.0061,
      "step": 18200
    },
    {
      "epoch": 9.76,
      "grad_norm": 1.1205120086669922,
      "learning_rate": 2.4053333333333332e-05,
      "loss": 0.0119,
      "step": 18300
    },
    {
      "epoch": 9.813333333333333,
      "grad_norm": 1.0783125162124634,
      "learning_rate": 1.872e-05,
      "loss": 0.0073,
      "step": 18400
    },
    {
      "epoch": 9.866666666666667,
      "grad_norm": 0.004055890254676342,
      "learning_rate": 1.3386666666666667e-05,
      "loss": 0.0091,
      "step": 18500
    }
  ],
  "logging_steps": 100,
  "max_steps": 18750,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 376298766336000.0,
  "train_batch_size": 64,
  "trial_name": null,
  "trial_params": null
}
