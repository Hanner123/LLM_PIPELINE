{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 9.866666666666667,
  "eval_steps": 500,
  "global_step": 18500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.05333333333333334,
      "grad_norm": 4.116649150848389,
      "learning_rate": 0.0009947200000000002,
      "loss": 0.5489,
      "step": 100
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 4.111152648925781,
      "learning_rate": 0.0009893866666666666,
      "loss": 0.3606,
      "step": 200
    },
    {
      "epoch": 0.16,
      "grad_norm": 1.2185876369476318,
      "learning_rate": 0.0009840533333333333,
      "loss": 0.3234,
      "step": 300
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 3.3149991035461426,
      "learning_rate": 0.00097872,
      "loss": 0.3199,
      "step": 400
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 3.576918125152588,
      "learning_rate": 0.0009733866666666667,
      "loss": 0.3014,
      "step": 500
    },
    {
      "epoch": 0.32,
      "grad_norm": 1.6276440620422363,
      "learning_rate": 0.0009680533333333333,
      "loss": 0.3089,
      "step": 600
    },
    {
      "epoch": 0.37333333333333335,
      "grad_norm": 1.7668952941894531,
      "learning_rate": 0.00096272,
      "loss": 0.3038,
      "step": 700
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 3.999793767929077,
      "learning_rate": 0.0009573866666666667,
      "loss": 0.2983,
      "step": 800
    },
    {
      "epoch": 0.48,
      "grad_norm": 4.877665996551514,
      "learning_rate": 0.0009520533333333333,
      "loss": 0.2882,
      "step": 900
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 2.691458225250244,
      "learning_rate": 0.0009467200000000001,
      "loss": 0.2935,
      "step": 1000
    },
    {
      "epoch": 0.5866666666666667,
      "grad_norm": 3.2908384799957275,
      "learning_rate": 0.0009413866666666668,
      "loss": 0.2911,
      "step": 1100
    },
    {
      "epoch": 0.64,
      "grad_norm": 4.553514003753662,
      "learning_rate": 0.0009360533333333333,
      "loss": 0.2628,
      "step": 1200
    },
    {
      "epoch": 0.6933333333333334,
      "grad_norm": 2.9596855640411377,
      "learning_rate": 0.00093072,
      "loss": 0.2554,
      "step": 1300
    },
    {
      "epoch": 0.7466666666666667,
      "grad_norm": 2.5759832859039307,
      "learning_rate": 0.0009253866666666667,
      "loss": 0.2736,
      "step": 1400
    },
    {
      "epoch": 0.8,
      "grad_norm": 2.431143283843994,
      "learning_rate": 0.0009200533333333333,
      "loss": 0.2557,
      "step": 1500
    },
    {
      "epoch": 0.8533333333333334,
      "grad_norm": 1.5707799196243286,
      "learning_rate": 0.0009147199999999999,
      "loss": 0.2647,
      "step": 1600
    },
    {
      "epoch": 0.9066666666666666,
      "grad_norm": 1.9914820194244385,
      "learning_rate": 0.0009093866666666667,
      "loss": 0.2482,
      "step": 1700
    },
    {
      "epoch": 0.96,
      "grad_norm": 3.2484264373779297,
      "learning_rate": 0.0009040533333333334,
      "loss": 0.2547,
      "step": 1800
    },
    {
      "epoch": 1.0133333333333334,
      "grad_norm": 0.9051880836486816,
      "learning_rate": 0.00089872,
      "loss": 0.2497,
      "step": 1900
    },
    {
      "epoch": 1.0666666666666667,
      "grad_norm": 1.1040949821472168,
      "learning_rate": 0.0008933866666666667,
      "loss": 0.1874,
      "step": 2000
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.39185774326324463,
      "learning_rate": 0.0008880533333333334,
      "loss": 0.1842,
      "step": 2100
    },
    {
      "epoch": 1.1733333333333333,
      "grad_norm": 1.6281388998031616,
      "learning_rate": 0.0008827199999999999,
      "loss": 0.1844,
      "step": 2200
    },
    {
      "epoch": 1.2266666666666666,
      "grad_norm": 2.5785815715789795,
      "learning_rate": 0.0008773866666666667,
      "loss": 0.1842,
      "step": 2300
    },
    {
      "epoch": 1.28,
      "grad_norm": 1.6152372360229492,
      "learning_rate": 0.0008720533333333334,
      "loss": 0.1805,
      "step": 2400
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 2.1841554641723633,
      "learning_rate": 0.0008667200000000001,
      "loss": 0.1822,
      "step": 2500
    },
    {
      "epoch": 1.3866666666666667,
      "grad_norm": 6.517370223999023,
      "learning_rate": 0.0008613866666666667,
      "loss": 0.1873,
      "step": 2600
    },
    {
      "epoch": 1.44,
      "grad_norm": 1.0988973379135132,
      "learning_rate": 0.0008560533333333333,
      "loss": 0.1927,
      "step": 2700
    },
    {
      "epoch": 1.4933333333333334,
      "grad_norm": 3.465580940246582,
      "learning_rate": 0.00085072,
      "loss": 0.1898,
      "step": 2800
    },
    {
      "epoch": 1.5466666666666666,
      "grad_norm": 1.2278897762298584,
      "learning_rate": 0.0008453866666666666,
      "loss": 0.1744,
      "step": 2900
    },
    {
      "epoch": 1.6,
      "grad_norm": 4.1446213722229,
      "learning_rate": 0.0008400533333333334,
      "loss": 0.1875,
      "step": 3000
    },
    {
      "epoch": 1.6533333333333333,
      "grad_norm": 1.3195955753326416,
      "learning_rate": 0.0008347200000000001,
      "loss": 0.1872,
      "step": 3100
    },
    {
      "epoch": 1.7066666666666666,
      "grad_norm": 2.138735055923462,
      "learning_rate": 0.0008293866666666667,
      "loss": 0.1824,
      "step": 3200
    },
    {
      "epoch": 1.76,
      "grad_norm": 0.8690288066864014,
      "learning_rate": 0.0008240533333333333,
      "loss": 0.1792,
      "step": 3300
    },
    {
      "epoch": 1.8133333333333335,
      "grad_norm": 3.1471314430236816,
      "learning_rate": 0.00081872,
      "loss": 0.1896,
      "step": 3400
    },
    {
      "epoch": 1.8666666666666667,
      "grad_norm": 1.0204195976257324,
      "learning_rate": 0.0008133866666666667,
      "loss": 0.188,
      "step": 3500
    },
    {
      "epoch": 1.92,
      "grad_norm": 1.9532541036605835,
      "learning_rate": 0.0008080533333333334,
      "loss": 0.2012,
      "step": 3600
    },
    {
      "epoch": 1.9733333333333334,
      "grad_norm": 1.6636450290679932,
      "learning_rate": 0.00080272,
      "loss": 0.1889,
      "step": 3700
    },
    {
      "epoch": 2.026666666666667,
      "grad_norm": 0.4571792185306549,
      "learning_rate": 0.0007973866666666667,
      "loss": 0.146,
      "step": 3800
    },
    {
      "epoch": 2.08,
      "grad_norm": 1.0593620538711548,
      "learning_rate": 0.0007920533333333334,
      "loss": 0.1221,
      "step": 3900
    },
    {
      "epoch": 2.1333333333333333,
      "grad_norm": 1.4202736616134644,
      "learning_rate": 0.00078672,
      "loss": 0.1221,
      "step": 4000
    },
    {
      "epoch": 2.1866666666666665,
      "grad_norm": 1.635491132736206,
      "learning_rate": 0.0007813866666666667,
      "loss": 0.1224,
      "step": 4100
    },
    {
      "epoch": 2.24,
      "grad_norm": 2.0856876373291016,
      "learning_rate": 0.0007760533333333333,
      "loss": 0.1305,
      "step": 4200
    },
    {
      "epoch": 2.2933333333333334,
      "grad_norm": 2.4449033737182617,
      "learning_rate": 0.00077072,
      "loss": 0.1243,
      "step": 4300
    },
    {
      "epoch": 2.3466666666666667,
      "grad_norm": 1.7750811576843262,
      "learning_rate": 0.0007653866666666667,
      "loss": 0.109,
      "step": 4400
    },
    {
      "epoch": 2.4,
      "grad_norm": 1.7830383777618408,
      "learning_rate": 0.0007600533333333334,
      "loss": 0.1525,
      "step": 4500
    },
    {
      "epoch": 2.453333333333333,
      "grad_norm": 1.9128587245941162,
      "learning_rate": 0.00075472,
      "loss": 0.1277,
      "step": 4600
    },
    {
      "epoch": 2.506666666666667,
      "grad_norm": 1.61978280544281,
      "learning_rate": 0.0007493866666666666,
      "loss": 0.1358,
      "step": 4700
    },
    {
      "epoch": 2.56,
      "grad_norm": 0.9865196347236633,
      "learning_rate": 0.0007440533333333333,
      "loss": 0.1322,
      "step": 4800
    },
    {
      "epoch": 2.6133333333333333,
      "grad_norm": 3.0089752674102783,
      "learning_rate": 0.0007387200000000001,
      "loss": 0.1421,
      "step": 4900
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 0.7202914953231812,
      "learning_rate": 0.0007333866666666667,
      "loss": 0.1279,
      "step": 5000
    },
    {
      "epoch": 2.7199999999999998,
      "grad_norm": 2.0538394451141357,
      "learning_rate": 0.0007280533333333334,
      "loss": 0.1385,
      "step": 5100
    },
    {
      "epoch": 2.7733333333333334,
      "grad_norm": 4.997796535491943,
      "learning_rate": 0.00072272,
      "loss": 0.1328,
      "step": 5200
    },
    {
      "epoch": 2.8266666666666667,
      "grad_norm": 2.763550043106079,
      "learning_rate": 0.0007173866666666666,
      "loss": 0.1266,
      "step": 5300
    },
    {
      "epoch": 2.88,
      "grad_norm": 3.5150094032287598,
      "learning_rate": 0.0007120533333333333,
      "loss": 0.1378,
      "step": 5400
    },
    {
      "epoch": 2.9333333333333336,
      "grad_norm": 1.719907283782959,
      "learning_rate": 0.00070672,
      "loss": 0.1383,
      "step": 5500
    },
    {
      "epoch": 2.986666666666667,
      "grad_norm": 2.3853142261505127,
      "learning_rate": 0.0007013866666666668,
      "loss": 0.1363,
      "step": 5600
    },
    {
      "epoch": 3.04,
      "grad_norm": 2.383309841156006,
      "learning_rate": 0.0006960533333333333,
      "loss": 0.0981,
      "step": 5700
    },
    {
      "epoch": 3.0933333333333333,
      "grad_norm": 3.1552467346191406,
      "learning_rate": 0.00069072,
      "loss": 0.0844,
      "step": 5800
    },
    {
      "epoch": 3.1466666666666665,
      "grad_norm": 1.239256501197815,
      "learning_rate": 0.0006853866666666667,
      "loss": 0.0917,
      "step": 5900
    },
    {
      "epoch": 3.2,
      "grad_norm": 1.9973405599594116,
      "learning_rate": 0.0006800533333333333,
      "loss": 0.092,
      "step": 6000
    },
    {
      "epoch": 3.2533333333333334,
      "grad_norm": 0.5894443988800049,
      "learning_rate": 0.00067472,
      "loss": 0.0903,
      "step": 6100
    },
    {
      "epoch": 3.3066666666666666,
      "grad_norm": 1.4477484226226807,
      "learning_rate": 0.0006693866666666666,
      "loss": 0.0978,
      "step": 6200
    },
    {
      "epoch": 3.36,
      "grad_norm": 1.2391031980514526,
      "learning_rate": 0.0006640533333333334,
      "loss": 0.0891,
      "step": 6300
    },
    {
      "epoch": 3.413333333333333,
      "grad_norm": 0.4530067443847656,
      "learning_rate": 0.00065872,
      "loss": 0.0929,
      "step": 6400
    },
    {
      "epoch": 3.466666666666667,
      "grad_norm": 0.3065347671508789,
      "learning_rate": 0.0006533866666666667,
      "loss": 0.0891,
      "step": 6500
    },
    {
      "epoch": 3.52,
      "grad_norm": 0.9807722568511963,
      "learning_rate": 0.0006480533333333334,
      "loss": 0.0962,
      "step": 6600
    },
    {
      "epoch": 3.5733333333333333,
      "grad_norm": 2.4410243034362793,
      "learning_rate": 0.0006427199999999999,
      "loss": 0.0914,
      "step": 6700
    },
    {
      "epoch": 3.626666666666667,
      "grad_norm": 0.9016589522361755,
      "learning_rate": 0.0006373866666666666,
      "loss": 0.1035,
      "step": 6800
    },
    {
      "epoch": 3.68,
      "grad_norm": 0.9682859182357788,
      "learning_rate": 0.0006320533333333334,
      "loss": 0.0844,
      "step": 6900
    },
    {
      "epoch": 3.7333333333333334,
      "grad_norm": 1.0709477663040161,
      "learning_rate": 0.0006267200000000001,
      "loss": 0.0989,
      "step": 7000
    },
    {
      "epoch": 3.7866666666666666,
      "grad_norm": 0.8643305897712708,
      "learning_rate": 0.0006213866666666667,
      "loss": 0.1025,
      "step": 7100
    },
    {
      "epoch": 3.84,
      "grad_norm": 1.3798342943191528,
      "learning_rate": 0.0006160533333333334,
      "loss": 0.0915,
      "step": 7200
    },
    {
      "epoch": 3.8933333333333335,
      "grad_norm": 0.3137708604335785,
      "learning_rate": 0.00061072,
      "loss": 0.1003,
      "step": 7300
    },
    {
      "epoch": 3.9466666666666668,
      "grad_norm": 4.663255214691162,
      "learning_rate": 0.0006053866666666666,
      "loss": 0.0852,
      "step": 7400
    },
    {
      "epoch": 4.0,
      "grad_norm": 0.7254210710525513,
      "learning_rate": 0.0006000533333333333,
      "loss": 0.0973,
      "step": 7500
    },
    {
      "epoch": 4.053333333333334,
      "grad_norm": 1.430753469467163,
      "learning_rate": 0.0005947200000000001,
      "loss": 0.0587,
      "step": 7600
    },
    {
      "epoch": 4.1066666666666665,
      "grad_norm": 0.9137749075889587,
      "learning_rate": 0.0005893866666666667,
      "loss": 0.0528,
      "step": 7700
    },
    {
      "epoch": 4.16,
      "grad_norm": 1.0173925161361694,
      "learning_rate": 0.0005840533333333333,
      "loss": 0.0614,
      "step": 7800
    },
    {
      "epoch": 4.213333333333333,
      "grad_norm": 0.22726738452911377,
      "learning_rate": 0.00057872,
      "loss": 0.054,
      "step": 7900
    },
    {
      "epoch": 4.266666666666667,
      "grad_norm": 1.492884635925293,
      "learning_rate": 0.0005733866666666667,
      "loss": 0.059,
      "step": 8000
    },
    {
      "epoch": 4.32,
      "grad_norm": 1.1902474164962769,
      "learning_rate": 0.0005680533333333333,
      "loss": 0.0731,
      "step": 8100
    },
    {
      "epoch": 4.373333333333333,
      "grad_norm": 0.5466612577438354,
      "learning_rate": 0.0005627200000000001,
      "loss": 0.0748,
      "step": 8200
    },
    {
      "epoch": 4.426666666666667,
      "grad_norm": 1.257187843322754,
      "learning_rate": 0.0005573866666666667,
      "loss": 0.0713,
      "step": 8300
    },
    {
      "epoch": 4.48,
      "grad_norm": 3.519174337387085,
      "learning_rate": 0.0005520533333333333,
      "loss": 0.0636,
      "step": 8400
    },
    {
      "epoch": 4.533333333333333,
      "grad_norm": 1.587515115737915,
      "learning_rate": 0.00054672,
      "loss": 0.0712,
      "step": 8500
    },
    {
      "epoch": 4.586666666666667,
      "grad_norm": 0.43023768067359924,
      "learning_rate": 0.0005413866666666667,
      "loss": 0.0645,
      "step": 8600
    },
    {
      "epoch": 4.64,
      "grad_norm": 2.1543891429901123,
      "learning_rate": 0.0005360533333333334,
      "loss": 0.0704,
      "step": 8700
    },
    {
      "epoch": 4.693333333333333,
      "grad_norm": 0.6715590357780457,
      "learning_rate": 0.0005307199999999999,
      "loss": 0.0691,
      "step": 8800
    },
    {
      "epoch": 4.746666666666667,
      "grad_norm": 0.43970414996147156,
      "learning_rate": 0.0005253866666666667,
      "loss": 0.0663,
      "step": 8900
    },
    {
      "epoch": 4.8,
      "grad_norm": 0.821118175983429,
      "learning_rate": 0.0005200533333333334,
      "loss": 0.0697,
      "step": 9000
    },
    {
      "epoch": 4.8533333333333335,
      "grad_norm": 1.890629529953003,
      "learning_rate": 0.00051472,
      "loss": 0.0631,
      "step": 9100
    },
    {
      "epoch": 4.906666666666666,
      "grad_norm": 0.8898472785949707,
      "learning_rate": 0.0005093866666666667,
      "loss": 0.0669,
      "step": 9200
    },
    {
      "epoch": 4.96,
      "grad_norm": 2.0345966815948486,
      "learning_rate": 0.0005040533333333333,
      "loss": 0.0706,
      "step": 9300
    },
    {
      "epoch": 5.013333333333334,
      "grad_norm": 1.4989537000656128,
      "learning_rate": 0.00049872,
      "loss": 0.0639,
      "step": 9400
    },
    {
      "epoch": 5.066666666666666,
      "grad_norm": 0.3067730665206909,
      "learning_rate": 0.0004933866666666667,
      "loss": 0.0406,
      "step": 9500
    },
    {
      "epoch": 5.12,
      "grad_norm": 0.11924272030591965,
      "learning_rate": 0.00048805333333333333,
      "loss": 0.0369,
      "step": 9600
    },
    {
      "epoch": 5.173333333333334,
      "grad_norm": 0.8163153529167175,
      "learning_rate": 0.00048272,
      "loss": 0.0412,
      "step": 9700
    },
    {
      "epoch": 5.226666666666667,
      "grad_norm": 0.16755391657352448,
      "learning_rate": 0.0004773866666666667,
      "loss": 0.0347,
      "step": 9800
    },
    {
      "epoch": 5.28,
      "grad_norm": 2.1846764087677,
      "learning_rate": 0.0004720533333333333,
      "loss": 0.0381,
      "step": 9900
    },
    {
      "epoch": 5.333333333333333,
      "grad_norm": 2.288041353225708,
      "learning_rate": 0.00046672000000000006,
      "loss": 0.0475,
      "step": 10000
    },
    {
      "epoch": 5.386666666666667,
      "grad_norm": 0.9137528538703918,
      "learning_rate": 0.0004613866666666667,
      "loss": 0.0443,
      "step": 10100
    },
    {
      "epoch": 5.44,
      "grad_norm": 1.120066523551941,
      "learning_rate": 0.0004560533333333333,
      "loss": 0.0438,
      "step": 10200
    },
    {
      "epoch": 5.493333333333333,
      "grad_norm": 0.47701865434646606,
      "learning_rate": 0.00045072,
      "loss": 0.0413,
      "step": 10300
    },
    {
      "epoch": 5.546666666666667,
      "grad_norm": 3.559443950653076,
      "learning_rate": 0.0004453866666666667,
      "loss": 0.047,
      "step": 10400
    },
    {
      "epoch": 5.6,
      "grad_norm": 0.3065274655818939,
      "learning_rate": 0.00044005333333333336,
      "loss": 0.0505,
      "step": 10500
    },
    {
      "epoch": 5.653333333333333,
      "grad_norm": 0.8538721799850464,
      "learning_rate": 0.00043472,
      "loss": 0.0452,
      "step": 10600
    },
    {
      "epoch": 5.706666666666667,
      "grad_norm": 1.0225770473480225,
      "learning_rate": 0.0004293866666666667,
      "loss": 0.0472,
      "step": 10700
    },
    {
      "epoch": 5.76,
      "grad_norm": 1.9242485761642456,
      "learning_rate": 0.00042405333333333335,
      "loss": 0.0632,
      "step": 10800
    },
    {
      "epoch": 5.8133333333333335,
      "grad_norm": 0.6690254211425781,
      "learning_rate": 0.00041872,
      "loss": 0.0419,
      "step": 10900
    },
    {
      "epoch": 5.866666666666667,
      "grad_norm": 1.80289888381958,
      "learning_rate": 0.0004133866666666667,
      "loss": 0.0538,
      "step": 11000
    },
    {
      "epoch": 5.92,
      "grad_norm": 4.172738552093506,
      "learning_rate": 0.00040805333333333334,
      "loss": 0.0424,
      "step": 11100
    },
    {
      "epoch": 5.973333333333334,
      "grad_norm": 0.828738808631897,
      "learning_rate": 0.00040272,
      "loss": 0.0439,
      "step": 11200
    },
    {
      "epoch": 6.026666666666666,
      "grad_norm": 0.14061547815799713,
      "learning_rate": 0.00039738666666666665,
      "loss": 0.0358,
      "step": 11300
    },
    {
      "epoch": 6.08,
      "grad_norm": 2.3795571327209473,
      "learning_rate": 0.00039205333333333333,
      "loss": 0.026,
      "step": 11400
    },
    {
      "epoch": 6.133333333333334,
      "grad_norm": 1.1359858512878418,
      "learning_rate": 0.00038672,
      "loss": 0.0258,
      "step": 11500
    },
    {
      "epoch": 6.1866666666666665,
      "grad_norm": 0.038568656891584396,
      "learning_rate": 0.00038138666666666664,
      "loss": 0.0339,
      "step": 11600
    },
    {
      "epoch": 6.24,
      "grad_norm": 3.044077157974243,
      "learning_rate": 0.0003760533333333334,
      "loss": 0.0252,
      "step": 11700
    },
    {
      "epoch": 6.293333333333333,
      "grad_norm": 1.477933406829834,
      "learning_rate": 0.00037072,
      "loss": 0.0333,
      "step": 11800
    },
    {
      "epoch": 6.346666666666667,
      "grad_norm": 0.5217610001564026,
      "learning_rate": 0.0003653866666666667,
      "loss": 0.0309,
      "step": 11900
    },
    {
      "epoch": 6.4,
      "grad_norm": 2.3302478790283203,
      "learning_rate": 0.00036005333333333336,
      "loss": 0.033,
      "step": 12000
    },
    {
      "epoch": 6.453333333333333,
      "grad_norm": 0.017027802765369415,
      "learning_rate": 0.00035472,
      "loss": 0.0306,
      "step": 12100
    },
    {
      "epoch": 6.506666666666667,
      "grad_norm": 0.03681832179427147,
      "learning_rate": 0.0003493866666666667,
      "loss": 0.041,
      "step": 12200
    },
    {
      "epoch": 6.5600000000000005,
      "grad_norm": 0.5394219160079956,
      "learning_rate": 0.00034405333333333336,
      "loss": 0.029,
      "step": 12300
    },
    {
      "epoch": 6.613333333333333,
      "grad_norm": 0.31361985206604004,
      "learning_rate": 0.00033872000000000004,
      "loss": 0.0353,
      "step": 12400
    },
    {
      "epoch": 6.666666666666667,
      "grad_norm": 1.0384165048599243,
      "learning_rate": 0.00033338666666666666,
      "loss": 0.0324,
      "step": 12500
    },
    {
      "epoch": 6.72,
      "grad_norm": 1.1364777088165283,
      "learning_rate": 0.0003280533333333333,
      "loss": 0.0322,
      "step": 12600
    },
    {
      "epoch": 6.773333333333333,
      "grad_norm": 0.6991569995880127,
      "learning_rate": 0.00032272000000000003,
      "loss": 0.0309,
      "step": 12700
    },
    {
      "epoch": 6.826666666666666,
      "grad_norm": 0.6136975288391113,
      "learning_rate": 0.00031738666666666665,
      "loss": 0.0416,
      "step": 12800
    },
    {
      "epoch": 6.88,
      "grad_norm": 0.28573212027549744,
      "learning_rate": 0.00031205333333333334,
      "loss": 0.031,
      "step": 12900
    },
    {
      "epoch": 6.933333333333334,
      "grad_norm": 0.5332845449447632,
      "learning_rate": 0.00030672,
      "loss": 0.0355,
      "step": 13000
    },
    {
      "epoch": 6.986666666666666,
      "grad_norm": 0.7360235452651978,
      "learning_rate": 0.0003013866666666667,
      "loss": 0.0339,
      "step": 13100
    },
    {
      "epoch": 7.04,
      "grad_norm": 1.2879623174667358,
      "learning_rate": 0.0002960533333333333,
      "loss": 0.019,
      "step": 13200
    },
    {
      "epoch": 7.093333333333334,
      "grad_norm": 1.4045823812484741,
      "learning_rate": 0.00029072,
      "loss": 0.0159,
      "step": 13300
    },
    {
      "epoch": 7.1466666666666665,
      "grad_norm": 0.3853466212749481,
      "learning_rate": 0.0002853866666666667,
      "loss": 0.0149,
      "step": 13400
    },
    {
      "epoch": 7.2,
      "grad_norm": 3.2228314876556396,
      "learning_rate": 0.0002800533333333333,
      "loss": 0.0191,
      "step": 13500
    },
    {
      "epoch": 7.253333333333333,
      "grad_norm": 0.02065885066986084,
      "learning_rate": 0.00027472,
      "loss": 0.014,
      "step": 13600
    },
    {
      "epoch": 7.306666666666667,
      "grad_norm": 1.8759796619415283,
      "learning_rate": 0.0002693866666666667,
      "loss": 0.0196,
      "step": 13700
    },
    {
      "epoch": 7.36,
      "grad_norm": 0.7913157939910889,
      "learning_rate": 0.0002640533333333333,
      "loss": 0.0236,
      "step": 13800
    },
    {
      "epoch": 7.413333333333333,
      "grad_norm": 0.254427969455719,
      "learning_rate": 0.00025872,
      "loss": 0.0217,
      "step": 13900
    },
    {
      "epoch": 7.466666666666667,
      "grad_norm": 1.0765103101730347,
      "learning_rate": 0.00025338666666666667,
      "loss": 0.0253,
      "step": 14000
    },
    {
      "epoch": 7.52,
      "grad_norm": 2.284684658050537,
      "learning_rate": 0.0002480533333333333,
      "loss": 0.0195,
      "step": 14100
    },
    {
      "epoch": 7.573333333333333,
      "grad_norm": 0.27118951082229614,
      "learning_rate": 0.00024272,
      "loss": 0.021,
      "step": 14200
    },
    {
      "epoch": 7.626666666666667,
      "grad_norm": 0.4912542402744293,
      "learning_rate": 0.00023738666666666666,
      "loss": 0.02,
      "step": 14300
    },
    {
      "epoch": 7.68,
      "grad_norm": 0.7133708000183105,
      "learning_rate": 0.00023205333333333334,
      "loss": 0.0196,
      "step": 14400
    },
    {
      "epoch": 7.733333333333333,
      "grad_norm": 0.5238533020019531,
      "learning_rate": 0.00022672,
      "loss": 0.021,
      "step": 14500
    },
    {
      "epoch": 7.786666666666667,
      "grad_norm": 5.1324920654296875,
      "learning_rate": 0.00022138666666666668,
      "loss": 0.022,
      "step": 14600
    },
    {
      "epoch": 7.84,
      "grad_norm": 0.034897081553936005,
      "learning_rate": 0.00021605333333333333,
      "loss": 0.0191,
      "step": 14700
    },
    {
      "epoch": 7.8933333333333335,
      "grad_norm": 0.3929056227207184,
      "learning_rate": 0.00021072,
      "loss": 0.0221,
      "step": 14800
    },
    {
      "epoch": 7.946666666666666,
      "grad_norm": 0.4736212491989136,
      "learning_rate": 0.00020538666666666667,
      "loss": 0.0238,
      "step": 14900
    },
    {
      "epoch": 8.0,
      "grad_norm": 0.45332786440849304,
      "learning_rate": 0.00020005333333333332,
      "loss": 0.015,
      "step": 15000
    },
    {
      "epoch": 8.053333333333333,
      "grad_norm": 0.4370117783546448,
      "learning_rate": 0.00019472,
      "loss": 0.0125,
      "step": 15100
    },
    {
      "epoch": 8.106666666666667,
      "grad_norm": 0.009704818949103355,
      "learning_rate": 0.0001893866666666667,
      "loss": 0.0159,
      "step": 15200
    },
    {
      "epoch": 8.16,
      "grad_norm": 2.67448353767395,
      "learning_rate": 0.00018405333333333334,
      "loss": 0.0095,
      "step": 15300
    },
    {
      "epoch": 8.213333333333333,
      "grad_norm": 0.0031313307117670774,
      "learning_rate": 0.00017872,
      "loss": 0.0164,
      "step": 15400
    },
    {
      "epoch": 8.266666666666667,
      "grad_norm": 0.42609041929244995,
      "learning_rate": 0.00017338666666666668,
      "loss": 0.0106,
      "step": 15500
    },
    {
      "epoch": 8.32,
      "grad_norm": 0.7183161377906799,
      "learning_rate": 0.00016805333333333333,
      "loss": 0.0138,
      "step": 15600
    },
    {
      "epoch": 8.373333333333333,
      "grad_norm": 0.21644136309623718,
      "learning_rate": 0.00016272000000000001,
      "loss": 0.0147,
      "step": 15700
    },
    {
      "epoch": 8.426666666666666,
      "grad_norm": 1.2747282981872559,
      "learning_rate": 0.00015738666666666667,
      "loss": 0.0129,
      "step": 15800
    },
    {
      "epoch": 8.48,
      "grad_norm": 1.7227662801742554,
      "learning_rate": 0.00015205333333333335,
      "loss": 0.0112,
      "step": 15900
    },
    {
      "epoch": 8.533333333333333,
      "grad_norm": 0.00962937343865633,
      "learning_rate": 0.00014672,
      "loss": 0.0081,
      "step": 16000
    },
    {
      "epoch": 8.586666666666666,
      "grad_norm": 0.14410656690597534,
      "learning_rate": 0.00014138666666666666,
      "loss": 0.0097,
      "step": 16100
    },
    {
      "epoch": 8.64,
      "grad_norm": 0.033286452293395996,
      "learning_rate": 0.00013605333333333334,
      "loss": 0.0134,
      "step": 16200
    },
    {
      "epoch": 8.693333333333333,
      "grad_norm": 0.5533082485198975,
      "learning_rate": 0.00013072,
      "loss": 0.0143,
      "step": 16300
    },
    {
      "epoch": 8.746666666666666,
      "grad_norm": 2.2840771675109863,
      "learning_rate": 0.00012538666666666668,
      "loss": 0.0137,
      "step": 16400
    },
    {
      "epoch": 8.8,
      "grad_norm": 0.03674876689910889,
      "learning_rate": 0.00012005333333333333,
      "loss": 0.0129,
      "step": 16500
    },
    {
      "epoch": 8.853333333333333,
      "grad_norm": 7.819170951843262,
      "learning_rate": 0.00011472,
      "loss": 0.0161,
      "step": 16600
    },
    {
      "epoch": 8.906666666666666,
      "grad_norm": 0.02454235404729843,
      "learning_rate": 0.00010938666666666667,
      "loss": 0.013,
      "step": 16700
    },
    {
      "epoch": 8.96,
      "grad_norm": 4.8320698738098145,
      "learning_rate": 0.00010405333333333334,
      "loss": 0.0143,
      "step": 16800
    },
    {
      "epoch": 9.013333333333334,
      "grad_norm": 0.00588849326595664,
      "learning_rate": 9.872e-05,
      "loss": 0.0109,
      "step": 16900
    },
    {
      "epoch": 9.066666666666666,
      "grad_norm": 0.377204567193985,
      "learning_rate": 9.338666666666667e-05,
      "loss": 0.008,
      "step": 17000
    },
    {
      "epoch": 9.12,
      "grad_norm": 2.626328229904175,
      "learning_rate": 8.805333333333333e-05,
      "loss": 0.0055,
      "step": 17100
    },
    {
      "epoch": 9.173333333333334,
      "grad_norm": 0.020365161821246147,
      "learning_rate": 8.272000000000001e-05,
      "loss": 0.0079,
      "step": 17200
    },
    {
      "epoch": 9.226666666666667,
      "grad_norm": 0.02065722830593586,
      "learning_rate": 7.738666666666668e-05,
      "loss": 0.0083,
      "step": 17300
    },
    {
      "epoch": 9.28,
      "grad_norm": 0.0024557295255362988,
      "learning_rate": 7.205333333333333e-05,
      "loss": 0.0079,
      "step": 17400
    },
    {
      "epoch": 9.333333333333334,
      "grad_norm": 0.07539521902799606,
      "learning_rate": 6.672e-05,
      "loss": 0.0095,
      "step": 17500
    },
    {
      "epoch": 9.386666666666667,
      "grad_norm": 0.00675989268347621,
      "learning_rate": 6.138666666666667e-05,
      "loss": 0.0127,
      "step": 17600
    },
    {
      "epoch": 9.44,
      "grad_norm": 0.007681012619286776,
      "learning_rate": 5.6053333333333334e-05,
      "loss": 0.0083,
      "step": 17700
    },
    {
      "epoch": 9.493333333333334,
      "grad_norm": 0.03302263841032982,
      "learning_rate": 5.072e-05,
      "loss": 0.0079,
      "step": 17800
    },
    {
      "epoch": 9.546666666666667,
      "grad_norm": 1.8145549297332764,
      "learning_rate": 4.5386666666666664e-05,
      "loss": 0.0073,
      "step": 17900
    },
    {
      "epoch": 9.6,
      "grad_norm": 0.21074792742729187,
      "learning_rate": 4.005333333333334e-05,
      "loss": 0.0091,
      "step": 18000
    },
    {
      "epoch": 9.653333333333332,
      "grad_norm": 0.002851247088983655,
      "learning_rate": 3.472e-05,
      "loss": 0.0058,
      "step": 18100
    },
    {
      "epoch": 9.706666666666667,
      "grad_norm": 1.9352610111236572,
      "learning_rate": 2.9386666666666664e-05,
      "loss": 0.0068,
      "step": 18200
    },
    {
      "epoch": 9.76,
      "grad_norm": 0.15379101037979126,
      "learning_rate": 2.4053333333333332e-05,
      "loss": 0.0086,
      "step": 18300
    },
    {
      "epoch": 9.813333333333333,
      "grad_norm": 0.08686574548482895,
      "learning_rate": 1.872e-05,
      "loss": 0.0085,
      "step": 18400
    },
    {
      "epoch": 9.866666666666667,
      "grad_norm": 0.004441377706825733,
      "learning_rate": 1.3386666666666667e-05,
      "loss": 0.0065,
      "step": 18500
    }
  ],
  "logging_steps": 100,
  "max_steps": 18750,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 376298766336000.0,
  "train_batch_size": 64,
  "trial_name": null,
  "trial_params": null
}
