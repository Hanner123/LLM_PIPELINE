{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 9.933333333333334,
  "eval_steps": 500,
  "global_step": 74500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.013333333333333334,
      "grad_norm": 2.54131817817688,
      "learning_rate": 1.99736e-05,
      "loss": 1.3548,
      "step": 100
    },
    {
      "epoch": 0.02666666666666667,
      "grad_norm": 3.531611680984497,
      "learning_rate": 1.9946933333333337e-05,
      "loss": 1.2509,
      "step": 200
    },
    {
      "epoch": 0.04,
      "grad_norm": 2.942603349685669,
      "learning_rate": 1.9920266666666666e-05,
      "loss": 1.1075,
      "step": 300
    },
    {
      "epoch": 0.05333333333333334,
      "grad_norm": 3.8454320430755615,
      "learning_rate": 1.9893600000000002e-05,
      "loss": 0.9796,
      "step": 400
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 4.337047576904297,
      "learning_rate": 1.9866933333333335e-05,
      "loss": 0.8599,
      "step": 500
    },
    {
      "epoch": 0.08,
      "grad_norm": 3.3056540489196777,
      "learning_rate": 1.9840266666666667e-05,
      "loss": 0.771,
      "step": 600
    },
    {
      "epoch": 0.09333333333333334,
      "grad_norm": 5.3122477531433105,
      "learning_rate": 1.9813600000000003e-05,
      "loss": 0.684,
      "step": 700
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 4.770808219909668,
      "learning_rate": 1.9786933333333335e-05,
      "loss": 0.5986,
      "step": 800
    },
    {
      "epoch": 0.12,
      "grad_norm": 3.025578737258911,
      "learning_rate": 1.9760266666666668e-05,
      "loss": 0.576,
      "step": 900
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 4.994915962219238,
      "learning_rate": 1.97336e-05,
      "loss": 0.5163,
      "step": 1000
    },
    {
      "epoch": 0.14666666666666667,
      "grad_norm": 5.875985622406006,
      "learning_rate": 1.9706933333333336e-05,
      "loss": 0.4934,
      "step": 1100
    },
    {
      "epoch": 0.16,
      "grad_norm": 5.904632091522217,
      "learning_rate": 1.968026666666667e-05,
      "loss": 0.4761,
      "step": 1200
    },
    {
      "epoch": 0.17333333333333334,
      "grad_norm": 3.606194019317627,
      "learning_rate": 1.96536e-05,
      "loss": 0.4341,
      "step": 1300
    },
    {
      "epoch": 0.18666666666666668,
      "grad_norm": 8.791433334350586,
      "learning_rate": 1.9626933333333337e-05,
      "loss": 0.45,
      "step": 1400
    },
    {
      "epoch": 0.2,
      "grad_norm": 7.162921905517578,
      "learning_rate": 1.9600266666666666e-05,
      "loss": 0.4178,
      "step": 1500
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 7.719866752624512,
      "learning_rate": 1.9573600000000002e-05,
      "loss": 0.4084,
      "step": 1600
    },
    {
      "epoch": 0.22666666666666666,
      "grad_norm": 4.047729015350342,
      "learning_rate": 1.9546933333333334e-05,
      "loss": 0.4025,
      "step": 1700
    },
    {
      "epoch": 0.24,
      "grad_norm": 5.131913661956787,
      "learning_rate": 1.9520266666666667e-05,
      "loss": 0.3964,
      "step": 1800
    },
    {
      "epoch": 0.25333333333333335,
      "grad_norm": 3.4550702571868896,
      "learning_rate": 1.9493600000000003e-05,
      "loss": 0.3925,
      "step": 1900
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 11.44594669342041,
      "learning_rate": 1.9466933333333335e-05,
      "loss": 0.339,
      "step": 2000
    },
    {
      "epoch": 0.28,
      "grad_norm": 11.08743667602539,
      "learning_rate": 1.9440266666666667e-05,
      "loss": 0.3704,
      "step": 2100
    },
    {
      "epoch": 0.29333333333333333,
      "grad_norm": 4.184300899505615,
      "learning_rate": 1.94136e-05,
      "loss": 0.3778,
      "step": 2200
    },
    {
      "epoch": 0.30666666666666664,
      "grad_norm": 5.253096580505371,
      "learning_rate": 1.9386933333333336e-05,
      "loss": 0.3794,
      "step": 2300
    },
    {
      "epoch": 0.32,
      "grad_norm": 7.888819217681885,
      "learning_rate": 1.9360266666666668e-05,
      "loss": 0.3397,
      "step": 2400
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.8920400142669678,
      "learning_rate": 1.93336e-05,
      "loss": 0.3436,
      "step": 2500
    },
    {
      "epoch": 0.3466666666666667,
      "grad_norm": 13.248079299926758,
      "learning_rate": 1.9306933333333337e-05,
      "loss": 0.3855,
      "step": 2600
    },
    {
      "epoch": 0.36,
      "grad_norm": 12.9493989944458,
      "learning_rate": 1.928026666666667e-05,
      "loss": 0.3378,
      "step": 2700
    },
    {
      "epoch": 0.37333333333333335,
      "grad_norm": 10.1876220703125,
      "learning_rate": 1.92536e-05,
      "loss": 0.3641,
      "step": 2800
    },
    {
      "epoch": 0.38666666666666666,
      "grad_norm": 8.8018798828125,
      "learning_rate": 1.9226933333333334e-05,
      "loss": 0.3498,
      "step": 2900
    },
    {
      "epoch": 0.4,
      "grad_norm": 6.915132999420166,
      "learning_rate": 1.920026666666667e-05,
      "loss": 0.3674,
      "step": 3000
    },
    {
      "epoch": 0.41333333333333333,
      "grad_norm": 20.040193557739258,
      "learning_rate": 1.9173600000000002e-05,
      "loss": 0.3297,
      "step": 3100
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 8.692453384399414,
      "learning_rate": 1.9146933333333335e-05,
      "loss": 0.3147,
      "step": 3200
    },
    {
      "epoch": 0.44,
      "grad_norm": 13.60999584197998,
      "learning_rate": 1.912026666666667e-05,
      "loss": 0.3055,
      "step": 3300
    },
    {
      "epoch": 0.4533333333333333,
      "grad_norm": 10.760847091674805,
      "learning_rate": 1.90936e-05,
      "loss": 0.3125,
      "step": 3400
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 9.814315795898438,
      "learning_rate": 1.9066933333333335e-05,
      "loss": 0.3646,
      "step": 3500
    },
    {
      "epoch": 0.48,
      "grad_norm": 9.610437393188477,
      "learning_rate": 1.9040266666666668e-05,
      "loss": 0.3165,
      "step": 3600
    },
    {
      "epoch": 0.49333333333333335,
      "grad_norm": 10.295694351196289,
      "learning_rate": 1.90136e-05,
      "loss": 0.3567,
      "step": 3700
    },
    {
      "epoch": 0.5066666666666667,
      "grad_norm": 9.511425018310547,
      "learning_rate": 1.8986933333333336e-05,
      "loss": 0.3316,
      "step": 3800
    },
    {
      "epoch": 0.52,
      "grad_norm": 4.291813850402832,
      "learning_rate": 1.896026666666667e-05,
      "loss": 0.3358,
      "step": 3900
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 45.70459747314453,
      "learning_rate": 1.89336e-05,
      "loss": 0.3305,
      "step": 4000
    },
    {
      "epoch": 0.5466666666666666,
      "grad_norm": 6.986459255218506,
      "learning_rate": 1.8906933333333334e-05,
      "loss": 0.3332,
      "step": 4100
    },
    {
      "epoch": 0.56,
      "grad_norm": 5.10563325881958,
      "learning_rate": 1.888026666666667e-05,
      "loss": 0.3273,
      "step": 4200
    },
    {
      "epoch": 0.5733333333333334,
      "grad_norm": 14.179991722106934,
      "learning_rate": 1.8853600000000002e-05,
      "loss": 0.3365,
      "step": 4300
    },
    {
      "epoch": 0.5866666666666667,
      "grad_norm": 13.375234603881836,
      "learning_rate": 1.8826933333333334e-05,
      "loss": 0.3453,
      "step": 4400
    },
    {
      "epoch": 0.6,
      "grad_norm": 9.821931838989258,
      "learning_rate": 1.880026666666667e-05,
      "loss": 0.3038,
      "step": 4500
    },
    {
      "epoch": 0.6133333333333333,
      "grad_norm": 1.9740771055221558,
      "learning_rate": 1.87736e-05,
      "loss": 0.3063,
      "step": 4600
    },
    {
      "epoch": 0.6266666666666667,
      "grad_norm": 7.29232931137085,
      "learning_rate": 1.8746933333333335e-05,
      "loss": 0.3179,
      "step": 4700
    },
    {
      "epoch": 0.64,
      "grad_norm": 10.913359642028809,
      "learning_rate": 1.8720266666666668e-05,
      "loss": 0.3066,
      "step": 4800
    },
    {
      "epoch": 0.6533333333333333,
      "grad_norm": 6.384754180908203,
      "learning_rate": 1.86936e-05,
      "loss": 0.293,
      "step": 4900
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 6.205984592437744,
      "learning_rate": 1.8666933333333336e-05,
      "loss": 0.2878,
      "step": 5000
    },
    {
      "epoch": 0.68,
      "grad_norm": 7.905359745025635,
      "learning_rate": 1.864026666666667e-05,
      "loss": 0.327,
      "step": 5100
    },
    {
      "epoch": 0.6933333333333334,
      "grad_norm": 1.223920464515686,
      "learning_rate": 1.86136e-05,
      "loss": 0.2674,
      "step": 5200
    },
    {
      "epoch": 0.7066666666666667,
      "grad_norm": 16.47577667236328,
      "learning_rate": 1.8586933333333333e-05,
      "loss": 0.3336,
      "step": 5300
    },
    {
      "epoch": 0.72,
      "grad_norm": 16.142824172973633,
      "learning_rate": 1.856026666666667e-05,
      "loss": 0.3097,
      "step": 5400
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 2.063720703125,
      "learning_rate": 1.85336e-05,
      "loss": 0.2955,
      "step": 5500
    },
    {
      "epoch": 0.7466666666666667,
      "grad_norm": 5.224261283874512,
      "learning_rate": 1.8506933333333334e-05,
      "loss": 0.3319,
      "step": 5600
    },
    {
      "epoch": 0.76,
      "grad_norm": 14.721929550170898,
      "learning_rate": 1.848026666666667e-05,
      "loss": 0.3094,
      "step": 5700
    },
    {
      "epoch": 0.7733333333333333,
      "grad_norm": 8.677061080932617,
      "learning_rate": 1.84536e-05,
      "loss": 0.3078,
      "step": 5800
    },
    {
      "epoch": 0.7866666666666666,
      "grad_norm": 5.134155750274658,
      "learning_rate": 1.8426933333333335e-05,
      "loss": 0.2752,
      "step": 5900
    },
    {
      "epoch": 0.8,
      "grad_norm": 8.978708267211914,
      "learning_rate": 1.8400266666666667e-05,
      "loss": 0.2953,
      "step": 6000
    },
    {
      "epoch": 0.8133333333333334,
      "grad_norm": 2.1311986446380615,
      "learning_rate": 1.8373600000000003e-05,
      "loss": 0.2879,
      "step": 6100
    },
    {
      "epoch": 0.8266666666666667,
      "grad_norm": 3.8941640853881836,
      "learning_rate": 1.8346933333333336e-05,
      "loss": 0.2925,
      "step": 6200
    },
    {
      "epoch": 0.84,
      "grad_norm": 7.601271152496338,
      "learning_rate": 1.8320266666666668e-05,
      "loss": 0.3107,
      "step": 6300
    },
    {
      "epoch": 0.8533333333333334,
      "grad_norm": 18.87073516845703,
      "learning_rate": 1.8293600000000004e-05,
      "loss": 0.3109,
      "step": 6400
    },
    {
      "epoch": 0.8666666666666667,
      "grad_norm": 3.978943109512329,
      "learning_rate": 1.8266933333333333e-05,
      "loss": 0.3169,
      "step": 6500
    },
    {
      "epoch": 0.88,
      "grad_norm": 6.464085578918457,
      "learning_rate": 1.824026666666667e-05,
      "loss": 0.3219,
      "step": 6600
    },
    {
      "epoch": 0.8933333333333333,
      "grad_norm": 16.587236404418945,
      "learning_rate": 1.82136e-05,
      "loss": 0.3038,
      "step": 6700
    },
    {
      "epoch": 0.9066666666666666,
      "grad_norm": 4.674067497253418,
      "learning_rate": 1.8186933333333334e-05,
      "loss": 0.2942,
      "step": 6800
    },
    {
      "epoch": 0.92,
      "grad_norm": 3.1598899364471436,
      "learning_rate": 1.816026666666667e-05,
      "loss": 0.3191,
      "step": 6900
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 9.530579566955566,
      "learning_rate": 1.8133600000000002e-05,
      "loss": 0.2733,
      "step": 7000
    },
    {
      "epoch": 0.9466666666666667,
      "grad_norm": 2.1163601875305176,
      "learning_rate": 1.8106933333333334e-05,
      "loss": 0.2977,
      "step": 7100
    },
    {
      "epoch": 0.96,
      "grad_norm": 12.54494857788086,
      "learning_rate": 1.8080266666666667e-05,
      "loss": 0.3086,
      "step": 7200
    },
    {
      "epoch": 0.9733333333333334,
      "grad_norm": 3.9907636642456055,
      "learning_rate": 1.8053600000000003e-05,
      "loss": 0.3173,
      "step": 7300
    },
    {
      "epoch": 0.9866666666666667,
      "grad_norm": 10.456136703491211,
      "learning_rate": 1.8026933333333335e-05,
      "loss": 0.3299,
      "step": 7400
    },
    {
      "epoch": 1.0,
      "grad_norm": 13.809103012084961,
      "learning_rate": 1.8000266666666668e-05,
      "loss": 0.2977,
      "step": 7500
    },
    {
      "epoch": 1.0133333333333334,
      "grad_norm": 6.660788059234619,
      "learning_rate": 1.7973600000000004e-05,
      "loss": 0.2667,
      "step": 7600
    },
    {
      "epoch": 1.0266666666666666,
      "grad_norm": 14.810029983520508,
      "learning_rate": 1.7946933333333333e-05,
      "loss": 0.2447,
      "step": 7700
    },
    {
      "epoch": 1.04,
      "grad_norm": 11.93407917022705,
      "learning_rate": 1.792026666666667e-05,
      "loss": 0.278,
      "step": 7800
    },
    {
      "epoch": 1.0533333333333332,
      "grad_norm": 3.613368272781372,
      "learning_rate": 1.78936e-05,
      "loss": 0.2716,
      "step": 7900
    },
    {
      "epoch": 1.0666666666666667,
      "grad_norm": 11.262245178222656,
      "learning_rate": 1.7866933333333333e-05,
      "loss": 0.2739,
      "step": 8000
    },
    {
      "epoch": 1.08,
      "grad_norm": 13.774404525756836,
      "learning_rate": 1.784026666666667e-05,
      "loss": 0.2898,
      "step": 8100
    },
    {
      "epoch": 1.0933333333333333,
      "grad_norm": 5.080472946166992,
      "learning_rate": 1.7813600000000002e-05,
      "loss": 0.284,
      "step": 8200
    },
    {
      "epoch": 1.1066666666666667,
      "grad_norm": 5.4116082191467285,
      "learning_rate": 1.7786933333333334e-05,
      "loss": 0.2739,
      "step": 8300
    },
    {
      "epoch": 1.12,
      "grad_norm": 9.547252655029297,
      "learning_rate": 1.7760266666666667e-05,
      "loss": 0.291,
      "step": 8400
    },
    {
      "epoch": 1.1333333333333333,
      "grad_norm": 12.86894702911377,
      "learning_rate": 1.7733600000000002e-05,
      "loss": 0.3039,
      "step": 8500
    },
    {
      "epoch": 1.1466666666666667,
      "grad_norm": 7.91793155670166,
      "learning_rate": 1.7706933333333335e-05,
      "loss": 0.2683,
      "step": 8600
    },
    {
      "epoch": 1.16,
      "grad_norm": 27.039854049682617,
      "learning_rate": 1.7680266666666667e-05,
      "loss": 0.2716,
      "step": 8700
    },
    {
      "epoch": 1.1733333333333333,
      "grad_norm": 7.64992094039917,
      "learning_rate": 1.7653600000000003e-05,
      "loss": 0.2779,
      "step": 8800
    },
    {
      "epoch": 1.1866666666666668,
      "grad_norm": 5.89465856552124,
      "learning_rate": 1.7626933333333332e-05,
      "loss": 0.2521,
      "step": 8900
    },
    {
      "epoch": 1.2,
      "grad_norm": 15.1021728515625,
      "learning_rate": 1.7600266666666668e-05,
      "loss": 0.271,
      "step": 9000
    },
    {
      "epoch": 1.2133333333333334,
      "grad_norm": 9.212249755859375,
      "learning_rate": 1.75736e-05,
      "loss": 0.2768,
      "step": 9100
    },
    {
      "epoch": 1.2266666666666666,
      "grad_norm": 19.20562744140625,
      "learning_rate": 1.7546933333333336e-05,
      "loss": 0.261,
      "step": 9200
    },
    {
      "epoch": 1.24,
      "grad_norm": 17.32509994506836,
      "learning_rate": 1.752026666666667e-05,
      "loss": 0.2718,
      "step": 9300
    },
    {
      "epoch": 1.2533333333333334,
      "grad_norm": 29.829105377197266,
      "learning_rate": 1.74936e-05,
      "loss": 0.2648,
      "step": 9400
    },
    {
      "epoch": 1.2666666666666666,
      "grad_norm": 3.4934885501861572,
      "learning_rate": 1.7466933333333337e-05,
      "loss": 0.2939,
      "step": 9500
    },
    {
      "epoch": 1.28,
      "grad_norm": 7.259252548217773,
      "learning_rate": 1.7440266666666666e-05,
      "loss": 0.2592,
      "step": 9600
    },
    {
      "epoch": 1.2933333333333334,
      "grad_norm": 7.918870449066162,
      "learning_rate": 1.7413600000000002e-05,
      "loss": 0.2909,
      "step": 9700
    },
    {
      "epoch": 1.3066666666666666,
      "grad_norm": 10.395539283752441,
      "learning_rate": 1.7386933333333335e-05,
      "loss": 0.2779,
      "step": 9800
    },
    {
      "epoch": 1.32,
      "grad_norm": 5.865448474884033,
      "learning_rate": 1.7360266666666667e-05,
      "loss": 0.2704,
      "step": 9900
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 7.704156398773193,
      "learning_rate": 1.7333600000000003e-05,
      "loss": 0.2678,
      "step": 10000
    },
    {
      "epoch": 1.3466666666666667,
      "grad_norm": 1.9092975854873657,
      "learning_rate": 1.7306933333333335e-05,
      "loss": 0.2864,
      "step": 10100
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 5.983145713806152,
      "learning_rate": 1.7280266666666668e-05,
      "loss": 0.2727,
      "step": 10200
    },
    {
      "epoch": 1.3733333333333333,
      "grad_norm": 24.19906234741211,
      "learning_rate": 1.72536e-05,
      "loss": 0.2552,
      "step": 10300
    },
    {
      "epoch": 1.3866666666666667,
      "grad_norm": 15.10512924194336,
      "learning_rate": 1.7226933333333336e-05,
      "loss": 0.2617,
      "step": 10400
    },
    {
      "epoch": 1.4,
      "grad_norm": 11.83218765258789,
      "learning_rate": 1.720026666666667e-05,
      "loss": 0.2491,
      "step": 10500
    },
    {
      "epoch": 1.4133333333333333,
      "grad_norm": 8.723553657531738,
      "learning_rate": 1.71736e-05,
      "loss": 0.266,
      "step": 10600
    },
    {
      "epoch": 1.4266666666666667,
      "grad_norm": 2.233884572982788,
      "learning_rate": 1.7146933333333337e-05,
      "loss": 0.2691,
      "step": 10700
    },
    {
      "epoch": 1.44,
      "grad_norm": 3.3646180629730225,
      "learning_rate": 1.7120266666666666e-05,
      "loss": 0.2768,
      "step": 10800
    },
    {
      "epoch": 1.4533333333333334,
      "grad_norm": 2.707979202270508,
      "learning_rate": 1.7093600000000002e-05,
      "loss": 0.28,
      "step": 10900
    },
    {
      "epoch": 1.4666666666666668,
      "grad_norm": 20.682809829711914,
      "learning_rate": 1.7066933333333334e-05,
      "loss": 0.301,
      "step": 11000
    },
    {
      "epoch": 1.48,
      "grad_norm": 3.258512020111084,
      "learning_rate": 1.7040266666666667e-05,
      "loss": 0.2424,
      "step": 11100
    },
    {
      "epoch": 1.4933333333333334,
      "grad_norm": 3.7875993251800537,
      "learning_rate": 1.7013600000000003e-05,
      "loss": 0.2314,
      "step": 11200
    },
    {
      "epoch": 1.5066666666666668,
      "grad_norm": 2.9165570735931396,
      "learning_rate": 1.6986933333333335e-05,
      "loss": 0.2407,
      "step": 11300
    },
    {
      "epoch": 1.52,
      "grad_norm": 8.642885208129883,
      "learning_rate": 1.6960266666666668e-05,
      "loss": 0.2202,
      "step": 11400
    },
    {
      "epoch": 1.5333333333333332,
      "grad_norm": 15.700501441955566,
      "learning_rate": 1.69336e-05,
      "loss": 0.2774,
      "step": 11500
    },
    {
      "epoch": 1.5466666666666666,
      "grad_norm": 5.855369567871094,
      "learning_rate": 1.6906933333333336e-05,
      "loss": 0.2818,
      "step": 11600
    },
    {
      "epoch": 1.56,
      "grad_norm": 10.792997360229492,
      "learning_rate": 1.6880266666666668e-05,
      "loss": 0.2775,
      "step": 11700
    },
    {
      "epoch": 1.5733333333333333,
      "grad_norm": 8.55409049987793,
      "learning_rate": 1.68536e-05,
      "loss": 0.2594,
      "step": 11800
    },
    {
      "epoch": 1.5866666666666667,
      "grad_norm": 10.444816589355469,
      "learning_rate": 1.6826933333333337e-05,
      "loss": 0.288,
      "step": 11900
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.5631774663925171,
      "learning_rate": 1.6800266666666666e-05,
      "loss": 0.2757,
      "step": 12000
    },
    {
      "epoch": 1.6133333333333333,
      "grad_norm": 0.553144633769989,
      "learning_rate": 1.67736e-05,
      "loss": 0.2602,
      "step": 12100
    },
    {
      "epoch": 1.6266666666666667,
      "grad_norm": 7.710902690887451,
      "learning_rate": 1.6746933333333334e-05,
      "loss": 0.2326,
      "step": 12200
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 24.824644088745117,
      "learning_rate": 1.672026666666667e-05,
      "loss": 0.2713,
      "step": 12300
    },
    {
      "epoch": 1.6533333333333333,
      "grad_norm": 2.9188153743743896,
      "learning_rate": 1.6693600000000002e-05,
      "loss": 0.2531,
      "step": 12400
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 8.5508451461792,
      "learning_rate": 1.6666933333333335e-05,
      "loss": 0.2696,
      "step": 12500
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 7.33681583404541,
      "learning_rate": 1.664026666666667e-05,
      "loss": 0.2498,
      "step": 12600
    },
    {
      "epoch": 1.6933333333333334,
      "grad_norm": 0.5685939192771912,
      "learning_rate": 1.66136e-05,
      "loss": 0.2743,
      "step": 12700
    },
    {
      "epoch": 1.7066666666666666,
      "grad_norm": 5.672163963317871,
      "learning_rate": 1.6586933333333335e-05,
      "loss": 0.2869,
      "step": 12800
    },
    {
      "epoch": 1.72,
      "grad_norm": 5.850165367126465,
      "learning_rate": 1.6560266666666668e-05,
      "loss": 0.2559,
      "step": 12900
    },
    {
      "epoch": 1.7333333333333334,
      "grad_norm": 0.41187819838523865,
      "learning_rate": 1.65336e-05,
      "loss": 0.2522,
      "step": 13000
    },
    {
      "epoch": 1.7466666666666666,
      "grad_norm": 30.08721160888672,
      "learning_rate": 1.6506933333333336e-05,
      "loss": 0.2739,
      "step": 13100
    },
    {
      "epoch": 1.76,
      "grad_norm": 5.646122455596924,
      "learning_rate": 1.648026666666667e-05,
      "loss": 0.2582,
      "step": 13200
    },
    {
      "epoch": 1.7733333333333334,
      "grad_norm": 0.6093108057975769,
      "learning_rate": 1.64536e-05,
      "loss": 0.2738,
      "step": 13300
    },
    {
      "epoch": 1.7866666666666666,
      "grad_norm": 10.609749794006348,
      "learning_rate": 1.6426933333333334e-05,
      "loss": 0.2803,
      "step": 13400
    },
    {
      "epoch": 1.8,
      "grad_norm": 12.414752960205078,
      "learning_rate": 1.640026666666667e-05,
      "loss": 0.267,
      "step": 13500
    },
    {
      "epoch": 1.8133333333333335,
      "grad_norm": 4.110555171966553,
      "learning_rate": 1.6373600000000002e-05,
      "loss": 0.2537,
      "step": 13600
    },
    {
      "epoch": 1.8266666666666667,
      "grad_norm": 4.481592655181885,
      "learning_rate": 1.6346933333333334e-05,
      "loss": 0.2381,
      "step": 13700
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 2.5386877059936523,
      "learning_rate": 1.632026666666667e-05,
      "loss": 0.2548,
      "step": 13800
    },
    {
      "epoch": 1.8533333333333335,
      "grad_norm": 4.4769606590271,
      "learning_rate": 1.62936e-05,
      "loss": 0.2559,
      "step": 13900
    },
    {
      "epoch": 1.8666666666666667,
      "grad_norm": 2.3572487831115723,
      "learning_rate": 1.6266933333333335e-05,
      "loss": 0.2702,
      "step": 14000
    },
    {
      "epoch": 1.88,
      "grad_norm": 4.334176540374756,
      "learning_rate": 1.6240266666666668e-05,
      "loss": 0.2706,
      "step": 14100
    },
    {
      "epoch": 1.8933333333333333,
      "grad_norm": 1.8276503086090088,
      "learning_rate": 1.62136e-05,
      "loss": 0.2912,
      "step": 14200
    },
    {
      "epoch": 1.9066666666666667,
      "grad_norm": 2.2007803916931152,
      "learning_rate": 1.6186933333333336e-05,
      "loss": 0.2473,
      "step": 14300
    },
    {
      "epoch": 1.92,
      "grad_norm": 11.572789192199707,
      "learning_rate": 1.616026666666667e-05,
      "loss": 0.2633,
      "step": 14400
    },
    {
      "epoch": 1.9333333333333333,
      "grad_norm": 5.457643985748291,
      "learning_rate": 1.61336e-05,
      "loss": 0.2694,
      "step": 14500
    },
    {
      "epoch": 1.9466666666666668,
      "grad_norm": 7.611429214477539,
      "learning_rate": 1.6106933333333333e-05,
      "loss": 0.2497,
      "step": 14600
    },
    {
      "epoch": 1.96,
      "grad_norm": 7.964212894439697,
      "learning_rate": 1.608026666666667e-05,
      "loss": 0.2676,
      "step": 14700
    },
    {
      "epoch": 1.9733333333333334,
      "grad_norm": 3.9025259017944336,
      "learning_rate": 1.60536e-05,
      "loss": 0.2487,
      "step": 14800
    },
    {
      "epoch": 1.9866666666666668,
      "grad_norm": 2.120248794555664,
      "learning_rate": 1.6026933333333334e-05,
      "loss": 0.2858,
      "step": 14900
    },
    {
      "epoch": 2.0,
      "grad_norm": 3.8966081142425537,
      "learning_rate": 1.600026666666667e-05,
      "loss": 0.2685,
      "step": 15000
    },
    {
      "epoch": 2.013333333333333,
      "grad_norm": 4.5099053382873535,
      "learning_rate": 1.59736e-05,
      "loss": 0.2257,
      "step": 15100
    },
    {
      "epoch": 2.026666666666667,
      "grad_norm": 14.367196083068848,
      "learning_rate": 1.5946933333333335e-05,
      "loss": 0.2185,
      "step": 15200
    },
    {
      "epoch": 2.04,
      "grad_norm": 9.915059089660645,
      "learning_rate": 1.5920266666666667e-05,
      "loss": 0.2816,
      "step": 15300
    },
    {
      "epoch": 2.0533333333333332,
      "grad_norm": 1.3669123649597168,
      "learning_rate": 1.5893600000000003e-05,
      "loss": 0.2383,
      "step": 15400
    },
    {
      "epoch": 2.066666666666667,
      "grad_norm": 2.8918981552124023,
      "learning_rate": 1.5866933333333336e-05,
      "loss": 0.2344,
      "step": 15500
    },
    {
      "epoch": 2.08,
      "grad_norm": 4.030177593231201,
      "learning_rate": 1.5840266666666668e-05,
      "loss": 0.231,
      "step": 15600
    },
    {
      "epoch": 2.0933333333333333,
      "grad_norm": 9.342854499816895,
      "learning_rate": 1.5813600000000004e-05,
      "loss": 0.2321,
      "step": 15700
    },
    {
      "epoch": 2.1066666666666665,
      "grad_norm": 14.932171821594238,
      "learning_rate": 1.5786933333333333e-05,
      "loss": 0.2383,
      "step": 15800
    },
    {
      "epoch": 2.12,
      "grad_norm": 6.000755786895752,
      "learning_rate": 1.576026666666667e-05,
      "loss": 0.2355,
      "step": 15900
    },
    {
      "epoch": 2.1333333333333333,
      "grad_norm": 11.48206901550293,
      "learning_rate": 1.57336e-05,
      "loss": 0.2596,
      "step": 16000
    },
    {
      "epoch": 2.1466666666666665,
      "grad_norm": 9.267736434936523,
      "learning_rate": 1.5706933333333334e-05,
      "loss": 0.2375,
      "step": 16100
    },
    {
      "epoch": 2.16,
      "grad_norm": 2.502650499343872,
      "learning_rate": 1.568026666666667e-05,
      "loss": 0.2182,
      "step": 16200
    },
    {
      "epoch": 2.1733333333333333,
      "grad_norm": 5.154574871063232,
      "learning_rate": 1.5653600000000002e-05,
      "loss": 0.2331,
      "step": 16300
    },
    {
      "epoch": 2.1866666666666665,
      "grad_norm": 13.51174545288086,
      "learning_rate": 1.5626933333333335e-05,
      "loss": 0.2519,
      "step": 16400
    },
    {
      "epoch": 2.2,
      "grad_norm": 9.640563011169434,
      "learning_rate": 1.5600266666666667e-05,
      "loss": 0.2219,
      "step": 16500
    },
    {
      "epoch": 2.2133333333333334,
      "grad_norm": 14.564421653747559,
      "learning_rate": 1.5573600000000003e-05,
      "loss": 0.2433,
      "step": 16600
    },
    {
      "epoch": 2.2266666666666666,
      "grad_norm": 12.147926330566406,
      "learning_rate": 1.5546933333333335e-05,
      "loss": 0.2755,
      "step": 16700
    },
    {
      "epoch": 2.24,
      "grad_norm": 9.133990287780762,
      "learning_rate": 1.5520266666666668e-05,
      "loss": 0.2673,
      "step": 16800
    },
    {
      "epoch": 2.2533333333333334,
      "grad_norm": 2.7578465938568115,
      "learning_rate": 1.5493600000000004e-05,
      "loss": 0.2172,
      "step": 16900
    },
    {
      "epoch": 2.2666666666666666,
      "grad_norm": 7.028263092041016,
      "learning_rate": 1.5466933333333333e-05,
      "loss": 0.2193,
      "step": 17000
    },
    {
      "epoch": 2.2800000000000002,
      "grad_norm": 16.978071212768555,
      "learning_rate": 1.544026666666667e-05,
      "loss": 0.2172,
      "step": 17100
    },
    {
      "epoch": 2.2933333333333334,
      "grad_norm": 9.402832984924316,
      "learning_rate": 1.54136e-05,
      "loss": 0.2459,
      "step": 17200
    },
    {
      "epoch": 2.3066666666666666,
      "grad_norm": 9.558100700378418,
      "learning_rate": 1.5386933333333333e-05,
      "loss": 0.2144,
      "step": 17300
    },
    {
      "epoch": 2.32,
      "grad_norm": 10.723066329956055,
      "learning_rate": 1.536026666666667e-05,
      "loss": 0.2314,
      "step": 17400
    },
    {
      "epoch": 2.3333333333333335,
      "grad_norm": 7.31330680847168,
      "learning_rate": 1.5333600000000002e-05,
      "loss": 0.2041,
      "step": 17500
    },
    {
      "epoch": 2.3466666666666667,
      "grad_norm": 8.540431022644043,
      "learning_rate": 1.5306933333333334e-05,
      "loss": 0.2262,
      "step": 17600
    },
    {
      "epoch": 2.36,
      "grad_norm": 8.357955932617188,
      "learning_rate": 1.5280266666666667e-05,
      "loss": 0.2177,
      "step": 17700
    },
    {
      "epoch": 2.3733333333333335,
      "grad_norm": 16.267580032348633,
      "learning_rate": 1.5253600000000002e-05,
      "loss": 0.2579,
      "step": 17800
    },
    {
      "epoch": 2.3866666666666667,
      "grad_norm": 7.013324737548828,
      "learning_rate": 1.5226933333333333e-05,
      "loss": 0.2596,
      "step": 17900
    },
    {
      "epoch": 2.4,
      "grad_norm": 12.624381065368652,
      "learning_rate": 1.5200266666666667e-05,
      "loss": 0.2751,
      "step": 18000
    },
    {
      "epoch": 2.413333333333333,
      "grad_norm": 12.185943603515625,
      "learning_rate": 1.5173600000000002e-05,
      "loss": 0.2354,
      "step": 18100
    },
    {
      "epoch": 2.4266666666666667,
      "grad_norm": 10.042634963989258,
      "learning_rate": 1.5146933333333334e-05,
      "loss": 0.2321,
      "step": 18200
    },
    {
      "epoch": 2.44,
      "grad_norm": 9.699673652648926,
      "learning_rate": 1.5120266666666668e-05,
      "loss": 0.2534,
      "step": 18300
    },
    {
      "epoch": 2.453333333333333,
      "grad_norm": 7.16894006729126,
      "learning_rate": 1.5093600000000002e-05,
      "loss": 0.2283,
      "step": 18400
    },
    {
      "epoch": 2.466666666666667,
      "grad_norm": 18.68061637878418,
      "learning_rate": 1.5066933333333335e-05,
      "loss": 0.2344,
      "step": 18500
    },
    {
      "epoch": 2.48,
      "grad_norm": 10.517431259155273,
      "learning_rate": 1.5040266666666667e-05,
      "loss": 0.249,
      "step": 18600
    },
    {
      "epoch": 2.493333333333333,
      "grad_norm": 3.08146071434021,
      "learning_rate": 1.5013600000000001e-05,
      "loss": 0.2426,
      "step": 18700
    },
    {
      "epoch": 2.506666666666667,
      "grad_norm": 9.727799415588379,
      "learning_rate": 1.4986933333333336e-05,
      "loss": 0.2524,
      "step": 18800
    },
    {
      "epoch": 2.52,
      "grad_norm": 6.126445293426514,
      "learning_rate": 1.4960266666666668e-05,
      "loss": 0.2081,
      "step": 18900
    },
    {
      "epoch": 2.533333333333333,
      "grad_norm": 6.104029655456543,
      "learning_rate": 1.4933600000000002e-05,
      "loss": 0.2511,
      "step": 19000
    },
    {
      "epoch": 2.546666666666667,
      "grad_norm": 5.178187847137451,
      "learning_rate": 1.4906933333333336e-05,
      "loss": 0.2259,
      "step": 19100
    },
    {
      "epoch": 2.56,
      "grad_norm": 0.4967096745967865,
      "learning_rate": 1.4880266666666667e-05,
      "loss": 0.236,
      "step": 19200
    },
    {
      "epoch": 2.5733333333333333,
      "grad_norm": 9.041288375854492,
      "learning_rate": 1.4853600000000001e-05,
      "loss": 0.1987,
      "step": 19300
    },
    {
      "epoch": 2.586666666666667,
      "grad_norm": 13.666289329528809,
      "learning_rate": 1.4826933333333335e-05,
      "loss": 0.2172,
      "step": 19400
    },
    {
      "epoch": 2.6,
      "grad_norm": 3.783898115158081,
      "learning_rate": 1.4800266666666668e-05,
      "loss": 0.2978,
      "step": 19500
    },
    {
      "epoch": 2.6133333333333333,
      "grad_norm": 4.125554084777832,
      "learning_rate": 1.4773600000000002e-05,
      "loss": 0.2372,
      "step": 19600
    },
    {
      "epoch": 2.626666666666667,
      "grad_norm": 2.70536470413208,
      "learning_rate": 1.4746933333333336e-05,
      "loss": 0.2159,
      "step": 19700
    },
    {
      "epoch": 2.64,
      "grad_norm": 2.949824810028076,
      "learning_rate": 1.4720266666666667e-05,
      "loss": 0.2469,
      "step": 19800
    },
    {
      "epoch": 2.6533333333333333,
      "grad_norm": 14.126888275146484,
      "learning_rate": 1.4693600000000001e-05,
      "loss": 0.229,
      "step": 19900
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 4.984320163726807,
      "learning_rate": 1.4666933333333335e-05,
      "loss": 0.2376,
      "step": 20000
    },
    {
      "epoch": 2.68,
      "grad_norm": 18.521827697753906,
      "learning_rate": 1.4640266666666668e-05,
      "loss": 0.2475,
      "step": 20100
    },
    {
      "epoch": 2.6933333333333334,
      "grad_norm": 11.514243125915527,
      "learning_rate": 1.4613600000000002e-05,
      "loss": 0.2504,
      "step": 20200
    },
    {
      "epoch": 2.7066666666666666,
      "grad_norm": 0.6392233371734619,
      "learning_rate": 1.4586933333333336e-05,
      "loss": 0.2169,
      "step": 20300
    },
    {
      "epoch": 2.7199999999999998,
      "grad_norm": 16.99213409423828,
      "learning_rate": 1.4560266666666667e-05,
      "loss": 0.2329,
      "step": 20400
    },
    {
      "epoch": 2.7333333333333334,
      "grad_norm": 5.039942741394043,
      "learning_rate": 1.4533600000000001e-05,
      "loss": 0.2624,
      "step": 20500
    },
    {
      "epoch": 2.7466666666666666,
      "grad_norm": 3.0144546031951904,
      "learning_rate": 1.4506933333333335e-05,
      "loss": 0.2178,
      "step": 20600
    },
    {
      "epoch": 2.76,
      "grad_norm": 10.649972915649414,
      "learning_rate": 1.4480266666666668e-05,
      "loss": 0.2403,
      "step": 20700
    },
    {
      "epoch": 2.7733333333333334,
      "grad_norm": 31.41294288635254,
      "learning_rate": 1.4453600000000002e-05,
      "loss": 0.2283,
      "step": 20800
    },
    {
      "epoch": 2.7866666666666666,
      "grad_norm": 4.212625026702881,
      "learning_rate": 1.4426933333333336e-05,
      "loss": 0.2175,
      "step": 20900
    },
    {
      "epoch": 2.8,
      "grad_norm": 26.049467086791992,
      "learning_rate": 1.4400266666666667e-05,
      "loss": 0.2519,
      "step": 21000
    },
    {
      "epoch": 2.8133333333333335,
      "grad_norm": 10.61108684539795,
      "learning_rate": 1.43736e-05,
      "loss": 0.2272,
      "step": 21100
    },
    {
      "epoch": 2.8266666666666667,
      "grad_norm": 24.04694366455078,
      "learning_rate": 1.4346933333333335e-05,
      "loss": 0.2425,
      "step": 21200
    },
    {
      "epoch": 2.84,
      "grad_norm": 2.546699285507202,
      "learning_rate": 1.4320266666666667e-05,
      "loss": 0.2061,
      "step": 21300
    },
    {
      "epoch": 2.8533333333333335,
      "grad_norm": 7.947699069976807,
      "learning_rate": 1.4293600000000002e-05,
      "loss": 0.2444,
      "step": 21400
    },
    {
      "epoch": 2.8666666666666667,
      "grad_norm": 20.78478240966797,
      "learning_rate": 1.4266933333333336e-05,
      "loss": 0.2559,
      "step": 21500
    },
    {
      "epoch": 2.88,
      "grad_norm": 10.876418113708496,
      "learning_rate": 1.4240266666666668e-05,
      "loss": 0.2482,
      "step": 21600
    },
    {
      "epoch": 2.8933333333333335,
      "grad_norm": 2.196047067642212,
      "learning_rate": 1.42136e-05,
      "loss": 0.2498,
      "step": 21700
    },
    {
      "epoch": 2.9066666666666667,
      "grad_norm": 9.574529647827148,
      "learning_rate": 1.4186933333333335e-05,
      "loss": 0.2068,
      "step": 21800
    },
    {
      "epoch": 2.92,
      "grad_norm": 6.580834865570068,
      "learning_rate": 1.4160266666666669e-05,
      "loss": 0.2456,
      "step": 21900
    },
    {
      "epoch": 2.9333333333333336,
      "grad_norm": 11.338698387145996,
      "learning_rate": 1.4133600000000001e-05,
      "loss": 0.2582,
      "step": 22000
    },
    {
      "epoch": 2.9466666666666668,
      "grad_norm": 4.450259685516357,
      "learning_rate": 1.4106933333333336e-05,
      "loss": 0.231,
      "step": 22100
    },
    {
      "epoch": 2.96,
      "grad_norm": 3.165902614593506,
      "learning_rate": 1.4080266666666668e-05,
      "loss": 0.2208,
      "step": 22200
    },
    {
      "epoch": 2.9733333333333336,
      "grad_norm": 7.225534439086914,
      "learning_rate": 1.40536e-05,
      "loss": 0.2215,
      "step": 22300
    },
    {
      "epoch": 2.986666666666667,
      "grad_norm": 7.66480827331543,
      "learning_rate": 1.4026933333333335e-05,
      "loss": 0.2692,
      "step": 22400
    },
    {
      "epoch": 3.0,
      "grad_norm": 8.595826148986816,
      "learning_rate": 1.4000266666666669e-05,
      "loss": 0.2197,
      "step": 22500
    },
    {
      "epoch": 3.013333333333333,
      "grad_norm": 4.116664409637451,
      "learning_rate": 1.3973600000000001e-05,
      "loss": 0.215,
      "step": 22600
    },
    {
      "epoch": 3.026666666666667,
      "grad_norm": 13.032206535339355,
      "learning_rate": 1.3946933333333335e-05,
      "loss": 0.2375,
      "step": 22700
    },
    {
      "epoch": 3.04,
      "grad_norm": 1.3143547773361206,
      "learning_rate": 1.3920266666666668e-05,
      "loss": 0.1884,
      "step": 22800
    },
    {
      "epoch": 3.0533333333333332,
      "grad_norm": 5.801323890686035,
      "learning_rate": 1.38936e-05,
      "loss": 0.1945,
      "step": 22900
    },
    {
      "epoch": 3.066666666666667,
      "grad_norm": 11.743334770202637,
      "learning_rate": 1.3866933333333334e-05,
      "loss": 0.1994,
      "step": 23000
    },
    {
      "epoch": 3.08,
      "grad_norm": 5.121908664703369,
      "learning_rate": 1.3840266666666669e-05,
      "loss": 0.2069,
      "step": 23100
    },
    {
      "epoch": 3.0933333333333333,
      "grad_norm": 8.947903633117676,
      "learning_rate": 1.3813600000000001e-05,
      "loss": 0.2534,
      "step": 23200
    },
    {
      "epoch": 3.1066666666666665,
      "grad_norm": 12.124777793884277,
      "learning_rate": 1.3786933333333335e-05,
      "loss": 0.2202,
      "step": 23300
    },
    {
      "epoch": 3.12,
      "grad_norm": 13.350340843200684,
      "learning_rate": 1.3760266666666668e-05,
      "loss": 0.2257,
      "step": 23400
    },
    {
      "epoch": 3.1333333333333333,
      "grad_norm": 15.16694450378418,
      "learning_rate": 1.37336e-05,
      "loss": 0.2271,
      "step": 23500
    },
    {
      "epoch": 3.1466666666666665,
      "grad_norm": 1.926349401473999,
      "learning_rate": 1.3706933333333334e-05,
      "loss": 0.2113,
      "step": 23600
    },
    {
      "epoch": 3.16,
      "grad_norm": 4.878108501434326,
      "learning_rate": 1.3680266666666668e-05,
      "loss": 0.2049,
      "step": 23700
    },
    {
      "epoch": 3.1733333333333333,
      "grad_norm": 17.50592041015625,
      "learning_rate": 1.3653600000000001e-05,
      "loss": 0.2072,
      "step": 23800
    },
    {
      "epoch": 3.1866666666666665,
      "grad_norm": 27.64177894592285,
      "learning_rate": 1.3626933333333335e-05,
      "loss": 0.2162,
      "step": 23900
    },
    {
      "epoch": 3.2,
      "grad_norm": 1.8882843255996704,
      "learning_rate": 1.3600266666666667e-05,
      "loss": 0.1947,
      "step": 24000
    },
    {
      "epoch": 3.2133333333333334,
      "grad_norm": 1.4676377773284912,
      "learning_rate": 1.35736e-05,
      "loss": 0.2017,
      "step": 24100
    },
    {
      "epoch": 3.2266666666666666,
      "grad_norm": 26.900392532348633,
      "learning_rate": 1.3546933333333334e-05,
      "loss": 0.2046,
      "step": 24200
    },
    {
      "epoch": 3.24,
      "grad_norm": 1.1834639310836792,
      "learning_rate": 1.3520266666666668e-05,
      "loss": 0.2096,
      "step": 24300
    },
    {
      "epoch": 3.2533333333333334,
      "grad_norm": 5.366994857788086,
      "learning_rate": 1.34936e-05,
      "loss": 0.2069,
      "step": 24400
    },
    {
      "epoch": 3.2666666666666666,
      "grad_norm": 10.641483306884766,
      "learning_rate": 1.3466933333333335e-05,
      "loss": 0.2246,
      "step": 24500
    },
    {
      "epoch": 3.2800000000000002,
      "grad_norm": 5.590004920959473,
      "learning_rate": 1.3440266666666667e-05,
      "loss": 0.2039,
      "step": 24600
    },
    {
      "epoch": 3.2933333333333334,
      "grad_norm": 12.371960639953613,
      "learning_rate": 1.3413600000000001e-05,
      "loss": 0.2193,
      "step": 24700
    },
    {
      "epoch": 3.3066666666666666,
      "grad_norm": 0.5224547982215881,
      "learning_rate": 1.3386933333333334e-05,
      "loss": 0.2476,
      "step": 24800
    },
    {
      "epoch": 3.32,
      "grad_norm": 16.886924743652344,
      "learning_rate": 1.3360266666666668e-05,
      "loss": 0.2368,
      "step": 24900
    },
    {
      "epoch": 3.3333333333333335,
      "grad_norm": 0.5553842782974243,
      "learning_rate": 1.3333600000000002e-05,
      "loss": 0.2112,
      "step": 25000
    },
    {
      "epoch": 3.3466666666666667,
      "grad_norm": 0.4220988154411316,
      "learning_rate": 1.3306933333333335e-05,
      "loss": 0.2116,
      "step": 25100
    },
    {
      "epoch": 3.36,
      "grad_norm": 8.194385528564453,
      "learning_rate": 1.3280266666666667e-05,
      "loss": 0.2198,
      "step": 25200
    },
    {
      "epoch": 3.3733333333333335,
      "grad_norm": 4.670923709869385,
      "learning_rate": 1.3253600000000001e-05,
      "loss": 0.1961,
      "step": 25300
    },
    {
      "epoch": 3.3866666666666667,
      "grad_norm": 9.512177467346191,
      "learning_rate": 1.3226933333333334e-05,
      "loss": 0.25,
      "step": 25400
    },
    {
      "epoch": 3.4,
      "grad_norm": 8.11994743347168,
      "learning_rate": 1.3200266666666668e-05,
      "loss": 0.1945,
      "step": 25500
    },
    {
      "epoch": 3.413333333333333,
      "grad_norm": 2.0870509147644043,
      "learning_rate": 1.3173600000000002e-05,
      "loss": 0.2298,
      "step": 25600
    },
    {
      "epoch": 3.4266666666666667,
      "grad_norm": 0.5990543961524963,
      "learning_rate": 1.3146933333333335e-05,
      "loss": 0.1901,
      "step": 25700
    },
    {
      "epoch": 3.44,
      "grad_norm": 25.043611526489258,
      "learning_rate": 1.3120266666666667e-05,
      "loss": 0.2192,
      "step": 25800
    },
    {
      "epoch": 3.453333333333333,
      "grad_norm": 12.40501880645752,
      "learning_rate": 1.3093600000000001e-05,
      "loss": 0.2167,
      "step": 25900
    },
    {
      "epoch": 3.466666666666667,
      "grad_norm": 10.487936973571777,
      "learning_rate": 1.3066933333333334e-05,
      "loss": 0.1878,
      "step": 26000
    },
    {
      "epoch": 3.48,
      "grad_norm": 6.296340465545654,
      "learning_rate": 1.3040266666666668e-05,
      "loss": 0.246,
      "step": 26100
    },
    {
      "epoch": 3.493333333333333,
      "grad_norm": 6.893881797790527,
      "learning_rate": 1.3013600000000002e-05,
      "loss": 0.2198,
      "step": 26200
    },
    {
      "epoch": 3.506666666666667,
      "grad_norm": 21.86178970336914,
      "learning_rate": 1.2986933333333334e-05,
      "loss": 0.2169,
      "step": 26300
    },
    {
      "epoch": 3.52,
      "grad_norm": 4.848341941833496,
      "learning_rate": 1.2960266666666667e-05,
      "loss": 0.1964,
      "step": 26400
    },
    {
      "epoch": 3.533333333333333,
      "grad_norm": 15.261000633239746,
      "learning_rate": 1.2933600000000001e-05,
      "loss": 0.2513,
      "step": 26500
    },
    {
      "epoch": 3.546666666666667,
      "grad_norm": 6.1477789878845215,
      "learning_rate": 1.2906933333333333e-05,
      "loss": 0.2569,
      "step": 26600
    },
    {
      "epoch": 3.56,
      "grad_norm": 1.6831825971603394,
      "learning_rate": 1.2880266666666668e-05,
      "loss": 0.2085,
      "step": 26700
    },
    {
      "epoch": 3.5733333333333333,
      "grad_norm": 6.645299911499023,
      "learning_rate": 1.2853600000000002e-05,
      "loss": 0.2426,
      "step": 26800
    },
    {
      "epoch": 3.586666666666667,
      "grad_norm": 7.183412551879883,
      "learning_rate": 1.2826933333333334e-05,
      "loss": 0.2352,
      "step": 26900
    },
    {
      "epoch": 3.6,
      "grad_norm": 3.986584424972534,
      "learning_rate": 1.2800266666666667e-05,
      "loss": 0.1916,
      "step": 27000
    },
    {
      "epoch": 3.6133333333333333,
      "grad_norm": 7.77167272567749,
      "learning_rate": 1.27736e-05,
      "loss": 0.2523,
      "step": 27100
    },
    {
      "epoch": 3.626666666666667,
      "grad_norm": 17.145540237426758,
      "learning_rate": 1.2746933333333333e-05,
      "loss": 0.2159,
      "step": 27200
    },
    {
      "epoch": 3.64,
      "grad_norm": 17.940969467163086,
      "learning_rate": 1.2720266666666667e-05,
      "loss": 0.2356,
      "step": 27300
    },
    {
      "epoch": 3.6533333333333333,
      "grad_norm": 10.838821411132812,
      "learning_rate": 1.2693600000000002e-05,
      "loss": 0.1978,
      "step": 27400
    },
    {
      "epoch": 3.6666666666666665,
      "grad_norm": 16.74256134033203,
      "learning_rate": 1.2666933333333334e-05,
      "loss": 0.2235,
      "step": 27500
    },
    {
      "epoch": 3.68,
      "grad_norm": 13.123373031616211,
      "learning_rate": 1.2640266666666667e-05,
      "loss": 0.215,
      "step": 27600
    },
    {
      "epoch": 3.6933333333333334,
      "grad_norm": 7.370539665222168,
      "learning_rate": 1.26136e-05,
      "loss": 0.2222,
      "step": 27700
    },
    {
      "epoch": 3.7066666666666666,
      "grad_norm": 16.71141815185547,
      "learning_rate": 1.2586933333333335e-05,
      "loss": 0.196,
      "step": 27800
    },
    {
      "epoch": 3.7199999999999998,
      "grad_norm": 4.553891658782959,
      "learning_rate": 1.2560266666666667e-05,
      "loss": 0.2006,
      "step": 27900
    },
    {
      "epoch": 3.7333333333333334,
      "grad_norm": 3.446809768676758,
      "learning_rate": 1.2533600000000001e-05,
      "loss": 0.223,
      "step": 28000
    },
    {
      "epoch": 3.7466666666666666,
      "grad_norm": 9.427301406860352,
      "learning_rate": 1.2506933333333336e-05,
      "loss": 0.2124,
      "step": 28100
    },
    {
      "epoch": 3.76,
      "grad_norm": 7.571011543273926,
      "learning_rate": 1.2480266666666668e-05,
      "loss": 0.247,
      "step": 28200
    },
    {
      "epoch": 3.7733333333333334,
      "grad_norm": 2.5658249855041504,
      "learning_rate": 1.24536e-05,
      "loss": 0.1881,
      "step": 28300
    },
    {
      "epoch": 3.7866666666666666,
      "grad_norm": 14.272769927978516,
      "learning_rate": 1.2426933333333335e-05,
      "loss": 0.251,
      "step": 28400
    },
    {
      "epoch": 3.8,
      "grad_norm": 1.6869663000106812,
      "learning_rate": 1.2400266666666667e-05,
      "loss": 0.2359,
      "step": 28500
    },
    {
      "epoch": 3.8133333333333335,
      "grad_norm": 5.490729808807373,
      "learning_rate": 1.2373600000000001e-05,
      "loss": 0.2394,
      "step": 28600
    },
    {
      "epoch": 3.8266666666666667,
      "grad_norm": 3.9214212894439697,
      "learning_rate": 1.2346933333333335e-05,
      "loss": 0.2344,
      "step": 28700
    },
    {
      "epoch": 3.84,
      "grad_norm": 10.315160751342773,
      "learning_rate": 1.2320266666666668e-05,
      "loss": 0.2117,
      "step": 28800
    },
    {
      "epoch": 3.8533333333333335,
      "grad_norm": 14.080850601196289,
      "learning_rate": 1.22936e-05,
      "loss": 0.2243,
      "step": 28900
    },
    {
      "epoch": 3.8666666666666667,
      "grad_norm": 3.0175344944000244,
      "learning_rate": 1.2266933333333334e-05,
      "loss": 0.2261,
      "step": 29000
    },
    {
      "epoch": 3.88,
      "grad_norm": 16.725847244262695,
      "learning_rate": 1.2240266666666667e-05,
      "loss": 0.2272,
      "step": 29100
    },
    {
      "epoch": 3.8933333333333335,
      "grad_norm": 1.1012080907821655,
      "learning_rate": 1.2213600000000001e-05,
      "loss": 0.2166,
      "step": 29200
    },
    {
      "epoch": 3.9066666666666667,
      "grad_norm": 3.6757583618164062,
      "learning_rate": 1.2186933333333335e-05,
      "loss": 0.2017,
      "step": 29300
    },
    {
      "epoch": 3.92,
      "grad_norm": 2.5732479095458984,
      "learning_rate": 1.2160266666666668e-05,
      "loss": 0.2058,
      "step": 29400
    },
    {
      "epoch": 3.9333333333333336,
      "grad_norm": 11.068404197692871,
      "learning_rate": 1.21336e-05,
      "loss": 0.2318,
      "step": 29500
    },
    {
      "epoch": 3.9466666666666668,
      "grad_norm": 4.561017036437988,
      "learning_rate": 1.2106933333333334e-05,
      "loss": 0.1852,
      "step": 29600
    },
    {
      "epoch": 3.96,
      "grad_norm": 18.670854568481445,
      "learning_rate": 1.2080266666666667e-05,
      "loss": 0.2199,
      "step": 29700
    },
    {
      "epoch": 3.9733333333333336,
      "grad_norm": 0.3194483518600464,
      "learning_rate": 1.2053600000000001e-05,
      "loss": 0.227,
      "step": 29800
    },
    {
      "epoch": 3.986666666666667,
      "grad_norm": 1.2323309183120728,
      "learning_rate": 1.2026933333333335e-05,
      "loss": 0.2075,
      "step": 29900
    },
    {
      "epoch": 4.0,
      "grad_norm": 16.808351516723633,
      "learning_rate": 1.2000266666666668e-05,
      "loss": 0.2151,
      "step": 30000
    },
    {
      "epoch": 4.013333333333334,
      "grad_norm": 2.423532247543335,
      "learning_rate": 1.19736e-05,
      "loss": 0.2108,
      "step": 30100
    },
    {
      "epoch": 4.026666666666666,
      "grad_norm": 3.9905385971069336,
      "learning_rate": 1.1946933333333334e-05,
      "loss": 0.2031,
      "step": 30200
    },
    {
      "epoch": 4.04,
      "grad_norm": 9.119561195373535,
      "learning_rate": 1.1920266666666667e-05,
      "loss": 0.1813,
      "step": 30300
    },
    {
      "epoch": 4.053333333333334,
      "grad_norm": 9.045799255371094,
      "learning_rate": 1.18936e-05,
      "loss": 0.243,
      "step": 30400
    },
    {
      "epoch": 4.066666666666666,
      "grad_norm": 13.033162117004395,
      "learning_rate": 1.1866933333333335e-05,
      "loss": 0.2125,
      "step": 30500
    },
    {
      "epoch": 4.08,
      "grad_norm": 21.33729362487793,
      "learning_rate": 1.1840266666666667e-05,
      "loss": 0.1957,
      "step": 30600
    },
    {
      "epoch": 4.093333333333334,
      "grad_norm": 9.668801307678223,
      "learning_rate": 1.18136e-05,
      "loss": 0.238,
      "step": 30700
    },
    {
      "epoch": 4.1066666666666665,
      "grad_norm": 1.5678409337997437,
      "learning_rate": 1.1786933333333334e-05,
      "loss": 0.1912,
      "step": 30800
    },
    {
      "epoch": 4.12,
      "grad_norm": 14.256940841674805,
      "learning_rate": 1.1760266666666668e-05,
      "loss": 0.2098,
      "step": 30900
    },
    {
      "epoch": 4.133333333333334,
      "grad_norm": 16.171239852905273,
      "learning_rate": 1.17336e-05,
      "loss": 0.2002,
      "step": 31000
    },
    {
      "epoch": 4.1466666666666665,
      "grad_norm": 10.124191284179688,
      "learning_rate": 1.1706933333333335e-05,
      "loss": 0.2067,
      "step": 31100
    },
    {
      "epoch": 4.16,
      "grad_norm": 16.96250343322754,
      "learning_rate": 1.1680266666666669e-05,
      "loss": 0.1939,
      "step": 31200
    },
    {
      "epoch": 4.173333333333334,
      "grad_norm": 0.7520156502723694,
      "learning_rate": 1.16536e-05,
      "loss": 0.1673,
      "step": 31300
    },
    {
      "epoch": 4.1866666666666665,
      "grad_norm": 9.906779289245605,
      "learning_rate": 1.1626933333333334e-05,
      "loss": 0.2059,
      "step": 31400
    },
    {
      "epoch": 4.2,
      "grad_norm": 4.159633636474609,
      "learning_rate": 1.1600266666666668e-05,
      "loss": 0.2029,
      "step": 31500
    },
    {
      "epoch": 4.213333333333333,
      "grad_norm": 17.147022247314453,
      "learning_rate": 1.15736e-05,
      "loss": 0.2264,
      "step": 31600
    },
    {
      "epoch": 4.226666666666667,
      "grad_norm": 1.2234978675842285,
      "learning_rate": 1.1546933333333335e-05,
      "loss": 0.2194,
      "step": 31700
    },
    {
      "epoch": 4.24,
      "grad_norm": 23.9370174407959,
      "learning_rate": 1.1520266666666669e-05,
      "loss": 0.1668,
      "step": 31800
    },
    {
      "epoch": 4.253333333333333,
      "grad_norm": 2.536545991897583,
      "learning_rate": 1.14936e-05,
      "loss": 0.1967,
      "step": 31900
    },
    {
      "epoch": 4.266666666666667,
      "grad_norm": 16.048500061035156,
      "learning_rate": 1.1466933333333334e-05,
      "loss": 0.2282,
      "step": 32000
    },
    {
      "epoch": 4.28,
      "grad_norm": 7.109256267547607,
      "learning_rate": 1.1440266666666668e-05,
      "loss": 0.211,
      "step": 32100
    },
    {
      "epoch": 4.293333333333333,
      "grad_norm": 18.688655853271484,
      "learning_rate": 1.14136e-05,
      "loss": 0.2146,
      "step": 32200
    },
    {
      "epoch": 4.306666666666667,
      "grad_norm": 6.281598091125488,
      "learning_rate": 1.1386933333333334e-05,
      "loss": 0.2432,
      "step": 32300
    },
    {
      "epoch": 4.32,
      "grad_norm": 18.511930465698242,
      "learning_rate": 1.1360266666666669e-05,
      "loss": 0.2116,
      "step": 32400
    },
    {
      "epoch": 4.333333333333333,
      "grad_norm": 11.584689140319824,
      "learning_rate": 1.13336e-05,
      "loss": 0.1797,
      "step": 32500
    },
    {
      "epoch": 4.346666666666667,
      "grad_norm": 3.8710858821868896,
      "learning_rate": 1.1306933333333334e-05,
      "loss": 0.1947,
      "step": 32600
    },
    {
      "epoch": 4.36,
      "grad_norm": 15.768472671508789,
      "learning_rate": 1.1280266666666668e-05,
      "loss": 0.2036,
      "step": 32700
    },
    {
      "epoch": 4.373333333333333,
      "grad_norm": 5.767270088195801,
      "learning_rate": 1.12536e-05,
      "loss": 0.1879,
      "step": 32800
    },
    {
      "epoch": 4.386666666666667,
      "grad_norm": 0.483004629611969,
      "learning_rate": 1.1226933333333334e-05,
      "loss": 0.1758,
      "step": 32900
    },
    {
      "epoch": 4.4,
      "grad_norm": 0.8642290234565735,
      "learning_rate": 1.1200266666666668e-05,
      "loss": 0.188,
      "step": 33000
    },
    {
      "epoch": 4.413333333333333,
      "grad_norm": 6.645131587982178,
      "learning_rate": 1.11736e-05,
      "loss": 0.2067,
      "step": 33100
    },
    {
      "epoch": 4.426666666666667,
      "grad_norm": 8.797831535339355,
      "learning_rate": 1.1146933333333333e-05,
      "loss": 0.2085,
      "step": 33200
    },
    {
      "epoch": 4.44,
      "grad_norm": 17.75446128845215,
      "learning_rate": 1.1120266666666668e-05,
      "loss": 0.167,
      "step": 33300
    },
    {
      "epoch": 4.453333333333333,
      "grad_norm": 12.105572700500488,
      "learning_rate": 1.10936e-05,
      "loss": 0.2161,
      "step": 33400
    },
    {
      "epoch": 4.466666666666667,
      "grad_norm": 18.373600006103516,
      "learning_rate": 1.1066933333333334e-05,
      "loss": 0.1837,
      "step": 33500
    },
    {
      "epoch": 4.48,
      "grad_norm": 0.5564841628074646,
      "learning_rate": 1.1040266666666668e-05,
      "loss": 0.2184,
      "step": 33600
    },
    {
      "epoch": 4.493333333333333,
      "grad_norm": 11.63772201538086,
      "learning_rate": 1.1013599999999999e-05,
      "loss": 0.1964,
      "step": 33700
    },
    {
      "epoch": 4.506666666666667,
      "grad_norm": 7.077812194824219,
      "learning_rate": 1.0986933333333333e-05,
      "loss": 0.1861,
      "step": 33800
    },
    {
      "epoch": 4.52,
      "grad_norm": 2.7268295288085938,
      "learning_rate": 1.0960266666666667e-05,
      "loss": 0.163,
      "step": 33900
    },
    {
      "epoch": 4.533333333333333,
      "grad_norm": 0.8674483299255371,
      "learning_rate": 1.0933600000000002e-05,
      "loss": 0.2192,
      "step": 34000
    },
    {
      "epoch": 4.546666666666667,
      "grad_norm": 3.564638137817383,
      "learning_rate": 1.0906933333333334e-05,
      "loss": 0.1795,
      "step": 34100
    },
    {
      "epoch": 4.5600000000000005,
      "grad_norm": 4.327739238739014,
      "learning_rate": 1.0880266666666668e-05,
      "loss": 0.2024,
      "step": 34200
    },
    {
      "epoch": 4.573333333333333,
      "grad_norm": 7.254251003265381,
      "learning_rate": 1.0853600000000002e-05,
      "loss": 0.1686,
      "step": 34300
    },
    {
      "epoch": 4.586666666666667,
      "grad_norm": 1.7603960037231445,
      "learning_rate": 1.0826933333333333e-05,
      "loss": 0.1795,
      "step": 34400
    },
    {
      "epoch": 4.6,
      "grad_norm": 14.604737281799316,
      "learning_rate": 1.0800266666666667e-05,
      "loss": 0.1684,
      "step": 34500
    },
    {
      "epoch": 4.613333333333333,
      "grad_norm": 2.6972341537475586,
      "learning_rate": 1.0773600000000001e-05,
      "loss": 0.2619,
      "step": 34600
    },
    {
      "epoch": 4.626666666666667,
      "grad_norm": 2.0260908603668213,
      "learning_rate": 1.0746933333333334e-05,
      "loss": 0.2075,
      "step": 34700
    },
    {
      "epoch": 4.64,
      "grad_norm": 3.3658523559570312,
      "learning_rate": 1.0720266666666668e-05,
      "loss": 0.1846,
      "step": 34800
    },
    {
      "epoch": 4.653333333333333,
      "grad_norm": 26.683656692504883,
      "learning_rate": 1.0693600000000002e-05,
      "loss": 0.203,
      "step": 34900
    },
    {
      "epoch": 4.666666666666667,
      "grad_norm": 4.294963836669922,
      "learning_rate": 1.0666933333333333e-05,
      "loss": 0.2025,
      "step": 35000
    },
    {
      "epoch": 4.68,
      "grad_norm": 13.484722137451172,
      "learning_rate": 1.0640266666666667e-05,
      "loss": 0.2473,
      "step": 35100
    },
    {
      "epoch": 4.693333333333333,
      "grad_norm": 11.907212257385254,
      "learning_rate": 1.0613600000000001e-05,
      "loss": 0.2073,
      "step": 35200
    },
    {
      "epoch": 4.706666666666667,
      "grad_norm": 1.4344894886016846,
      "learning_rate": 1.0586933333333334e-05,
      "loss": 0.2254,
      "step": 35300
    },
    {
      "epoch": 4.72,
      "grad_norm": 4.614639759063721,
      "learning_rate": 1.0560266666666668e-05,
      "loss": 0.1957,
      "step": 35400
    },
    {
      "epoch": 4.733333333333333,
      "grad_norm": 7.461950778961182,
      "learning_rate": 1.0533600000000002e-05,
      "loss": 0.1917,
      "step": 35500
    },
    {
      "epoch": 4.746666666666667,
      "grad_norm": 0.31168821454048157,
      "learning_rate": 1.0506933333333333e-05,
      "loss": 0.2316,
      "step": 35600
    },
    {
      "epoch": 4.76,
      "grad_norm": 8.013995170593262,
      "learning_rate": 1.0480266666666667e-05,
      "loss": 0.1941,
      "step": 35700
    },
    {
      "epoch": 4.773333333333333,
      "grad_norm": 4.551304340362549,
      "learning_rate": 1.0453600000000001e-05,
      "loss": 0.2051,
      "step": 35800
    },
    {
      "epoch": 4.786666666666667,
      "grad_norm": 16.994518280029297,
      "learning_rate": 1.0426933333333333e-05,
      "loss": 0.2008,
      "step": 35900
    },
    {
      "epoch": 4.8,
      "grad_norm": 0.44604015350341797,
      "learning_rate": 1.0400266666666668e-05,
      "loss": 0.2237,
      "step": 36000
    },
    {
      "epoch": 4.8133333333333335,
      "grad_norm": 1.3539212942123413,
      "learning_rate": 1.0373600000000002e-05,
      "loss": 0.2127,
      "step": 36100
    },
    {
      "epoch": 4.826666666666666,
      "grad_norm": 13.55205249786377,
      "learning_rate": 1.0346933333333333e-05,
      "loss": 0.1869,
      "step": 36200
    },
    {
      "epoch": 4.84,
      "grad_norm": 5.981228828430176,
      "learning_rate": 1.0320266666666667e-05,
      "loss": 0.165,
      "step": 36300
    },
    {
      "epoch": 4.8533333333333335,
      "grad_norm": 6.181283473968506,
      "learning_rate": 1.0293600000000001e-05,
      "loss": 0.2367,
      "step": 36400
    },
    {
      "epoch": 4.866666666666667,
      "grad_norm": 15.086688041687012,
      "learning_rate": 1.0266933333333333e-05,
      "loss": 0.2294,
      "step": 36500
    },
    {
      "epoch": 4.88,
      "grad_norm": 12.760409355163574,
      "learning_rate": 1.0240266666666667e-05,
      "loss": 0.1539,
      "step": 36600
    },
    {
      "epoch": 4.8933333333333335,
      "grad_norm": 2.398329973220825,
      "learning_rate": 1.0213600000000002e-05,
      "loss": 0.1958,
      "step": 36700
    },
    {
      "epoch": 4.906666666666666,
      "grad_norm": 8.513564109802246,
      "learning_rate": 1.0186933333333332e-05,
      "loss": 0.2121,
      "step": 36800
    },
    {
      "epoch": 4.92,
      "grad_norm": 20.32755470275879,
      "learning_rate": 1.0160266666666667e-05,
      "loss": 0.2269,
      "step": 36900
    },
    {
      "epoch": 4.933333333333334,
      "grad_norm": 2.855778932571411,
      "learning_rate": 1.01336e-05,
      "loss": 0.1987,
      "step": 37000
    },
    {
      "epoch": 4.946666666666666,
      "grad_norm": 2.5600287914276123,
      "learning_rate": 1.0106933333333335e-05,
      "loss": 0.1979,
      "step": 37100
    },
    {
      "epoch": 4.96,
      "grad_norm": 13.727527618408203,
      "learning_rate": 1.0080266666666667e-05,
      "loss": 0.1895,
      "step": 37200
    },
    {
      "epoch": 4.973333333333334,
      "grad_norm": 9.328853607177734,
      "learning_rate": 1.0053600000000001e-05,
      "loss": 0.1818,
      "step": 37300
    },
    {
      "epoch": 4.986666666666666,
      "grad_norm": 4.1188859939575195,
      "learning_rate": 1.0026933333333336e-05,
      "loss": 0.1818,
      "step": 37400
    },
    {
      "epoch": 5.0,
      "grad_norm": 21.68355941772461,
      "learning_rate": 1.0000266666666666e-05,
      "loss": 0.1669,
      "step": 37500
    },
    {
      "epoch": 5.013333333333334,
      "grad_norm": 1.3529454469680786,
      "learning_rate": 9.9736e-06,
      "loss": 0.1966,
      "step": 37600
    },
    {
      "epoch": 5.026666666666666,
      "grad_norm": 9.160152435302734,
      "learning_rate": 9.946933333333335e-06,
      "loss": 0.1993,
      "step": 37700
    },
    {
      "epoch": 5.04,
      "grad_norm": 11.651732444763184,
      "learning_rate": 9.920266666666667e-06,
      "loss": 0.221,
      "step": 37800
    },
    {
      "epoch": 5.053333333333334,
      "grad_norm": 0.4525630474090576,
      "learning_rate": 9.893600000000001e-06,
      "loss": 0.2015,
      "step": 37900
    },
    {
      "epoch": 5.066666666666666,
      "grad_norm": 13.341971397399902,
      "learning_rate": 9.866933333333334e-06,
      "loss": 0.1767,
      "step": 38000
    },
    {
      "epoch": 5.08,
      "grad_norm": 7.9135260581970215,
      "learning_rate": 9.840266666666668e-06,
      "loss": 0.1935,
      "step": 38100
    },
    {
      "epoch": 5.093333333333334,
      "grad_norm": 8.342005729675293,
      "learning_rate": 9.8136e-06,
      "loss": 0.1978,
      "step": 38200
    },
    {
      "epoch": 5.1066666666666665,
      "grad_norm": 1.3044805526733398,
      "learning_rate": 9.786933333333335e-06,
      "loss": 0.1627,
      "step": 38300
    },
    {
      "epoch": 5.12,
      "grad_norm": 8.030858039855957,
      "learning_rate": 9.760266666666669e-06,
      "loss": 0.2056,
      "step": 38400
    },
    {
      "epoch": 5.133333333333334,
      "grad_norm": 22.2084903717041,
      "learning_rate": 9.733600000000001e-06,
      "loss": 0.2221,
      "step": 38500
    },
    {
      "epoch": 5.1466666666666665,
      "grad_norm": 4.137096881866455,
      "learning_rate": 9.706933333333334e-06,
      "loss": 0.1454,
      "step": 38600
    },
    {
      "epoch": 5.16,
      "grad_norm": 14.593667030334473,
      "learning_rate": 9.680266666666668e-06,
      "loss": 0.1958,
      "step": 38700
    },
    {
      "epoch": 5.173333333333334,
      "grad_norm": 12.934248924255371,
      "learning_rate": 9.6536e-06,
      "loss": 0.1503,
      "step": 38800
    },
    {
      "epoch": 5.1866666666666665,
      "grad_norm": 18.640350341796875,
      "learning_rate": 9.626933333333334e-06,
      "loss": 0.1801,
      "step": 38900
    },
    {
      "epoch": 5.2,
      "grad_norm": 5.0171942710876465,
      "learning_rate": 9.600266666666669e-06,
      "loss": 0.2128,
      "step": 39000
    },
    {
      "epoch": 5.213333333333333,
      "grad_norm": 9.540709495544434,
      "learning_rate": 9.573600000000001e-06,
      "loss": 0.1901,
      "step": 39100
    },
    {
      "epoch": 5.226666666666667,
      "grad_norm": 2.0811538696289062,
      "learning_rate": 9.546933333333333e-06,
      "loss": 0.1832,
      "step": 39200
    },
    {
      "epoch": 5.24,
      "grad_norm": 7.107668876647949,
      "learning_rate": 9.520266666666668e-06,
      "loss": 0.2029,
      "step": 39300
    },
    {
      "epoch": 5.253333333333333,
      "grad_norm": 6.673218727111816,
      "learning_rate": 9.4936e-06,
      "loss": 0.1705,
      "step": 39400
    },
    {
      "epoch": 5.266666666666667,
      "grad_norm": 2.4474873542785645,
      "learning_rate": 9.466933333333334e-06,
      "loss": 0.2126,
      "step": 39500
    },
    {
      "epoch": 5.28,
      "grad_norm": 11.671000480651855,
      "learning_rate": 9.440266666666668e-06,
      "loss": 0.1443,
      "step": 39600
    },
    {
      "epoch": 5.293333333333333,
      "grad_norm": 6.996885776519775,
      "learning_rate": 9.4136e-06,
      "loss": 0.1905,
      "step": 39700
    },
    {
      "epoch": 5.306666666666667,
      "grad_norm": 10.193925857543945,
      "learning_rate": 9.386933333333335e-06,
      "loss": 0.197,
      "step": 39800
    },
    {
      "epoch": 5.32,
      "grad_norm": 1.7552189826965332,
      "learning_rate": 9.360266666666667e-06,
      "loss": 0.1654,
      "step": 39900
    },
    {
      "epoch": 5.333333333333333,
      "grad_norm": 3.503591775894165,
      "learning_rate": 9.3336e-06,
      "loss": 0.195,
      "step": 40000
    },
    {
      "epoch": 5.346666666666667,
      "grad_norm": 6.382853031158447,
      "learning_rate": 9.306933333333334e-06,
      "loss": 0.1912,
      "step": 40100
    },
    {
      "epoch": 5.36,
      "grad_norm": 15.044140815734863,
      "learning_rate": 9.280266666666668e-06,
      "loss": 0.1989,
      "step": 40200
    },
    {
      "epoch": 5.373333333333333,
      "grad_norm": 26.34731674194336,
      "learning_rate": 9.2536e-06,
      "loss": 0.2112,
      "step": 40300
    },
    {
      "epoch": 5.386666666666667,
      "grad_norm": 23.46990203857422,
      "learning_rate": 9.226933333333335e-06,
      "loss": 0.1665,
      "step": 40400
    },
    {
      "epoch": 5.4,
      "grad_norm": 5.882835388183594,
      "learning_rate": 9.200266666666667e-06,
      "loss": 0.2036,
      "step": 40500
    },
    {
      "epoch": 5.413333333333333,
      "grad_norm": 4.5386962890625,
      "learning_rate": 9.1736e-06,
      "loss": 0.1729,
      "step": 40600
    },
    {
      "epoch": 5.426666666666667,
      "grad_norm": 0.46507105231285095,
      "learning_rate": 9.146933333333334e-06,
      "loss": 0.1951,
      "step": 40700
    },
    {
      "epoch": 5.44,
      "grad_norm": 0.6912207007408142,
      "learning_rate": 9.120266666666668e-06,
      "loss": 0.198,
      "step": 40800
    },
    {
      "epoch": 5.453333333333333,
      "grad_norm": 0.3587738275527954,
      "learning_rate": 9.0936e-06,
      "loss": 0.2035,
      "step": 40900
    },
    {
      "epoch": 5.466666666666667,
      "grad_norm": 3.863821268081665,
      "learning_rate": 9.066933333333335e-06,
      "loss": 0.1718,
      "step": 41000
    },
    {
      "epoch": 5.48,
      "grad_norm": 15.682168960571289,
      "learning_rate": 9.040266666666667e-06,
      "loss": 0.2102,
      "step": 41100
    },
    {
      "epoch": 5.493333333333333,
      "grad_norm": 12.231302261352539,
      "learning_rate": 9.013600000000001e-06,
      "loss": 0.1987,
      "step": 41200
    },
    {
      "epoch": 5.506666666666667,
      "grad_norm": 9.433095932006836,
      "learning_rate": 8.986933333333334e-06,
      "loss": 0.2031,
      "step": 41300
    },
    {
      "epoch": 5.52,
      "grad_norm": 3.3649051189422607,
      "learning_rate": 8.960266666666668e-06,
      "loss": 0.1981,
      "step": 41400
    },
    {
      "epoch": 5.533333333333333,
      "grad_norm": 16.077571868896484,
      "learning_rate": 8.9336e-06,
      "loss": 0.1634,
      "step": 41500
    },
    {
      "epoch": 5.546666666666667,
      "grad_norm": 21.786663055419922,
      "learning_rate": 8.906933333333334e-06,
      "loss": 0.1788,
      "step": 41600
    },
    {
      "epoch": 5.5600000000000005,
      "grad_norm": 4.325893402099609,
      "learning_rate": 8.880266666666667e-06,
      "loss": 0.1704,
      "step": 41700
    },
    {
      "epoch": 5.573333333333333,
      "grad_norm": 3.8875086307525635,
      "learning_rate": 8.853600000000001e-06,
      "loss": 0.2014,
      "step": 41800
    },
    {
      "epoch": 5.586666666666667,
      "grad_norm": 16.531936645507812,
      "learning_rate": 8.826933333333334e-06,
      "loss": 0.2041,
      "step": 41900
    },
    {
      "epoch": 5.6,
      "grad_norm": 3.544901132583618,
      "learning_rate": 8.800266666666668e-06,
      "loss": 0.1968,
      "step": 42000
    },
    {
      "epoch": 5.613333333333333,
      "grad_norm": 12.205377578735352,
      "learning_rate": 8.7736e-06,
      "loss": 0.1908,
      "step": 42100
    },
    {
      "epoch": 5.626666666666667,
      "grad_norm": 13.582674980163574,
      "learning_rate": 8.746933333333334e-06,
      "loss": 0.1931,
      "step": 42200
    },
    {
      "epoch": 5.64,
      "grad_norm": 0.6378212571144104,
      "learning_rate": 8.720266666666667e-06,
      "loss": 0.198,
      "step": 42300
    },
    {
      "epoch": 5.653333333333333,
      "grad_norm": 9.061920166015625,
      "learning_rate": 8.693600000000001e-06,
      "loss": 0.2358,
      "step": 42400
    },
    {
      "epoch": 5.666666666666667,
      "grad_norm": 9.475695610046387,
      "learning_rate": 8.666933333333333e-06,
      "loss": 0.2117,
      "step": 42500
    },
    {
      "epoch": 5.68,
      "grad_norm": 4.8049211502075195,
      "learning_rate": 8.640266666666668e-06,
      "loss": 0.1839,
      "step": 42600
    },
    {
      "epoch": 5.693333333333333,
      "grad_norm": 5.533324241638184,
      "learning_rate": 8.613600000000002e-06,
      "loss": 0.167,
      "step": 42700
    },
    {
      "epoch": 5.706666666666667,
      "grad_norm": 19.251405715942383,
      "learning_rate": 8.586933333333334e-06,
      "loss": 0.1899,
      "step": 42800
    },
    {
      "epoch": 5.72,
      "grad_norm": 12.579652786254883,
      "learning_rate": 8.560266666666668e-06,
      "loss": 0.1999,
      "step": 42900
    },
    {
      "epoch": 5.733333333333333,
      "grad_norm": 10.57128620147705,
      "learning_rate": 8.5336e-06,
      "loss": 0.195,
      "step": 43000
    },
    {
      "epoch": 5.746666666666667,
      "grad_norm": 0.18346698582172394,
      "learning_rate": 8.506933333333333e-06,
      "loss": 0.2168,
      "step": 43100
    },
    {
      "epoch": 5.76,
      "grad_norm": 20.415937423706055,
      "learning_rate": 8.480266666666667e-06,
      "loss": 0.2102,
      "step": 43200
    },
    {
      "epoch": 5.773333333333333,
      "grad_norm": 11.03055191040039,
      "learning_rate": 8.453600000000002e-06,
      "loss": 0.2119,
      "step": 43300
    },
    {
      "epoch": 5.786666666666667,
      "grad_norm": 11.617734909057617,
      "learning_rate": 8.426933333333334e-06,
      "loss": 0.1772,
      "step": 43400
    },
    {
      "epoch": 5.8,
      "grad_norm": 14.032968521118164,
      "learning_rate": 8.400266666666668e-06,
      "loss": 0.1573,
      "step": 43500
    },
    {
      "epoch": 5.8133333333333335,
      "grad_norm": 9.096837043762207,
      "learning_rate": 8.3736e-06,
      "loss": 0.1758,
      "step": 43600
    },
    {
      "epoch": 5.826666666666666,
      "grad_norm": 0.18095938861370087,
      "learning_rate": 8.346933333333333e-06,
      "loss": 0.1928,
      "step": 43700
    },
    {
      "epoch": 5.84,
      "grad_norm": 35.5779914855957,
      "learning_rate": 8.320266666666667e-06,
      "loss": 0.157,
      "step": 43800
    },
    {
      "epoch": 5.8533333333333335,
      "grad_norm": 22.715742111206055,
      "learning_rate": 8.293600000000001e-06,
      "loss": 0.1834,
      "step": 43900
    },
    {
      "epoch": 5.866666666666667,
      "grad_norm": 13.408613204956055,
      "learning_rate": 8.266933333333334e-06,
      "loss": 0.1928,
      "step": 44000
    },
    {
      "epoch": 5.88,
      "grad_norm": 7.417118072509766,
      "learning_rate": 8.240266666666668e-06,
      "loss": 0.167,
      "step": 44100
    },
    {
      "epoch": 5.8933333333333335,
      "grad_norm": 1.3019001483917236,
      "learning_rate": 8.2136e-06,
      "loss": 0.15,
      "step": 44200
    },
    {
      "epoch": 5.906666666666666,
      "grad_norm": 1.1271706819534302,
      "learning_rate": 8.186933333333335e-06,
      "loss": 0.1665,
      "step": 44300
    },
    {
      "epoch": 5.92,
      "grad_norm": 5.487853050231934,
      "learning_rate": 8.160266666666667e-06,
      "loss": 0.1732,
      "step": 44400
    },
    {
      "epoch": 5.933333333333334,
      "grad_norm": 7.270611763000488,
      "learning_rate": 8.133600000000001e-06,
      "loss": 0.2115,
      "step": 44500
    },
    {
      "epoch": 5.946666666666666,
      "grad_norm": 16.953369140625,
      "learning_rate": 8.106933333333334e-06,
      "loss": 0.1922,
      "step": 44600
    },
    {
      "epoch": 5.96,
      "grad_norm": 6.171934127807617,
      "learning_rate": 8.080266666666668e-06,
      "loss": 0.1576,
      "step": 44700
    },
    {
      "epoch": 5.973333333333334,
      "grad_norm": 1.4547617435455322,
      "learning_rate": 8.0536e-06,
      "loss": 0.1707,
      "step": 44800
    },
    {
      "epoch": 5.986666666666666,
      "grad_norm": 20.73331069946289,
      "learning_rate": 8.026933333333334e-06,
      "loss": 0.1992,
      "step": 44900
    },
    {
      "epoch": 6.0,
      "grad_norm": 11.464400291442871,
      "learning_rate": 8.000266666666667e-06,
      "loss": 0.1858,
      "step": 45000
    },
    {
      "epoch": 6.013333333333334,
      "grad_norm": 1.304343819618225,
      "learning_rate": 7.973600000000001e-06,
      "loss": 0.1597,
      "step": 45100
    },
    {
      "epoch": 6.026666666666666,
      "grad_norm": 19.51143455505371,
      "learning_rate": 7.946933333333334e-06,
      "loss": 0.1483,
      "step": 45200
    },
    {
      "epoch": 6.04,
      "grad_norm": 12.111104965209961,
      "learning_rate": 7.920266666666668e-06,
      "loss": 0.1333,
      "step": 45300
    },
    {
      "epoch": 6.053333333333334,
      "grad_norm": 1.3430581092834473,
      "learning_rate": 7.8936e-06,
      "loss": 0.1604,
      "step": 45400
    },
    {
      "epoch": 6.066666666666666,
      "grad_norm": 1.0531502962112427,
      "learning_rate": 7.866933333333334e-06,
      "loss": 0.1752,
      "step": 45500
    },
    {
      "epoch": 6.08,
      "grad_norm": 1.3865524530410767,
      "learning_rate": 7.840266666666667e-06,
      "loss": 0.171,
      "step": 45600
    },
    {
      "epoch": 6.093333333333334,
      "grad_norm": 3.244472026824951,
      "learning_rate": 7.813600000000001e-06,
      "loss": 0.1999,
      "step": 45700
    },
    {
      "epoch": 6.1066666666666665,
      "grad_norm": 12.421440124511719,
      "learning_rate": 7.786933333333333e-06,
      "loss": 0.179,
      "step": 45800
    },
    {
      "epoch": 6.12,
      "grad_norm": 2.513986825942993,
      "learning_rate": 7.760266666666667e-06,
      "loss": 0.1885,
      "step": 45900
    },
    {
      "epoch": 6.133333333333334,
      "grad_norm": 25.57530975341797,
      "learning_rate": 7.733600000000002e-06,
      "loss": 0.154,
      "step": 46000
    },
    {
      "epoch": 6.1466666666666665,
      "grad_norm": 11.599739074707031,
      "learning_rate": 7.706933333333334e-06,
      "loss": 0.2039,
      "step": 46100
    },
    {
      "epoch": 6.16,
      "grad_norm": 24.23214340209961,
      "learning_rate": 7.680266666666667e-06,
      "loss": 0.201,
      "step": 46200
    },
    {
      "epoch": 6.173333333333334,
      "grad_norm": 12.862343788146973,
      "learning_rate": 7.6536e-06,
      "loss": 0.1878,
      "step": 46300
    },
    {
      "epoch": 6.1866666666666665,
      "grad_norm": 0.42873889207839966,
      "learning_rate": 7.626933333333334e-06,
      "loss": 0.1853,
      "step": 46400
    },
    {
      "epoch": 6.2,
      "grad_norm": 5.032922744750977,
      "learning_rate": 7.6002666666666665e-06,
      "loss": 0.1801,
      "step": 46500
    },
    {
      "epoch": 6.213333333333333,
      "grad_norm": 6.4477925300598145,
      "learning_rate": 7.573600000000001e-06,
      "loss": 0.181,
      "step": 46600
    },
    {
      "epoch": 6.226666666666667,
      "grad_norm": 8.138752937316895,
      "learning_rate": 7.546933333333334e-06,
      "loss": 0.19,
      "step": 46700
    },
    {
      "epoch": 6.24,
      "grad_norm": 23.432893753051758,
      "learning_rate": 7.520266666666666e-06,
      "loss": 0.2028,
      "step": 46800
    },
    {
      "epoch": 6.253333333333333,
      "grad_norm": 2.0939462184906006,
      "learning_rate": 7.4936000000000006e-06,
      "loss": 0.1808,
      "step": 46900
    },
    {
      "epoch": 6.266666666666667,
      "grad_norm": 8.637131690979004,
      "learning_rate": 7.466933333333334e-06,
      "loss": 0.1931,
      "step": 47000
    },
    {
      "epoch": 6.28,
      "grad_norm": 0.36688557267189026,
      "learning_rate": 7.440266666666667e-06,
      "loss": 0.1823,
      "step": 47100
    },
    {
      "epoch": 6.293333333333333,
      "grad_norm": 11.571274757385254,
      "learning_rate": 7.4136000000000005e-06,
      "loss": 0.1917,
      "step": 47200
    },
    {
      "epoch": 6.306666666666667,
      "grad_norm": 11.233525276184082,
      "learning_rate": 7.386933333333334e-06,
      "loss": 0.194,
      "step": 47300
    },
    {
      "epoch": 6.32,
      "grad_norm": 0.10178657621145248,
      "learning_rate": 7.360266666666668e-06,
      "loss": 0.1978,
      "step": 47400
    },
    {
      "epoch": 6.333333333333333,
      "grad_norm": 6.804658889770508,
      "learning_rate": 7.3336e-06,
      "loss": 0.1701,
      "step": 47500
    },
    {
      "epoch": 6.346666666666667,
      "grad_norm": 12.408530235290527,
      "learning_rate": 7.306933333333334e-06,
      "loss": 0.1603,
      "step": 47600
    },
    {
      "epoch": 6.36,
      "grad_norm": 0.7747877836227417,
      "learning_rate": 7.280266666666668e-06,
      "loss": 0.1865,
      "step": 47700
    },
    {
      "epoch": 6.373333333333333,
      "grad_norm": 5.551791191101074,
      "learning_rate": 7.2536e-06,
      "loss": 0.1935,
      "step": 47800
    },
    {
      "epoch": 6.386666666666667,
      "grad_norm": 21.01951789855957,
      "learning_rate": 7.226933333333334e-06,
      "loss": 0.1564,
      "step": 47900
    },
    {
      "epoch": 6.4,
      "grad_norm": 3.859401226043701,
      "learning_rate": 7.200266666666668e-06,
      "loss": 0.1874,
      "step": 48000
    },
    {
      "epoch": 6.413333333333333,
      "grad_norm": 14.20114803314209,
      "learning_rate": 7.1736e-06,
      "loss": 0.1831,
      "step": 48100
    },
    {
      "epoch": 6.426666666666667,
      "grad_norm": 6.848519325256348,
      "learning_rate": 7.1469333333333335e-06,
      "loss": 0.1831,
      "step": 48200
    },
    {
      "epoch": 6.44,
      "grad_norm": 28.15115737915039,
      "learning_rate": 7.120266666666668e-06,
      "loss": 0.2034,
      "step": 48300
    },
    {
      "epoch": 6.453333333333333,
      "grad_norm": 2.4377951622009277,
      "learning_rate": 7.0936e-06,
      "loss": 0.1509,
      "step": 48400
    },
    {
      "epoch": 6.466666666666667,
      "grad_norm": 12.428770065307617,
      "learning_rate": 7.0669333333333335e-06,
      "loss": 0.1988,
      "step": 48500
    },
    {
      "epoch": 6.48,
      "grad_norm": 5.289759635925293,
      "learning_rate": 7.040266666666668e-06,
      "loss": 0.1639,
      "step": 48600
    },
    {
      "epoch": 6.493333333333333,
      "grad_norm": 5.920706272125244,
      "learning_rate": 7.0136e-06,
      "loss": 0.2191,
      "step": 48700
    },
    {
      "epoch": 6.506666666666667,
      "grad_norm": 9.025228500366211,
      "learning_rate": 6.986933333333334e-06,
      "loss": 0.1791,
      "step": 48800
    },
    {
      "epoch": 6.52,
      "grad_norm": 20.544509887695312,
      "learning_rate": 6.9602666666666675e-06,
      "loss": 0.1908,
      "step": 48900
    },
    {
      "epoch": 6.533333333333333,
      "grad_norm": 15.677947044372559,
      "learning_rate": 6.9336e-06,
      "loss": 0.1582,
      "step": 49000
    },
    {
      "epoch": 6.546666666666667,
      "grad_norm": 25.142059326171875,
      "learning_rate": 6.906933333333334e-06,
      "loss": 0.1961,
      "step": 49100
    },
    {
      "epoch": 6.5600000000000005,
      "grad_norm": 2.262908697128296,
      "learning_rate": 6.8802666666666674e-06,
      "loss": 0.1975,
      "step": 49200
    },
    {
      "epoch": 6.573333333333333,
      "grad_norm": 13.116320610046387,
      "learning_rate": 6.8536e-06,
      "loss": 0.2028,
      "step": 49300
    },
    {
      "epoch": 6.586666666666667,
      "grad_norm": 1.250461220741272,
      "learning_rate": 6.826933333333334e-06,
      "loss": 0.2075,
      "step": 49400
    },
    {
      "epoch": 6.6,
      "grad_norm": 11.926380157470703,
      "learning_rate": 6.800266666666667e-06,
      "loss": 0.1694,
      "step": 49500
    },
    {
      "epoch": 6.613333333333333,
      "grad_norm": 0.18654020130634308,
      "learning_rate": 6.7736e-06,
      "loss": 0.1606,
      "step": 49600
    },
    {
      "epoch": 6.626666666666667,
      "grad_norm": 0.13108572363853455,
      "learning_rate": 6.746933333333334e-06,
      "loss": 0.1841,
      "step": 49700
    },
    {
      "epoch": 6.64,
      "grad_norm": 1.7684786319732666,
      "learning_rate": 6.720266666666667e-06,
      "loss": 0.1346,
      "step": 49800
    },
    {
      "epoch": 6.653333333333333,
      "grad_norm": 1.3892186880111694,
      "learning_rate": 6.6936e-06,
      "loss": 0.1759,
      "step": 49900
    },
    {
      "epoch": 6.666666666666667,
      "grad_norm": 4.010998725891113,
      "learning_rate": 6.666933333333334e-06,
      "loss": 0.1756,
      "step": 50000
    },
    {
      "epoch": 6.68,
      "grad_norm": 19.358089447021484,
      "learning_rate": 6.640266666666667e-06,
      "loss": 0.1794,
      "step": 50100
    },
    {
      "epoch": 6.693333333333333,
      "grad_norm": 16.945098876953125,
      "learning_rate": 6.6136e-06,
      "loss": 0.1944,
      "step": 50200
    },
    {
      "epoch": 6.706666666666667,
      "grad_norm": 2.1723031997680664,
      "learning_rate": 6.586933333333334e-06,
      "loss": 0.2048,
      "step": 50300
    },
    {
      "epoch": 6.72,
      "grad_norm": 0.8856297731399536,
      "learning_rate": 6.560266666666667e-06,
      "loss": 0.1932,
      "step": 50400
    },
    {
      "epoch": 6.733333333333333,
      "grad_norm": 8.996837615966797,
      "learning_rate": 6.533600000000001e-06,
      "loss": 0.1788,
      "step": 50500
    },
    {
      "epoch": 6.746666666666667,
      "grad_norm": 0.8467409014701843,
      "learning_rate": 6.506933333333334e-06,
      "loss": 0.1904,
      "step": 50600
    },
    {
      "epoch": 6.76,
      "grad_norm": 5.993545055389404,
      "learning_rate": 6.480266666666667e-06,
      "loss": 0.1825,
      "step": 50700
    },
    {
      "epoch": 6.773333333333333,
      "grad_norm": 10.763545989990234,
      "learning_rate": 6.453600000000001e-06,
      "loss": 0.1763,
      "step": 50800
    },
    {
      "epoch": 6.786666666666667,
      "grad_norm": 0.6001567244529724,
      "learning_rate": 6.426933333333334e-06,
      "loss": 0.1695,
      "step": 50900
    },
    {
      "epoch": 6.8,
      "grad_norm": 11.71642017364502,
      "learning_rate": 6.400266666666667e-06,
      "loss": 0.1502,
      "step": 51000
    },
    {
      "epoch": 6.8133333333333335,
      "grad_norm": 17.056642532348633,
      "learning_rate": 6.373600000000001e-06,
      "loss": 0.1892,
      "step": 51100
    },
    {
      "epoch": 6.826666666666666,
      "grad_norm": 18.465721130371094,
      "learning_rate": 6.3469333333333336e-06,
      "loss": 0.2099,
      "step": 51200
    },
    {
      "epoch": 6.84,
      "grad_norm": 15.128161430358887,
      "learning_rate": 6.320266666666667e-06,
      "loss": 0.1839,
      "step": 51300
    },
    {
      "epoch": 6.8533333333333335,
      "grad_norm": 11.893064498901367,
      "learning_rate": 6.293600000000001e-06,
      "loss": 0.1917,
      "step": 51400
    },
    {
      "epoch": 6.866666666666667,
      "grad_norm": 19.448772430419922,
      "learning_rate": 6.2669333333333335e-06,
      "loss": 0.1787,
      "step": 51500
    },
    {
      "epoch": 6.88,
      "grad_norm": 2.2407610416412354,
      "learning_rate": 6.240266666666667e-06,
      "loss": 0.1489,
      "step": 51600
    },
    {
      "epoch": 6.8933333333333335,
      "grad_norm": 4.514382362365723,
      "learning_rate": 6.213600000000001e-06,
      "loss": 0.1955,
      "step": 51700
    },
    {
      "epoch": 6.906666666666666,
      "grad_norm": 6.209812641143799,
      "learning_rate": 6.186933333333333e-06,
      "loss": 0.2256,
      "step": 51800
    },
    {
      "epoch": 6.92,
      "grad_norm": 3.024359941482544,
      "learning_rate": 6.160266666666667e-06,
      "loss": 0.1501,
      "step": 51900
    },
    {
      "epoch": 6.933333333333334,
      "grad_norm": 1.062647819519043,
      "learning_rate": 6.133600000000001e-06,
      "loss": 0.1849,
      "step": 52000
    },
    {
      "epoch": 6.946666666666666,
      "grad_norm": 6.288954734802246,
      "learning_rate": 6.106933333333333e-06,
      "loss": 0.1748,
      "step": 52100
    },
    {
      "epoch": 6.96,
      "grad_norm": 13.490036964416504,
      "learning_rate": 6.0802666666666675e-06,
      "loss": 0.1703,
      "step": 52200
    },
    {
      "epoch": 6.973333333333334,
      "grad_norm": 27.263214111328125,
      "learning_rate": 6.053600000000001e-06,
      "loss": 0.1966,
      "step": 52300
    },
    {
      "epoch": 6.986666666666666,
      "grad_norm": 30.534954071044922,
      "learning_rate": 6.026933333333333e-06,
      "loss": 0.1812,
      "step": 52400
    },
    {
      "epoch": 7.0,
      "grad_norm": 15.3949613571167,
      "learning_rate": 6.000266666666667e-06,
      "loss": 0.1624,
      "step": 52500
    },
    {
      "epoch": 7.013333333333334,
      "grad_norm": 4.872892379760742,
      "learning_rate": 5.973600000000001e-06,
      "loss": 0.2108,
      "step": 52600
    },
    {
      "epoch": 7.026666666666666,
      "grad_norm": 6.060730457305908,
      "learning_rate": 5.946933333333333e-06,
      "loss": 0.1977,
      "step": 52700
    },
    {
      "epoch": 7.04,
      "grad_norm": 0.348629891872406,
      "learning_rate": 5.920266666666667e-06,
      "loss": 0.1603,
      "step": 52800
    },
    {
      "epoch": 7.053333333333334,
      "grad_norm": 1.2012290954589844,
      "learning_rate": 5.893600000000001e-06,
      "loss": 0.2043,
      "step": 52900
    },
    {
      "epoch": 7.066666666666666,
      "grad_norm": 2.987914562225342,
      "learning_rate": 5.866933333333333e-06,
      "loss": 0.1927,
      "step": 53000
    },
    {
      "epoch": 7.08,
      "grad_norm": 5.0918192863464355,
      "learning_rate": 5.840266666666667e-06,
      "loss": 0.1686,
      "step": 53100
    },
    {
      "epoch": 7.093333333333334,
      "grad_norm": 0.1614711582660675,
      "learning_rate": 5.8136000000000005e-06,
      "loss": 0.1281,
      "step": 53200
    },
    {
      "epoch": 7.1066666666666665,
      "grad_norm": 11.312766075134277,
      "learning_rate": 5.786933333333333e-06,
      "loss": 0.1705,
      "step": 53300
    },
    {
      "epoch": 7.12,
      "grad_norm": 1.5179908275604248,
      "learning_rate": 5.760266666666667e-06,
      "loss": 0.195,
      "step": 53400
    },
    {
      "epoch": 7.133333333333334,
      "grad_norm": 18.219398498535156,
      "learning_rate": 5.7336000000000005e-06,
      "loss": 0.1705,
      "step": 53500
    },
    {
      "epoch": 7.1466666666666665,
      "grad_norm": 0.18830959498882294,
      "learning_rate": 5.706933333333335e-06,
      "loss": 0.1439,
      "step": 53600
    },
    {
      "epoch": 7.16,
      "grad_norm": 34.715614318847656,
      "learning_rate": 5.680266666666667e-06,
      "loss": 0.1775,
      "step": 53700
    },
    {
      "epoch": 7.173333333333334,
      "grad_norm": 3.021620512008667,
      "learning_rate": 5.6536e-06,
      "loss": 0.1881,
      "step": 53800
    },
    {
      "epoch": 7.1866666666666665,
      "grad_norm": 13.34521770477295,
      "learning_rate": 5.6269333333333345e-06,
      "loss": 0.1855,
      "step": 53900
    },
    {
      "epoch": 7.2,
      "grad_norm": 8.992134094238281,
      "learning_rate": 5.600266666666667e-06,
      "loss": 0.1465,
      "step": 54000
    },
    {
      "epoch": 7.213333333333333,
      "grad_norm": 17.900423049926758,
      "learning_rate": 5.5736e-06,
      "loss": 0.1384,
      "step": 54100
    },
    {
      "epoch": 7.226666666666667,
      "grad_norm": 5.556480407714844,
      "learning_rate": 5.5469333333333345e-06,
      "loss": 0.1211,
      "step": 54200
    },
    {
      "epoch": 7.24,
      "grad_norm": 6.622219562530518,
      "learning_rate": 5.520266666666667e-06,
      "loss": 0.1587,
      "step": 54300
    },
    {
      "epoch": 7.253333333333333,
      "grad_norm": 24.61495018005371,
      "learning_rate": 5.4936e-06,
      "loss": 0.1793,
      "step": 54400
    },
    {
      "epoch": 7.266666666666667,
      "grad_norm": 1.5400230884552002,
      "learning_rate": 5.466933333333334e-06,
      "loss": 0.1654,
      "step": 54500
    },
    {
      "epoch": 7.28,
      "grad_norm": 11.153782844543457,
      "learning_rate": 5.440266666666667e-06,
      "loss": 0.2179,
      "step": 54600
    },
    {
      "epoch": 7.293333333333333,
      "grad_norm": 7.432883262634277,
      "learning_rate": 5.4136e-06,
      "loss": 0.1726,
      "step": 54700
    },
    {
      "epoch": 7.306666666666667,
      "grad_norm": 12.449888229370117,
      "learning_rate": 5.386933333333334e-06,
      "loss": 0.1729,
      "step": 54800
    },
    {
      "epoch": 7.32,
      "grad_norm": 12.859626770019531,
      "learning_rate": 5.360266666666667e-06,
      "loss": 0.1513,
      "step": 54900
    },
    {
      "epoch": 7.333333333333333,
      "grad_norm": 14.143017768859863,
      "learning_rate": 5.3336e-06,
      "loss": 0.1766,
      "step": 55000
    },
    {
      "epoch": 7.346666666666667,
      "grad_norm": 17.598012924194336,
      "learning_rate": 5.306933333333334e-06,
      "loss": 0.178,
      "step": 55100
    },
    {
      "epoch": 7.36,
      "grad_norm": 12.963571548461914,
      "learning_rate": 5.280266666666667e-06,
      "loss": 0.2176,
      "step": 55200
    },
    {
      "epoch": 7.373333333333333,
      "grad_norm": 1.254692792892456,
      "learning_rate": 5.253600000000001e-06,
      "loss": 0.1949,
      "step": 55300
    },
    {
      "epoch": 7.386666666666667,
      "grad_norm": 0.2575947940349579,
      "learning_rate": 5.226933333333334e-06,
      "loss": 0.2011,
      "step": 55400
    },
    {
      "epoch": 7.4,
      "grad_norm": 2.2626986503601074,
      "learning_rate": 5.200266666666667e-06,
      "loss": 0.1673,
      "step": 55500
    },
    {
      "epoch": 7.413333333333333,
      "grad_norm": 6.104809284210205,
      "learning_rate": 5.173600000000001e-06,
      "loss": 0.18,
      "step": 55600
    },
    {
      "epoch": 7.426666666666667,
      "grad_norm": 8.479296684265137,
      "learning_rate": 5.146933333333334e-06,
      "loss": 0.1948,
      "step": 55700
    },
    {
      "epoch": 7.44,
      "grad_norm": 0.6468912959098816,
      "learning_rate": 5.1202666666666665e-06,
      "loss": 0.1982,
      "step": 55800
    },
    {
      "epoch": 7.453333333333333,
      "grad_norm": 14.273466110229492,
      "learning_rate": 5.093600000000001e-06,
      "loss": 0.187,
      "step": 55900
    },
    {
      "epoch": 7.466666666666667,
      "grad_norm": 12.365473747253418,
      "learning_rate": 5.066933333333334e-06,
      "loss": 0.2083,
      "step": 56000
    },
    {
      "epoch": 7.48,
      "grad_norm": 5.736056804656982,
      "learning_rate": 5.0402666666666664e-06,
      "loss": 0.1391,
      "step": 56100
    },
    {
      "epoch": 7.493333333333333,
      "grad_norm": 7.306015491485596,
      "learning_rate": 5.013600000000001e-06,
      "loss": 0.1684,
      "step": 56200
    },
    {
      "epoch": 7.506666666666667,
      "grad_norm": 18.042516708374023,
      "learning_rate": 4.986933333333334e-06,
      "loss": 0.1889,
      "step": 56300
    },
    {
      "epoch": 7.52,
      "grad_norm": 9.21253490447998,
      "learning_rate": 4.960266666666667e-06,
      "loss": 0.1879,
      "step": 56400
    },
    {
      "epoch": 7.533333333333333,
      "grad_norm": 1.9476393461227417,
      "learning_rate": 4.9336000000000005e-06,
      "loss": 0.1526,
      "step": 56500
    },
    {
      "epoch": 7.546666666666667,
      "grad_norm": 5.265914440155029,
      "learning_rate": 4.906933333333334e-06,
      "loss": 0.1603,
      "step": 56600
    },
    {
      "epoch": 7.5600000000000005,
      "grad_norm": 1.317934513092041,
      "learning_rate": 4.880266666666667e-06,
      "loss": 0.1506,
      "step": 56700
    },
    {
      "epoch": 7.573333333333333,
      "grad_norm": 10.599661827087402,
      "learning_rate": 4.8536e-06,
      "loss": 0.1698,
      "step": 56800
    },
    {
      "epoch": 7.586666666666667,
      "grad_norm": 11.533944129943848,
      "learning_rate": 4.826933333333334e-06,
      "loss": 0.1799,
      "step": 56900
    },
    {
      "epoch": 7.6,
      "grad_norm": 6.576314926147461,
      "learning_rate": 4.800266666666667e-06,
      "loss": 0.2057,
      "step": 57000
    },
    {
      "epoch": 7.613333333333333,
      "grad_norm": 11.61752700805664,
      "learning_rate": 4.7736e-06,
      "loss": 0.175,
      "step": 57100
    },
    {
      "epoch": 7.626666666666667,
      "grad_norm": 12.187435150146484,
      "learning_rate": 4.746933333333334e-06,
      "loss": 0.1719,
      "step": 57200
    },
    {
      "epoch": 7.64,
      "grad_norm": 10.128958702087402,
      "learning_rate": 4.720266666666667e-06,
      "loss": 0.1813,
      "step": 57300
    },
    {
      "epoch": 7.653333333333333,
      "grad_norm": 3.155054807662964,
      "learning_rate": 4.6936e-06,
      "loss": 0.168,
      "step": 57400
    },
    {
      "epoch": 7.666666666666667,
      "grad_norm": 15.211862564086914,
      "learning_rate": 4.6669333333333336e-06,
      "loss": 0.1408,
      "step": 57500
    },
    {
      "epoch": 7.68,
      "grad_norm": 14.096559524536133,
      "learning_rate": 4.640266666666667e-06,
      "loss": 0.1812,
      "step": 57600
    },
    {
      "epoch": 7.693333333333333,
      "grad_norm": 0.4188944697380066,
      "learning_rate": 4.6136e-06,
      "loss": 0.1833,
      "step": 57700
    },
    {
      "epoch": 7.706666666666667,
      "grad_norm": 0.7325696349143982,
      "learning_rate": 4.5869333333333335e-06,
      "loss": 0.1835,
      "step": 57800
    },
    {
      "epoch": 7.72,
      "grad_norm": 14.438896179199219,
      "learning_rate": 4.560266666666668e-06,
      "loss": 0.1833,
      "step": 57900
    },
    {
      "epoch": 7.733333333333333,
      "grad_norm": 2.4368979930877686,
      "learning_rate": 4.5336e-06,
      "loss": 0.1925,
      "step": 58000
    },
    {
      "epoch": 7.746666666666667,
      "grad_norm": 0.9573600888252258,
      "learning_rate": 4.506933333333333e-06,
      "loss": 0.1839,
      "step": 58100
    },
    {
      "epoch": 7.76,
      "grad_norm": 1.4560565948486328,
      "learning_rate": 4.4802666666666676e-06,
      "loss": 0.1602,
      "step": 58200
    },
    {
      "epoch": 7.773333333333333,
      "grad_norm": 3.334270477294922,
      "learning_rate": 4.4536e-06,
      "loss": 0.1714,
      "step": 58300
    },
    {
      "epoch": 7.786666666666667,
      "grad_norm": 8.414456367492676,
      "learning_rate": 4.426933333333333e-06,
      "loss": 0.1515,
      "step": 58400
    },
    {
      "epoch": 7.8,
      "grad_norm": 6.0535149574279785,
      "learning_rate": 4.4002666666666675e-06,
      "loss": 0.1326,
      "step": 58500
    },
    {
      "epoch": 7.8133333333333335,
      "grad_norm": 0.4602063298225403,
      "learning_rate": 4.373600000000001e-06,
      "loss": 0.1656,
      "step": 58600
    },
    {
      "epoch": 7.826666666666666,
      "grad_norm": 2.3702430725097656,
      "learning_rate": 4.346933333333333e-06,
      "loss": 0.1817,
      "step": 58700
    },
    {
      "epoch": 7.84,
      "grad_norm": 11.290014266967773,
      "learning_rate": 4.320266666666667e-06,
      "loss": 0.16,
      "step": 58800
    },
    {
      "epoch": 7.8533333333333335,
      "grad_norm": 0.5426076054573059,
      "learning_rate": 4.293600000000001e-06,
      "loss": 0.1784,
      "step": 58900
    },
    {
      "epoch": 7.866666666666667,
      "grad_norm": 0.2310800850391388,
      "learning_rate": 4.266933333333333e-06,
      "loss": 0.1667,
      "step": 59000
    },
    {
      "epoch": 7.88,
      "grad_norm": 7.171346664428711,
      "learning_rate": 4.240266666666667e-06,
      "loss": 0.1964,
      "step": 59100
    },
    {
      "epoch": 7.8933333333333335,
      "grad_norm": 0.6520147919654846,
      "learning_rate": 4.213600000000001e-06,
      "loss": 0.1737,
      "step": 59200
    },
    {
      "epoch": 7.906666666666666,
      "grad_norm": 0.7061256170272827,
      "learning_rate": 4.186933333333333e-06,
      "loss": 0.1731,
      "step": 59300
    },
    {
      "epoch": 7.92,
      "grad_norm": 16.480512619018555,
      "learning_rate": 4.160266666666667e-06,
      "loss": 0.1867,
      "step": 59400
    },
    {
      "epoch": 7.933333333333334,
      "grad_norm": 10.799705505371094,
      "learning_rate": 4.1336000000000005e-06,
      "loss": 0.1893,
      "step": 59500
    },
    {
      "epoch": 7.946666666666666,
      "grad_norm": 8.415678024291992,
      "learning_rate": 4.106933333333334e-06,
      "loss": 0.1619,
      "step": 59600
    },
    {
      "epoch": 7.96,
      "grad_norm": 1.849660038948059,
      "learning_rate": 4.080266666666667e-06,
      "loss": 0.1764,
      "step": 59700
    },
    {
      "epoch": 7.973333333333334,
      "grad_norm": 4.2323737144470215,
      "learning_rate": 4.0536000000000005e-06,
      "loss": 0.1356,
      "step": 59800
    },
    {
      "epoch": 7.986666666666666,
      "grad_norm": 8.747182846069336,
      "learning_rate": 4.026933333333334e-06,
      "loss": 0.1712,
      "step": 59900
    },
    {
      "epoch": 8.0,
      "grad_norm": 6.533971309661865,
      "learning_rate": 4.000266666666667e-06,
      "loss": 0.172,
      "step": 60000
    },
    {
      "epoch": 8.013333333333334,
      "grad_norm": 2.1666417121887207,
      "learning_rate": 3.9736e-06,
      "loss": 0.1233,
      "step": 60100
    },
    {
      "epoch": 8.026666666666667,
      "grad_norm": 0.5043943524360657,
      "learning_rate": 3.946933333333334e-06,
      "loss": 0.1541,
      "step": 60200
    },
    {
      "epoch": 8.04,
      "grad_norm": 16.458683013916016,
      "learning_rate": 3.920266666666667e-06,
      "loss": 0.1775,
      "step": 60300
    },
    {
      "epoch": 8.053333333333333,
      "grad_norm": 14.663002014160156,
      "learning_rate": 3.8936e-06,
      "loss": 0.1642,
      "step": 60400
    },
    {
      "epoch": 8.066666666666666,
      "grad_norm": 0.3086796700954437,
      "learning_rate": 3.866933333333334e-06,
      "loss": 0.1738,
      "step": 60500
    },
    {
      "epoch": 8.08,
      "grad_norm": 0.4769284129142761,
      "learning_rate": 3.840266666666667e-06,
      "loss": 0.22,
      "step": 60600
    },
    {
      "epoch": 8.093333333333334,
      "grad_norm": 7.65771484375,
      "learning_rate": 3.8136000000000002e-06,
      "loss": 0.1465,
      "step": 60700
    },
    {
      "epoch": 8.106666666666667,
      "grad_norm": 10.365714073181152,
      "learning_rate": 3.7869333333333335e-06,
      "loss": 0.1615,
      "step": 60800
    },
    {
      "epoch": 8.12,
      "grad_norm": 11.974763870239258,
      "learning_rate": 3.7602666666666673e-06,
      "loss": 0.193,
      "step": 60900
    },
    {
      "epoch": 8.133333333333333,
      "grad_norm": 11.609942436218262,
      "learning_rate": 3.7336000000000006e-06,
      "loss": 0.1639,
      "step": 61000
    },
    {
      "epoch": 8.146666666666667,
      "grad_norm": 0.7881537675857544,
      "learning_rate": 3.7069333333333334e-06,
      "loss": 0.1641,
      "step": 61100
    },
    {
      "epoch": 8.16,
      "grad_norm": 18.9764461517334,
      "learning_rate": 3.680266666666667e-06,
      "loss": 0.1628,
      "step": 61200
    },
    {
      "epoch": 8.173333333333334,
      "grad_norm": 0.5492441058158875,
      "learning_rate": 3.6536000000000005e-06,
      "loss": 0.1632,
      "step": 61300
    },
    {
      "epoch": 8.186666666666667,
      "grad_norm": 5.397511005401611,
      "learning_rate": 3.6269333333333334e-06,
      "loss": 0.1949,
      "step": 61400
    },
    {
      "epoch": 8.2,
      "grad_norm": 11.82060718536377,
      "learning_rate": 3.600266666666667e-06,
      "loss": 0.1653,
      "step": 61500
    },
    {
      "epoch": 8.213333333333333,
      "grad_norm": 9.098936080932617,
      "learning_rate": 3.5736000000000004e-06,
      "loss": 0.1721,
      "step": 61600
    },
    {
      "epoch": 8.226666666666667,
      "grad_norm": 2.812870740890503,
      "learning_rate": 3.5469333333333337e-06,
      "loss": 0.1755,
      "step": 61700
    },
    {
      "epoch": 8.24,
      "grad_norm": 8.393320083618164,
      "learning_rate": 3.520266666666667e-06,
      "loss": 0.16,
      "step": 61800
    },
    {
      "epoch": 8.253333333333334,
      "grad_norm": 19.520801544189453,
      "learning_rate": 3.4936000000000003e-06,
      "loss": 0.1733,
      "step": 61900
    },
    {
      "epoch": 8.266666666666667,
      "grad_norm": 3.1652181148529053,
      "learning_rate": 3.4669333333333336e-06,
      "loss": 0.1471,
      "step": 62000
    },
    {
      "epoch": 8.28,
      "grad_norm": 16.6374454498291,
      "learning_rate": 3.440266666666667e-06,
      "loss": 0.1831,
      "step": 62100
    },
    {
      "epoch": 8.293333333333333,
      "grad_norm": 0.3224363327026367,
      "learning_rate": 3.4136000000000002e-06,
      "loss": 0.1921,
      "step": 62200
    },
    {
      "epoch": 8.306666666666667,
      "grad_norm": 9.123080253601074,
      "learning_rate": 3.3869333333333335e-06,
      "loss": 0.1851,
      "step": 62300
    },
    {
      "epoch": 8.32,
      "grad_norm": 10.174029350280762,
      "learning_rate": 3.360266666666667e-06,
      "loss": 0.1698,
      "step": 62400
    },
    {
      "epoch": 8.333333333333334,
      "grad_norm": 28.150766372680664,
      "learning_rate": 3.3336e-06,
      "loss": 0.1776,
      "step": 62500
    },
    {
      "epoch": 8.346666666666668,
      "grad_norm": 10.153923034667969,
      "learning_rate": 3.3069333333333335e-06,
      "loss": 0.1979,
      "step": 62600
    },
    {
      "epoch": 8.36,
      "grad_norm": 12.31323528289795,
      "learning_rate": 3.280266666666667e-06,
      "loss": 0.1488,
      "step": 62700
    },
    {
      "epoch": 8.373333333333333,
      "grad_norm": 11.916321754455566,
      "learning_rate": 3.2536e-06,
      "loss": 0.1424,
      "step": 62800
    },
    {
      "epoch": 8.386666666666667,
      "grad_norm": 11.276313781738281,
      "learning_rate": 3.2269333333333334e-06,
      "loss": 0.1693,
      "step": 62900
    },
    {
      "epoch": 8.4,
      "grad_norm": 14.72175121307373,
      "learning_rate": 3.200266666666667e-06,
      "loss": 0.167,
      "step": 63000
    },
    {
      "epoch": 8.413333333333334,
      "grad_norm": 0.7549378275871277,
      "learning_rate": 3.1736e-06,
      "loss": 0.1566,
      "step": 63100
    },
    {
      "epoch": 8.426666666666666,
      "grad_norm": 3.225322961807251,
      "learning_rate": 3.1469333333333333e-06,
      "loss": 0.1608,
      "step": 63200
    },
    {
      "epoch": 8.44,
      "grad_norm": 0.16025404632091522,
      "learning_rate": 3.120266666666667e-06,
      "loss": 0.1993,
      "step": 63300
    },
    {
      "epoch": 8.453333333333333,
      "grad_norm": 10.122552871704102,
      "learning_rate": 3.0936000000000003e-06,
      "loss": 0.1436,
      "step": 63400
    },
    {
      "epoch": 8.466666666666667,
      "grad_norm": 12.145590782165527,
      "learning_rate": 3.0669333333333336e-06,
      "loss": 0.1799,
      "step": 63500
    },
    {
      "epoch": 8.48,
      "grad_norm": 8.714784622192383,
      "learning_rate": 3.040266666666667e-06,
      "loss": 0.1877,
      "step": 63600
    },
    {
      "epoch": 8.493333333333334,
      "grad_norm": 14.996529579162598,
      "learning_rate": 3.0136000000000003e-06,
      "loss": 0.1827,
      "step": 63700
    },
    {
      "epoch": 8.506666666666666,
      "grad_norm": 6.890920162200928,
      "learning_rate": 2.9869333333333336e-06,
      "loss": 0.1422,
      "step": 63800
    },
    {
      "epoch": 8.52,
      "grad_norm": 8.798065185546875,
      "learning_rate": 2.960266666666667e-06,
      "loss": 0.1575,
      "step": 63900
    },
    {
      "epoch": 8.533333333333333,
      "grad_norm": 14.80895709991455,
      "learning_rate": 2.9336e-06,
      "loss": 0.1523,
      "step": 64000
    },
    {
      "epoch": 8.546666666666667,
      "grad_norm": 0.3382943570613861,
      "learning_rate": 2.906933333333334e-06,
      "loss": 0.189,
      "step": 64100
    },
    {
      "epoch": 8.56,
      "grad_norm": 0.2595175802707672,
      "learning_rate": 2.8802666666666668e-06,
      "loss": 0.126,
      "step": 64200
    },
    {
      "epoch": 8.573333333333334,
      "grad_norm": 2.1061158180236816,
      "learning_rate": 2.8536e-06,
      "loss": 0.1903,
      "step": 64300
    },
    {
      "epoch": 8.586666666666666,
      "grad_norm": 4.404387950897217,
      "learning_rate": 2.826933333333334e-06,
      "loss": 0.1855,
      "step": 64400
    },
    {
      "epoch": 8.6,
      "grad_norm": 0.41417548060417175,
      "learning_rate": 2.8002666666666667e-06,
      "loss": 0.1638,
      "step": 64500
    },
    {
      "epoch": 8.613333333333333,
      "grad_norm": 3.22334885597229,
      "learning_rate": 2.7736e-06,
      "loss": 0.1958,
      "step": 64600
    },
    {
      "epoch": 8.626666666666667,
      "grad_norm": 10.165444374084473,
      "learning_rate": 2.7469333333333337e-06,
      "loss": 0.1691,
      "step": 64700
    },
    {
      "epoch": 8.64,
      "grad_norm": 0.40129008889198303,
      "learning_rate": 2.720266666666667e-06,
      "loss": 0.1723,
      "step": 64800
    },
    {
      "epoch": 8.653333333333332,
      "grad_norm": 2.3859570026397705,
      "learning_rate": 2.6936e-06,
      "loss": 0.1788,
      "step": 64900
    },
    {
      "epoch": 8.666666666666666,
      "grad_norm": 4.5156378746032715,
      "learning_rate": 2.6669333333333337e-06,
      "loss": 0.1798,
      "step": 65000
    },
    {
      "epoch": 8.68,
      "grad_norm": 2.396221399307251,
      "learning_rate": 2.640266666666667e-06,
      "loss": 0.1893,
      "step": 65100
    },
    {
      "epoch": 8.693333333333333,
      "grad_norm": 0.6231759786605835,
      "learning_rate": 2.6136e-06,
      "loss": 0.1653,
      "step": 65200
    },
    {
      "epoch": 8.706666666666667,
      "grad_norm": 0.3685292899608612,
      "learning_rate": 2.5869333333333336e-06,
      "loss": 0.1673,
      "step": 65300
    },
    {
      "epoch": 8.72,
      "grad_norm": 17.208660125732422,
      "learning_rate": 2.560266666666667e-06,
      "loss": 0.2036,
      "step": 65400
    },
    {
      "epoch": 8.733333333333333,
      "grad_norm": 19.3857364654541,
      "learning_rate": 2.5335999999999998e-06,
      "loss": 0.1848,
      "step": 65500
    },
    {
      "epoch": 8.746666666666666,
      "grad_norm": 2.0124614238739014,
      "learning_rate": 2.5069333333333335e-06,
      "loss": 0.1579,
      "step": 65600
    },
    {
      "epoch": 8.76,
      "grad_norm": 4.00788688659668,
      "learning_rate": 2.480266666666667e-06,
      "loss": 0.1601,
      "step": 65700
    },
    {
      "epoch": 8.773333333333333,
      "grad_norm": 9.136418342590332,
      "learning_rate": 2.4536e-06,
      "loss": 0.1707,
      "step": 65800
    },
    {
      "epoch": 8.786666666666667,
      "grad_norm": 7.202546119689941,
      "learning_rate": 2.426933333333334e-06,
      "loss": 0.157,
      "step": 65900
    },
    {
      "epoch": 8.8,
      "grad_norm": 10.511234283447266,
      "learning_rate": 2.4002666666666667e-06,
      "loss": 0.1772,
      "step": 66000
    },
    {
      "epoch": 8.813333333333333,
      "grad_norm": 4.053338050842285,
      "learning_rate": 2.3736e-06,
      "loss": 0.1738,
      "step": 66100
    },
    {
      "epoch": 8.826666666666666,
      "grad_norm": 9.690519332885742,
      "learning_rate": 2.3469333333333338e-06,
      "loss": 0.1576,
      "step": 66200
    },
    {
      "epoch": 8.84,
      "grad_norm": 2.972736358642578,
      "learning_rate": 2.3202666666666666e-06,
      "loss": 0.1748,
      "step": 66300
    },
    {
      "epoch": 8.853333333333333,
      "grad_norm": 18.747215270996094,
      "learning_rate": 2.2936000000000004e-06,
      "loss": 0.1506,
      "step": 66400
    },
    {
      "epoch": 8.866666666666667,
      "grad_norm": 0.196649432182312,
      "learning_rate": 2.2669333333333337e-06,
      "loss": 0.1519,
      "step": 66500
    },
    {
      "epoch": 8.88,
      "grad_norm": 19.125478744506836,
      "learning_rate": 2.240266666666667e-06,
      "loss": 0.1678,
      "step": 66600
    },
    {
      "epoch": 8.893333333333333,
      "grad_norm": 9.962053298950195,
      "learning_rate": 2.2136000000000003e-06,
      "loss": 0.1851,
      "step": 66700
    },
    {
      "epoch": 8.906666666666666,
      "grad_norm": 14.93825912475586,
      "learning_rate": 2.1869333333333336e-06,
      "loss": 0.1582,
      "step": 66800
    },
    {
      "epoch": 8.92,
      "grad_norm": 10.371707916259766,
      "learning_rate": 2.160266666666667e-06,
      "loss": 0.1775,
      "step": 66900
    },
    {
      "epoch": 8.933333333333334,
      "grad_norm": 14.830789566040039,
      "learning_rate": 2.1336e-06,
      "loss": 0.1522,
      "step": 67000
    },
    {
      "epoch": 8.946666666666667,
      "grad_norm": 26.034013748168945,
      "learning_rate": 2.1069333333333335e-06,
      "loss": 0.1684,
      "step": 67100
    },
    {
      "epoch": 8.96,
      "grad_norm": 0.9516153931617737,
      "learning_rate": 2.080266666666667e-06,
      "loss": 0.1885,
      "step": 67200
    },
    {
      "epoch": 8.973333333333333,
      "grad_norm": 0.5152891874313354,
      "learning_rate": 2.0536e-06,
      "loss": 0.1525,
      "step": 67300
    },
    {
      "epoch": 8.986666666666666,
      "grad_norm": 0.14681760966777802,
      "learning_rate": 2.0269333333333334e-06,
      "loss": 0.1618,
      "step": 67400
    },
    {
      "epoch": 9.0,
      "grad_norm": 6.129608154296875,
      "learning_rate": 2.0002666666666667e-06,
      "loss": 0.1441,
      "step": 67500
    },
    {
      "epoch": 9.013333333333334,
      "grad_norm": 9.607069969177246,
      "learning_rate": 1.9736e-06,
      "loss": 0.1403,
      "step": 67600
    },
    {
      "epoch": 9.026666666666667,
      "grad_norm": 17.828632354736328,
      "learning_rate": 1.9469333333333333e-06,
      "loss": 0.1397,
      "step": 67700
    },
    {
      "epoch": 9.04,
      "grad_norm": 4.124533653259277,
      "learning_rate": 1.920266666666667e-06,
      "loss": 0.1106,
      "step": 67800
    },
    {
      "epoch": 9.053333333333333,
      "grad_norm": 16.984500885009766,
      "learning_rate": 1.8936000000000002e-06,
      "loss": 0.1747,
      "step": 67900
    },
    {
      "epoch": 9.066666666666666,
      "grad_norm": 22.008731842041016,
      "learning_rate": 1.8669333333333335e-06,
      "loss": 0.161,
      "step": 68000
    },
    {
      "epoch": 9.08,
      "grad_norm": 4.875970840454102,
      "learning_rate": 1.8402666666666668e-06,
      "loss": 0.1867,
      "step": 68100
    },
    {
      "epoch": 9.093333333333334,
      "grad_norm": 4.890084266662598,
      "learning_rate": 1.8136e-06,
      "loss": 0.1525,
      "step": 68200
    },
    {
      "epoch": 9.106666666666667,
      "grad_norm": 0.7001702189445496,
      "learning_rate": 1.7869333333333336e-06,
      "loss": 0.1824,
      "step": 68300
    },
    {
      "epoch": 9.12,
      "grad_norm": 2.631011486053467,
      "learning_rate": 1.7602666666666667e-06,
      "loss": 0.1845,
      "step": 68400
    },
    {
      "epoch": 9.133333333333333,
      "grad_norm": 17.968059539794922,
      "learning_rate": 1.7336e-06,
      "loss": 0.1891,
      "step": 68500
    },
    {
      "epoch": 9.146666666666667,
      "grad_norm": 50.2859992980957,
      "learning_rate": 1.7069333333333335e-06,
      "loss": 0.1636,
      "step": 68600
    },
    {
      "epoch": 9.16,
      "grad_norm": 14.44841480255127,
      "learning_rate": 1.6802666666666666e-06,
      "loss": 0.192,
      "step": 68700
    },
    {
      "epoch": 9.173333333333334,
      "grad_norm": 14.675247192382812,
      "learning_rate": 1.6536000000000001e-06,
      "loss": 0.1587,
      "step": 68800
    },
    {
      "epoch": 9.186666666666667,
      "grad_norm": 0.3676046133041382,
      "learning_rate": 1.6269333333333334e-06,
      "loss": 0.1684,
      "step": 68900
    },
    {
      "epoch": 9.2,
      "grad_norm": 11.607419967651367,
      "learning_rate": 1.600266666666667e-06,
      "loss": 0.18,
      "step": 69000
    },
    {
      "epoch": 9.213333333333333,
      "grad_norm": 12.591584205627441,
      "learning_rate": 1.5736e-06,
      "loss": 0.1827,
      "step": 69100
    },
    {
      "epoch": 9.226666666666667,
      "grad_norm": 0.7269870042800903,
      "learning_rate": 1.5469333333333334e-06,
      "loss": 0.1639,
      "step": 69200
    },
    {
      "epoch": 9.24,
      "grad_norm": 6.627227306365967,
      "learning_rate": 1.5202666666666669e-06,
      "loss": 0.1761,
      "step": 69300
    },
    {
      "epoch": 9.253333333333334,
      "grad_norm": 34.87822723388672,
      "learning_rate": 1.4936e-06,
      "loss": 0.171,
      "step": 69400
    },
    {
      "epoch": 9.266666666666667,
      "grad_norm": 9.701499938964844,
      "learning_rate": 1.4669333333333335e-06,
      "loss": 0.174,
      "step": 69500
    },
    {
      "epoch": 9.28,
      "grad_norm": 38.64802169799805,
      "learning_rate": 1.4402666666666668e-06,
      "loss": 0.1494,
      "step": 69600
    },
    {
      "epoch": 9.293333333333333,
      "grad_norm": 8.630985260009766,
      "learning_rate": 1.4136000000000003e-06,
      "loss": 0.1552,
      "step": 69700
    },
    {
      "epoch": 9.306666666666667,
      "grad_norm": 5.937535762786865,
      "learning_rate": 1.3869333333333334e-06,
      "loss": 0.1599,
      "step": 69800
    },
    {
      "epoch": 9.32,
      "grad_norm": 3.766040086746216,
      "learning_rate": 1.3602666666666667e-06,
      "loss": 0.1761,
      "step": 69900
    },
    {
      "epoch": 9.333333333333334,
      "grad_norm": 6.450113773345947,
      "learning_rate": 1.3336000000000002e-06,
      "loss": 0.1669,
      "step": 70000
    },
    {
      "epoch": 9.346666666666668,
      "grad_norm": 0.6152761578559875,
      "learning_rate": 1.3069333333333333e-06,
      "loss": 0.1733,
      "step": 70100
    },
    {
      "epoch": 9.36,
      "grad_norm": 10.980165481567383,
      "learning_rate": 1.2802666666666669e-06,
      "loss": 0.2035,
      "step": 70200
    },
    {
      "epoch": 9.373333333333333,
      "grad_norm": 26.384477615356445,
      "learning_rate": 1.2536000000000002e-06,
      "loss": 0.172,
      "step": 70300
    },
    {
      "epoch": 9.386666666666667,
      "grad_norm": 7.2966108322143555,
      "learning_rate": 1.2269333333333335e-06,
      "loss": 0.1453,
      "step": 70400
    },
    {
      "epoch": 9.4,
      "grad_norm": 0.7717053294181824,
      "learning_rate": 1.2002666666666668e-06,
      "loss": 0.1736,
      "step": 70500
    },
    {
      "epoch": 9.413333333333334,
      "grad_norm": 2.122814178466797,
      "learning_rate": 1.1736e-06,
      "loss": 0.1627,
      "step": 70600
    },
    {
      "epoch": 9.426666666666666,
      "grad_norm": 7.474236011505127,
      "learning_rate": 1.1469333333333334e-06,
      "loss": 0.1301,
      "step": 70700
    },
    {
      "epoch": 9.44,
      "grad_norm": 0.5190141797065735,
      "learning_rate": 1.1202666666666667e-06,
      "loss": 0.1931,
      "step": 70800
    },
    {
      "epoch": 9.453333333333333,
      "grad_norm": 0.919028639793396,
      "learning_rate": 1.0936e-06,
      "loss": 0.157,
      "step": 70900
    },
    {
      "epoch": 9.466666666666667,
      "grad_norm": 2.069249153137207,
      "learning_rate": 1.0669333333333335e-06,
      "loss": 0.1484,
      "step": 71000
    },
    {
      "epoch": 9.48,
      "grad_norm": 1.0787569284439087,
      "learning_rate": 1.0402666666666668e-06,
      "loss": 0.1357,
      "step": 71100
    },
    {
      "epoch": 9.493333333333334,
      "grad_norm": 19.372196197509766,
      "learning_rate": 1.0136000000000001e-06,
      "loss": 0.1625,
      "step": 71200
    },
    {
      "epoch": 9.506666666666666,
      "grad_norm": 3.0568134784698486,
      "learning_rate": 9.869333333333334e-07,
      "loss": 0.1545,
      "step": 71300
    },
    {
      "epoch": 9.52,
      "grad_norm": 0.3072206377983093,
      "learning_rate": 9.602666666666667e-07,
      "loss": 0.1644,
      "step": 71400
    },
    {
      "epoch": 9.533333333333333,
      "grad_norm": 16.102794647216797,
      "learning_rate": 9.336e-07,
      "loss": 0.1595,
      "step": 71500
    },
    {
      "epoch": 9.546666666666667,
      "grad_norm": 5.041835784912109,
      "learning_rate": 9.069333333333335e-07,
      "loss": 0.1621,
      "step": 71600
    },
    {
      "epoch": 9.56,
      "grad_norm": 13.809104919433594,
      "learning_rate": 8.802666666666668e-07,
      "loss": 0.17,
      "step": 71700
    },
    {
      "epoch": 9.573333333333334,
      "grad_norm": 3.554908275604248,
      "learning_rate": 8.536000000000001e-07,
      "loss": 0.1827,
      "step": 71800
    },
    {
      "epoch": 9.586666666666666,
      "grad_norm": 0.9149110317230225,
      "learning_rate": 8.269333333333334e-07,
      "loss": 0.1664,
      "step": 71900
    },
    {
      "epoch": 9.6,
      "grad_norm": 12.80840015411377,
      "learning_rate": 8.002666666666667e-07,
      "loss": 0.1583,
      "step": 72000
    },
    {
      "epoch": 9.613333333333333,
      "grad_norm": 8.236490249633789,
      "learning_rate": 7.736000000000001e-07,
      "loss": 0.1591,
      "step": 72100
    },
    {
      "epoch": 9.626666666666667,
      "grad_norm": 5.659752368927002,
      "learning_rate": 7.469333333333334e-07,
      "loss": 0.1792,
      "step": 72200
    },
    {
      "epoch": 9.64,
      "grad_norm": 6.496099472045898,
      "learning_rate": 7.202666666666667e-07,
      "loss": 0.1639,
      "step": 72300
    },
    {
      "epoch": 9.653333333333332,
      "grad_norm": 14.16482925415039,
      "learning_rate": 6.936000000000001e-07,
      "loss": 0.158,
      "step": 72400
    },
    {
      "epoch": 9.666666666666666,
      "grad_norm": 6.249344348907471,
      "learning_rate": 6.669333333333333e-07,
      "loss": 0.136,
      "step": 72500
    },
    {
      "epoch": 9.68,
      "grad_norm": 2.7003774642944336,
      "learning_rate": 6.402666666666666e-07,
      "loss": 0.188,
      "step": 72600
    },
    {
      "epoch": 9.693333333333333,
      "grad_norm": 12.682883262634277,
      "learning_rate": 6.136e-07,
      "loss": 0.1765,
      "step": 72700
    },
    {
      "epoch": 9.706666666666667,
      "grad_norm": 0.5899399518966675,
      "learning_rate": 5.869333333333333e-07,
      "loss": 0.1612,
      "step": 72800
    },
    {
      "epoch": 9.72,
      "grad_norm": 12.66872501373291,
      "learning_rate": 5.602666666666668e-07,
      "loss": 0.1501,
      "step": 72900
    },
    {
      "epoch": 9.733333333333333,
      "grad_norm": 18.15076446533203,
      "learning_rate": 5.336000000000001e-07,
      "loss": 0.1676,
      "step": 73000
    },
    {
      "epoch": 9.746666666666666,
      "grad_norm": 0.7660354375839233,
      "learning_rate": 5.069333333333334e-07,
      "loss": 0.1885,
      "step": 73100
    },
    {
      "epoch": 9.76,
      "grad_norm": 16.856473922729492,
      "learning_rate": 4.802666666666667e-07,
      "loss": 0.1948,
      "step": 73200
    },
    {
      "epoch": 9.773333333333333,
      "grad_norm": 12.070262908935547,
      "learning_rate": 4.5360000000000004e-07,
      "loss": 0.1659,
      "step": 73300
    },
    {
      "epoch": 9.786666666666667,
      "grad_norm": 3.835123300552368,
      "learning_rate": 4.2693333333333334e-07,
      "loss": 0.1772,
      "step": 73400
    },
    {
      "epoch": 9.8,
      "grad_norm": 11.441450119018555,
      "learning_rate": 4.002666666666667e-07,
      "loss": 0.163,
      "step": 73500
    },
    {
      "epoch": 9.813333333333333,
      "grad_norm": 13.695916175842285,
      "learning_rate": 3.736e-07,
      "loss": 0.1642,
      "step": 73600
    },
    {
      "epoch": 9.826666666666666,
      "grad_norm": 0.27738815546035767,
      "learning_rate": 3.4693333333333337e-07,
      "loss": 0.1593,
      "step": 73700
    },
    {
      "epoch": 9.84,
      "grad_norm": 8.113584518432617,
      "learning_rate": 3.2026666666666673e-07,
      "loss": 0.1956,
      "step": 73800
    },
    {
      "epoch": 9.853333333333333,
      "grad_norm": 17.470783233642578,
      "learning_rate": 2.9360000000000003e-07,
      "loss": 0.1754,
      "step": 73900
    },
    {
      "epoch": 9.866666666666667,
      "grad_norm": 12.014216423034668,
      "learning_rate": 2.6693333333333334e-07,
      "loss": 0.1643,
      "step": 74000
    },
    {
      "epoch": 9.88,
      "grad_norm": 2.2627768516540527,
      "learning_rate": 2.402666666666667e-07,
      "loss": 0.1721,
      "step": 74100
    },
    {
      "epoch": 9.893333333333333,
      "grad_norm": 0.08254691958427429,
      "learning_rate": 2.1360000000000003e-07,
      "loss": 0.1518,
      "step": 74200
    },
    {
      "epoch": 9.906666666666666,
      "grad_norm": 6.228024005889893,
      "learning_rate": 1.8693333333333334e-07,
      "loss": 0.1395,
      "step": 74300
    },
    {
      "epoch": 9.92,
      "grad_norm": 24.429662704467773,
      "learning_rate": 1.602666666666667e-07,
      "loss": 0.1614,
      "step": 74400
    },
    {
      "epoch": 9.933333333333334,
      "grad_norm": 16.786714553833008,
      "learning_rate": 1.336e-07,
      "loss": 0.1472,
      "step": 74500
    }
  ],
  "logging_steps": 100,
  "max_steps": 75000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 378841325568000.0,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
